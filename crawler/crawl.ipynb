{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\mhass\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB of Websites by Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63926, 'zh-cn': 16652, 'id': 8471, 'ru': 4302, 'es': 3924, 'de': 3500, 'ja': 3380, 'pt': 3346, 'ko': 2730, 'fr': 2618, 'vi': 1817, 'it': 1497, 'tr': 1457, 'nl': 1152, 'pl': 1105, 'ar': 1085, 'fa': 1067, 'th': 968, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 336, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 242, 'bn': 228, 'sk': 206, 'so': 171, 'he': 169, 'lt': 131, 'sl': 108, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n",
    "\n",
    "# list all unique languages on a new line and total number of unique languages\n",
    "unique_languages = set(languages)\n",
    "print(unique_languages)\n",
    "print(f\"Total number of unique languages: {len(unique_languages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total websites in English: 63926\n",
      "Total websites in Chinese(simplified): 16652\n",
      "Total websites in Chinese(traditional): 15\n",
      "Total websites in Korean: 2730\n",
      "Total websites in Japanese: 3380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese(simplified): {len(websites)}\")\n",
    "\n",
    "# zh-tw\n",
    "websites = websites_table.search(Website.language == \"zh-tw\")\n",
    "print(f\"Total websites in Chinese(traditional): {len(websites)}\")\n",
    "\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Random Websites from the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 random domains for eng, zh-cn, ko, ja languages\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "# get 25 random websites\n",
    "random_en_websites = random.sample(websites, 25)\n",
    "print(\"Random websites in English:\")\n",
    "for website in random_en_websites:\n",
    "    print(website['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Chinese(simplified):\n",
      "piggymates.com\n",
      "xiningchina.com\n",
      "qianbangjiaoyu.com\n",
      "dnbbm.com\n",
      "sanygroup.com\n",
      "qxjf-art.com\n",
      "xiamiaoyangzhi.com\n",
      "cqkuaisu.com\n",
      "cshuaqun.com\n",
      "gongyeqg.com\n",
      "18avx.com\n",
      "chinabrx.com\n",
      "rendaikuan.com\n",
      "szyueshan.com\n",
      "qdzhiruitong.com\n",
      "freereceivesms.com\n",
      "gggoodgame.com\n",
      "hellobike.com\n",
      "allstar-era.com\n",
      "kxunchina.com\n",
      "zgrtcm.com\n",
      "yumerzx.com\n",
      "leg1678.com\n",
      "shenzhen-nanning.com\n",
      "262196.cn\n",
      "Random websites in Chinese(simplified):\n",
      "fytlsm.com\n",
      "hapclock.com\n",
      "jtk100.com\n",
      "iduduapp.com\n",
      "sdbxqy.com\n",
      "njchangxue.com\n",
      "daimonchina.com\n",
      "zhaoshimy.com\n",
      "jutu360.com\n",
      "wodessay.com\n",
      "yataixuanhao.com\n",
      "hztaiyi.com\n",
      "sujienk.com\n",
      "liusuliusu.com\n",
      "cmdjdkj.com\n",
      "hycmzc.com\n",
      "ytshenhong.com\n",
      "znote8899.com\n",
      "jxlesong.com\n",
      "jax-china.com\n",
      "51haotou.com\n",
      "xinglistqy.com\n",
      "mtsbjy.com\n",
      "ysu.edu.cn\n",
      "tianqingshiyin.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "# get 25 random websites\n",
    "random_ZhCn_websites1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for zh-cn language which is different from the previous 25 websites in random_ZhCn_websites1\n",
    "random_ZhCn_websites2 = random.sample(websites, 25)\n",
    "# check random_ZhCn_websites2 is different from random_ZhCn_websites1\n",
    "for website in random_ZhCn_websites2:\n",
    "    if website in random_ZhCn_websites1:\n",
    "        print(\"Random websites in Chinese(simplified) are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ZhCn_websites2.remove(website)\n",
    "        random_ZhCn_websites2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean:\n",
      "21stcbc.org\n",
      "lcd1004.co.kr\n",
      "pvxywg.com\n",
      "wtwt248.com\n",
      "papalah.pw\n",
      "jusoya10.com\n",
      "netpro.co.kr\n",
      "bestone-work.com\n",
      "kassashair.com\n",
      "chroscience.com\n",
      "studypatent.com\n",
      "ovotv.com\n",
      "chosong.co.kr\n",
      "xsmzjc.com\n",
      "daehangreenpower.com\n",
      "xingyueboke.com\n",
      "womaneconomy.co.kr\n",
      "keyixs.com\n",
      "xn--939au0g3vw1iaq8a469c.kr\n",
      "19878719.com\n",
      "scshangting.com\n",
      "rongbaodianmo.com\n",
      "mgyqw.com\n",
      "dabangapp.com\n",
      "qiutianxia29.com\n",
      "Random websites in Korean:\n",
      "uqcjvpk.cn\n",
      "mobilitytv.co.kr\n",
      "whichav.video\n",
      "jxcgyl.com\n",
      "interpark.com\n",
      "jiexunec.com\n",
      "1234567.com.cn\n",
      "neworbis.com\n",
      "heywakeup.com.tw\n",
      "wozai-travel.com\n",
      "smdv.kr\n",
      "ezalba.co.kr\n",
      "haobofangshui.com\n",
      "torrentsee217.com\n",
      "ruantongzhi.com\n",
      "cmuma.xyz\n",
      "ttlock.com\n",
      "jmdoor.com.tw\n",
      "newtoki.help\n",
      "sxwlz.com\n",
      "sdmeixiusy.com\n",
      "optisun.vip\n",
      "clean-clean-peru.com\n",
      "11toon112.com\n",
      "aniweek.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "# get 25 random websites\n",
    "random__ko_websites_1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for korean language which is different from the previous 25 websites in random__ko_websites_1\n",
    "random__ko_websites_2 = random.sample(websites, 25)\n",
    "# check random__ko_websites_2 is different from random__ko_websites_1\n",
    "for website in random__ko_websites_2:\n",
    "    if website in random__ko_websites_1:\n",
    "        print(\"Random websites in Korean are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random__ko_websites_2.remove(website)\n",
    "        random__ko_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_2:\n",
    "    print(website['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean: [{'url': 'yxzwlkj.com', 'language': 'ko', 'timestamp': '2024-06-08T08:34:31.376923'}, {'url': 'jshaoou.com', 'language': 'ko', 'timestamp': '2024-06-08T07:06:15.695528'}]\n"
     ]
    }
   ],
   "source": [
    "# give two more random korean websites\n",
    "random__ko_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Korean:\", random__ko_websites_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359198.com\n",
      "toonkor326.com\n",
      "dbcnews.co.kr\n",
      "hbwocheng.com\n",
      "whichav.video\n",
      "pinksisly.com\n",
      "chenzhongtech.com\n",
      "bluezz.com.tw\n",
      "gdzhukou.com\n",
      "oplove16.com\n",
      "yamoa3.site\n",
      "zhongfa1688.com\n",
      "douyuanxiuhe.com\n",
      "yp.com.hk\n",
      "imendon.com\n",
      "fxfx217.com\n",
      "htwhbook.com\n",
      "yedam.com\n",
      "homeplus.co.kr\n",
      "newhua99.xyz\n",
      "88p2p.com\n",
      "shyuwangfangshui.com\n",
      "fenghemp.com\n",
      "daoom.co.kr\n",
      "nfqlife.com\n",
      "evolutionplaynow.com\n",
      "limeitianhe.com\n",
      "yebigun1.mil.kr\n",
      "anpservice.net\n",
      "trfsgs.com\n"
     ]
    }
   ],
   "source": [
    "# give 30 random websites from the database for korean, chinese and japanese languages\n",
    "# get 30 random websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "random_websites = random.sample(websites, 30)\n",
    "# just list urls\n",
    "for website in random_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nobori-mart.net\n",
      "keirin-mobile.jp\n",
      "sesto.jp\n",
      "eigeki.com\n",
      "valor-luvitapp.com\n",
      "kuzen.io\n",
      "enechange.co.jp\n",
      "hanako.tokyo\n",
      "saiyasune.com\n",
      "gogojungle.co.jp\n",
      "madamefigaro.jp\n",
      "reil.co.jp\n",
      "koichidomoto-fc.net\n",
      "ganbalegends.com\n",
      "jal.co.jp\n",
      "dengekionline.com\n",
      "bribaby.jp\n",
      "hankyu.co.jp\n",
      "yutasan.co\n",
      "laxd.com\n",
      "karakubuy.com\n",
      "hellouniweb.com\n",
      "android4front.jp\n",
      "nichiga.net\n",
      "cardrush-pokemon.jp\n",
      "Random websites in Japanese:\n",
      "domonet.jp\n",
      "sabory-blog.com\n",
      "tsurisuke.com\n",
      "hero-news.com\n",
      "halmek.co.jp\n",
      "monotaro.com\n",
      "homes.co.jp\n",
      "ai-eye.jp\n",
      "pushcode.jp\n",
      "tyuemon.com\n",
      "ifdef.jp\n",
      "mangakoma.net\n",
      "sangacio.com\n",
      "jobop.jp\n",
      "rere.jp\n",
      "thp-shop.co.jp\n",
      "brandnavi-online.com\n",
      "ranking.net\n",
      "edesk.jp\n",
      "kimuratan.jp\n",
      "xn--pckua2a7gp15o89zb.com\n",
      "kawashima-ya.jp\n",
      "nippon-foundation.or.jp\n",
      "boxingnews.jp\n",
      "axa-direct.co.jp\n"
     ]
    }
   ],
   "source": [
    "# get 25 random websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "random_ja_websites = random.sample(websites, 25)\n",
    "# just list urls\n",
    "for website in random_ja_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "# get 25 more random japanese websites\n",
    "random_ja_websites_2 = random.sample(websites, 25)\n",
    "# check random_ja_websites_2 is different from random_ja_websites\n",
    "for website in random_ja_websites_2:\n",
    "    if website in random_ja_websites:\n",
    "        # print(\"Random websites in Japanese are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ja_websites_2.remove(website)\n",
    "        random_ja_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Japanese:\")\n",
    "for website in random_ja_websites_2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Japanese: [{'url': 'comiful.net', 'language': 'ja', 'timestamp': '2024-06-08T09:51:24.302366'}, {'url': 'hentaiasmr.moe', 'language': 'ja', 'timestamp': '2024-06-08T02:31:00.175990'}]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 more random japanese websites\n",
    "random_ja_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Japanese:\", random_ja_websites_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of user agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# get free proxies from online\n",
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # fetch proxy list from online\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})\n",
    "\n",
    "\n",
    "    # print(proxy_table)\n",
    "\n",
    "    # # Extract proxy IPs and ports\n",
    "    proxies = []\n",
    "    proxy_table = proxy_table.find(\"tbody\")\n",
    "    # print(proxy_table)\n",
    "    for row in proxy_table.find_all(\"tr\"):\n",
    "        proxies.append({\n",
    "        \"ip\":   row.find_all(\"td\")[0].string,\n",
    "        \"port\": row.find_all(\"td\")[1].string\n",
    "        })\n",
    "        # print(row)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_35.naver\n",
      "Extracting links from: https://policy.naver.com/rules/youthpolicy.html\n",
      "Extracting links from: http://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_16.naver\n",
      "Extracting links from: https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login\n",
      "Extracting links from: https://www.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_17.naver\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_18.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "Extracting links from: https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "Extracting links from: https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "Extracting links from: http://naver.com\n",
      "Extracting links from: https://policy.naver.com/policy/service.html\n",
      "Extracting links from: http://www.naver.com\n",
      "Extracting links from: https://help.naver.com\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "Extracting links from: https://right.naver.com\n",
      "Extracting links from: https://help.naver.com/\n",
      "Extracting links from: http://help.naver.com/\n",
      "Extracting links from: https://help.naver.com/support/home.nhn\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "Extracting links from: http://policy.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "Extracting links from: http://policy.naver.com/rules/service_location.html\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063\n",
      "Extracting links from: https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "Extracting links from: https://tv.naver.com/v/13644277/list/594825\n",
      "Extracting links from: https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/privacyinfo\n",
      "Extracting links from: http://www.naver.com/rules/service.html\n",
      "Extracting links from: http://www.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "Extracting links from: https://www.naver.com/more.html\n",
      "Extracting links from: https://www.naver.com/policy/service.html\n",
      "Extracting links from: https://www.naver.com/policy/privacy.html\n",
      "Extracting links from: https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "Extracting links from: http://www.naver.com/rules/disclaimer.html\n",
      "Extracting links from: https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "Extracting links from: https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "Extracting links from: https://gam.naver.com/optout/main \n",
      "Error accessing https://gam.naver.com/optout/main : 404 Client Error: 404 for url: https://gam.naver.com/optout/main%20\n",
      "Extracting links from: https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "Extracting links from: https://blog.naver.com/n_privacy\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://notice.naver.com/notices/privacypolicy/13880\n",
      "Extracting links from: https://www.naver.com/policy/youthpolicy.html\n",
      "Extracting links from: https://help.naver.com/alias/report/Protection_report.naver\n",
      "Extracting links from: https://policy.naver.com/policy/service_group.html\n",
      "Extracting links from: https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://blog.naver.com/blogpeople/220823707644\n",
      "Extracting links from: https://help.naver.com/support/reportCenter/home.help\n",
      "Extracting links from: https://green.naver.com/\n",
      "Extracting links from: https://privacy.naver.com\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14997\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14236\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14235\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14234\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14233\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223471678731\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223463199093\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223455075484\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223446809582\n",
      "Extracting links from: http://study.jr.naver.com/privacy/\n",
      "Extracting links from: https://jr.naver.com/\n",
      "Extracting links from: https://help.naver.com/service/5636/category/bookmark\n",
      "Extracting links from: https://jr.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/report/report_m_3.naver\n",
      "Extracting links from: https://right.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html#a4\n",
      "Extracting links from: https://inoti.naver.com/inoti/main.nhn\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "Extracting links from: http://policy.naver.com/policy/disclaimer.html\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "Extracting links from: https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html#a2_3\n",
      "Extracting links from: http://blog.naver.com/n_privacy/80143119849\n",
      "Extracting links from: https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "Extracting links from: https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n",
      "All extracted links:\n",
      "https://help.naver.com/alias/search/word/word_35.naver\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "http://www.naver.com/\n",
      "https://help.naver.com/alias/search/word/word_16.naver\n",
      "https://www.navercorp.com\n",
      "https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "https://nid.naver.com/nidlogin.login\n",
      "https://www.naver.com\n",
      "https://help.naver.com/alias/search/word/word_17.naver\n",
      "https://help.naver.com/alias/search/word/word_18.naver\n",
      "https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "https://www.navercorp.com/\n",
      "https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "http://naver.com\n",
      "https://policy.naver.com/policy/service.html\n",
      "http://www.naver.com\n",
      "https://help.naver.com\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "https://right.naver.com\n",
      "https://help.naver.com/\n",
      "http://www.navercorp.com/\n",
      "http://recruit.navercorp.com/\n",
      "https://www.navercorp.com/nhn/company/proposalGuide.nhn\n",
      "http://help.naver.com/\n",
      "https://help.naver.com/support/home.nhn\n",
      "http://policy.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "http://policy.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "http://policy.naver.com/rules/service_location.html\n",
      "https://notice.naver.com/notices/LBS/14063\n",
      "https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "https://tv.naver.com/v/13644277/list/594825\n",
      "https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "https://policy.naver.com/policy/privacy.html\n",
      "https://www.law.go.kr/LSW/lsInfoP.do?efYd=20200805&lsiSeq=213857#0000\n",
      "http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm\n",
      "https://centerforplainlanguage.org/learning-training/five-steps-plain-language/\n",
      "https://privacy.naver.com/privacyinfo\n",
      "http://www.naver.com/rules/service.html\n",
      "http://www.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "http://www.law.go.kr/법령/형사소송법/(13454,20150731)\n",
      "http://www.law.go.kr/lsInfoP.do?lsiSeq=160962&efYd=20141015#0000\n",
      "http://www.law.go.kr/법령/전기통신사업법/(13011,20150120)\n",
      "https://www.naver.com/more.html\n",
      "https://recruit.navercorp.com/\n",
      "https://www.navercorp.com/naver/proposalInquire\n",
      "https://www.naver.com/policy/service.html\n",
      "https://www.naver.com/policy/privacy.html\n",
      "https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "http://www.naver.com/rules/disclaimer.html\n",
      "https://www.navercorp.com/ko/company/proposalRegister.nhn\n",
      "https://s.pstatic.net/static/www/rules/naverBrandRequest.doc\n",
      "https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "https://gam.naver.com/optout/main \n",
      "https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "http://www.navercorp.com/ko/index.nhn\n",
      "https://blog.naver.com/n_privacy\n",
      "https://privacy.kisa.or.kr/main.do\n",
      "https://www.spo.go.kr/site/spo/main.do\n",
      "http://www.police.go.kr/www/security/cyber.jsp\n",
      "https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://notice.naver.com/notices/privacypolicy/13880\n",
      "https://www.naver.com/policy/youthpolicy.html\n",
      "https://help.naver.com/alias/report/Protection_report.naver\n",
      "https://policy.naver.com/policy/service_group.html\n",
      "https://www.kiso.or.kr/%EC%95%8C%EB%A6%BC%EB%A7%88%EB%8B%B9/%EC%A3%BC%EC%9A%94-%EA%B3%B5%EA%B0%9C%EC%82%AC%ED%95%AD/%EC%A0%95%EC%B1%85%EA%B2%B0%EC%A0%95/\n",
      "https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://blog.naver.com/blogpeople/220823707644\n",
      "https://www.kiso.or.kr/%EC%A0%95%EC%B1%85%EC%9C%84%EC%9B%90%ED%9A%8C/%EC%A0%95%EC%B1%85%EA%B7%9C%EC%A0%95/\n",
      "https://www.kiso.or.kr/%EC%A0%95%EB%B3%B4%EC%84%BC%ED%84%B0/kiso-%EC%A0%95%EC%B1%85/guideline/\n",
      "https://help.naver.com/support/reportCenter/home.help\n",
      "https://green.naver.com/\n",
      "https://privacy.naver.com\n",
      "https://notice.naver.com/notices/privacynid/14997\n",
      "https://notice.naver.com/notices/privacynid/14236\n",
      "https://notice.naver.com/notices/privacynid/14235\n",
      "https://notice.naver.com/notices/privacynid/14234\n",
      "https://notice.naver.com/notices/privacynid/14233\n",
      "https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "https://blog.naver.com//n_privacy/223471678731\n",
      "https://blog.naver.com//n_privacy/223463199093\n",
      "https://blog.naver.com//n_privacy/223455075484\n",
      "https://blog.naver.com//n_privacy/223446809582\n",
      "http://study.jr.naver.com/privacy/\n",
      "https://jr.naver.com/\n",
      "https://help.naver.com/service/5636/category/bookmark\n",
      "https://jr.naver.com\n",
      "https://help.naver.com/alias/report/report_m_3.naver\n",
      "https://right.naver.com/\n",
      "https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "https://policy.naver.com/policy/privacy.html#a4\n",
      "https://www.facebook.com/naverprivacy\n",
      "https://inoti.naver.com/inoti/main.nhn\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "http://policy.naver.com/policy/disclaimer.html\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "http://policy.naver.com/policy/privacy.html#a2_3\n",
      "http://blog.naver.com/n_privacy/80143119849\n",
      "http://ko.wikipedia.org/wiki/HTTP_Cookie\n",
      "http://cookiecentral.com/\n",
      "http://www.howstuffworks.com/cookie.htm\n",
      "https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://www.naver.com/',  # Replace with your URLs\n",
    "    'https://www.997788.com/',\t\n",
    "]\n",
    "\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Add a list of user agents here\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    # proxies = get_proxies()\n",
    "    # proxy = random.choice(proxies)\n",
    "    # print(f\"Using proxy: {proxy}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        if original_domain in url:\n",
    "            print(f\"Extracting links from: {url}\")\n",
    "            links = extract_links(url)\n",
    "            for link in links:\n",
    "                if link not in visited_links:\n",
    "                    # if original_domain in link:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    original_domain = \"naver.com\"\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        print(f\"Extracting links from: {url}\")\n",
    "        links = extract_links(url)\n",
    "        for link in links:\n",
    "            parsed_link = urlparse(link)\n",
    "            if link not in visited_links:\n",
    "                # check if original domain is in the link\n",
    "                if original_domain in parsed_link.netloc:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    original_domain = parsed_start_url.netloc\n",
    "    \n",
    "    print(f\"Original domain: {original_domain}, Start URL: {start_url}\")\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bn', 'de', 'en', 'fa', 'pt', 'websites', 'id', 'it', 'vi', 'fr', 'ja', 'zh-cn', 'ru', 'es', 'pl', 'ko', 'policy_links', 'sv'}\n"
     ]
    }
   ],
   "source": [
    "# read tinydb database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "\n",
    "# read all tables names\n",
    "tables = db.tables()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6088\n"
     ]
    }
   ],
   "source": [
    "# read policy_links table\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "policy_links = policy_links_table.all()\n",
    "print(len(policy_links))\n",
    "\n",
    "# find thirty domains for Japan and Korea which only has 1 privacy policy related link links\n",
    "\n",
    "# find thirty random domains for each country\n",
    "random_korean_domains = []\n",
    "random_japanese_domains = []\n",
    "\n",
    "for domain in policy_links:\n",
    "    if domain['country'] == \"Korea\":\n",
    "        random_korean_domains.append(domain['domain'])\n",
    "    elif domain['country'] == \"Japan\":\n",
    "        random_japanese_domains.append(domain['domain'])\n",
    "\n",
    "random_korean_domains = random.sample(random_korean_domains, 30)\n",
    "random_japanese_domains = random.sample(random_japanese_domains, 30)\n",
    "\n",
    "# find all_links for each domain and filter only privacy related links\n",
    "# for korean domains\n",
    "korean_privacy_links = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Extracted Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "# policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "5341\n",
      "Number of domains with no privacy links: 470\n",
      "Number of domains with privacy links: 2889\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "2383\n"
     ]
    }
   ],
   "source": [
    "print(len(privacy_domains))\n",
    "\n",
    "# give number of privacy related links for each domain\n",
    "japan_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            japan_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(japan_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tumi.co.jp/privacyPolicy.html\n",
      "https://mocom.tv/wo/privacy.phtml\n",
      "https://locondo.jp/shop/contents/privacy\n",
      "https://anzen.ne.jp/html/info.html#about_privacy\n",
      "https://gumpla.jp/privacy\n",
      "https://support.ac-pocketcamp.com/en-AU/privacy_policy\n",
      "https://ths-fooduniform.jp/html/privacy.html\n",
      "https://tanosu.com/privacy/\n",
      "https://buffaloes.co.jp/company/privacy.html\n",
      "https://cdn.kenko-mileage.jp/policy/privacy_policy.html\n",
      "https://sen-n.com/privacy-policy/\n",
      "https://crescendoalle.com/pages/privacy\n",
      "https://atomtech.co.jp/policies/privacy-policy\n",
      "https://wadatsumi.co/policies/privacy-policy\n",
      "https://danmachi-danchro.com/privacy-policy/\n",
      "https://tryt-worker.jp/company/privacypolicy/\n",
      "https://privacy.tver.jp/tver-id-external-data-integration/\n",
      "https://talk.jp/privacy\n",
      "https://ball-goods.com/?mode=privacy\n",
      "https://izumo-pbx.jp/contact.html#privacy\n",
      "https://sug-web.jp/privacy-policy/\n",
      "https://denkohome.com/privacy/\n",
      "https://freeblog-video.com/privacy-policy/\n",
      "https://irotsuku.com/info/privacy\n",
      "https://www.convention.co.jp/privacy/\n",
      "https://emmafrancis.jp/contents/terms#privacy_content\n",
      "https://www.evastore.jp/shop/pages/privacy.aspx#anchor-cookie_block\n",
      "https://www.bornfreegroup.jp/privacypolicy\n",
      "https://car-repo.jp/privacy-policy\n",
      "https://howsie-shop.jp/pages/privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from japan_privacy_links\n",
    "random_japan_privacy_links = random.sample(japan_privacy_links, 30)\n",
    "for link in random_japan_privacy_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0           https://www.tumi.co.jp/privacyPolicy.html   \n",
      "1                   https://mocom.tv/wo/privacy.phtml   \n",
      "2            https://locondo.jp/shop/contents/privacy   \n",
      "3    https://anzen.ne.jp/html/info.html#about_privacy   \n",
      "4                           https://gumpla.jp/privacy   \n",
      "5   https://support.ac-pocketcamp.com/en-AU/privac...   \n",
      "6        https://ths-fooduniform.jp/html/privacy.html   \n",
      "7                         https://tanosu.com/privacy/   \n",
      "8        https://buffaloes.co.jp/company/privacy.html   \n",
      "9   https://cdn.kenko-mileage.jp/policy/privacy_po...   \n",
      "10                  https://sen-n.com/privacy-policy/   \n",
      "11            https://crescendoalle.com/pages/privacy   \n",
      "12     https://atomtech.co.jp/policies/privacy-policy   \n",
      "13       https://wadatsumi.co/policies/privacy-policy   \n",
      "14       https://danmachi-danchro.com/privacy-policy/   \n",
      "15      https://tryt-worker.jp/company/privacypolicy/   \n",
      "16  https://privacy.tver.jp/tver-id-external-data-...   \n",
      "17                            https://talk.jp/privacy   \n",
      "18               https://ball-goods.com/?mode=privacy   \n",
      "19          https://izumo-pbx.jp/contact.html#privacy   \n",
      "20                 https://sug-web.jp/privacy-policy/   \n",
      "21                     https://denkohome.com/privacy/   \n",
      "22         https://freeblog-video.com/privacy-policy/   \n",
      "23                  https://irotsuku.com/info/privacy   \n",
      "24              https://www.convention.co.jp/privacy/   \n",
      "25  https://emmafrancis.jp/contents/terms#privacy_...   \n",
      "26  https://www.evastore.jp/shop/pages/privacy.asp...   \n",
      "27         https://www.bornfreegroup.jp/privacypolicy   \n",
      "28                 https://car-repo.jp/privacy-policy   \n",
      "29               https://howsie-shop.jp/pages/privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   3aa81fddc11f27554b348c4bacbdd6892201fd52f8316c...  \n",
      "1   f93003101a1dc35a8f84f77d5de4e50349401e6d7c4a68...  \n",
      "2   0604ad08c6272bc865c58070051960ddfd5c300f184d37...  \n",
      "3   731e8b4b89538c01b1b76420dff82480750bca42dd4d0f...  \n",
      "4   5fe338baacbd7f12d15613ae812baf4dd208b4d7d366a1...  \n",
      "5   4cfbdc8279ca6745bd1f58ee304f2664d80e8c439f2ffc...  \n",
      "6   0463f7194b0d3656e2354bc5b665044706b80a1cf22807...  \n",
      "7   2903c5b5b7fdc1debad2519b112f6336a7fbb35b4dc2bf...  \n",
      "8   b8693e71c58177ea524837d0503a4cfe207b5c1255ee0b...  \n",
      "9   024a526957f6b4637b126fdf22972e336da0f19c26ae27...  \n",
      "10  ee602a8262c3146e5b85c8a2ccec7334771e3d4b642e43...  \n",
      "11  f9e29796d4e6e2805030d3152ebb73b1297b26cc566892...  \n",
      "12  f3d0f3111dea5cc64f168f9927c3484b1232ef225dec01...  \n",
      "13  114031d73fe94b624387a9a61be1963621eba2cee79d04...  \n",
      "14  97eac313480d61761278cd181640503989412bbdf93637...  \n",
      "15  d5287ba8731acaf5aa06af1d0efd71f154713c7db69b6f...  \n",
      "16  60290a97a2bc3830100a40686c30336eaec60205c08944...  \n",
      "17  17a2b0fa9e931351b5581078c36f0aed9cf152c18bee47...  \n",
      "18  be860561cbe09928fd8369165d5220cd49506cf5acc2fd...  \n",
      "19  d0a945673bbae0fded01e61ad5d67fff842eca1011ff07...  \n",
      "20  d6bfe39b0eb99223ef4d81f7055e581ec683cae3a2dd4b...  \n",
      "21  0edaf50f6ee2ab3d38009a753d3105fd3a1f677d433efa...  \n",
      "22  ab47d235fbfc20f22d0ccdd16d0be74d7a7254a39f8d33...  \n",
      "23  c628917e715e52f6e9006e016a9aa09175b5b0e7aedc4c...  \n",
      "24  babc3060c024455ee2ade4741f5d8e49be716a1f783bd4...  \n",
      "25  e2e3885ef0abb740caf804b640a1bcd73bba652f5aaaa9...  \n",
      "26  a85a4da4288ffc49316c36e8cfdc7d0e1eb92dab2f1e6c...  \n",
      "27  2a82c56ac400c385da4d277ae8c0d4c9c120b596b10921...  \n",
      "28  0646142af80e14eabc4a880466468e493ed1e8485f24b9...  \n",
      "29  a92c2f2f86f61a7897c95c6634357081e55e91ce38dd6f...  \n"
     ]
    }
   ],
   "source": [
    "# random_japan_privacy_links\n",
    "# create a dataframe with random_japan_privacy_links and each url's hash value using sha256\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_japan_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df.to_csv(\"policies/sample_policies_japanese/sample_policies_japanese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Error fetching the website: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_japan_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df[df['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ja\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_japanese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n",
      "1409\n",
      "Number of domains with no privacy links: 1292\n",
      "Number of domains with privacy links: 1437\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "korean_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            korean_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(korean_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://newsway.co.kr/company/privacypolicy\n",
      "https://daumee.co.kr/member/privacy.html\n",
      "https://tophub.today/allactivity?privacy_source=activity_log_top_menu\n",
      "https://duole.com/privacy/v2/gouji?template_id=1#nav6\n",
      "https://xyzcdn.net/privacy#content\n",
      "https://bluepops.co.kr/member/privacy.html\n",
      "https://fifaro.com/text/privacy\n",
      "https://myprotein.tw/customer-services/privacy-and-security.list\n",
      "https://nsfwkr.net/bbs/register.php#privacy\n",
      "https://devicemart.co.kr/service/privacy\n",
      "https://gck99.com.tw/privacy.php\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://hk-bestcasino.com/privacy-policy/\n",
      "https://sanmin.com.tw/static/termspprivacy\n",
      "https://lolchess.gg/about/privacy\n",
      "http://www.creme21.co.kr/member/privacy.html\n",
      "https://ajou.ac.kr/kr/ajou/privacy.do\n",
      "https://balletnmodel.com/?mode=privacy\n",
      "https://roo.cash/privacy\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://m.cnhnb.com/html/zhuanti/privacy/?id=1\n",
      "https://privacy.kakao.com/main?lang=ko\n",
      "https://htisec.com/zh-hk/data-privacy-policy\n",
      "https://cagongtv.com/content/privacy\n",
      "https://godamanga.com/privacy\n",
      "https://nikke-kr.com/privacypolicy#_Pursuant_to_our\n",
      "https://store.steamchina.com/privacy_agreement?snr=1_60_4__global-responsive-menu\n",
      "https://nikke-kr.com/privacypolicy#_Changes\n",
      "https://www.pcone.com.tw/service/privacyPolicy\n",
      "https://qquing.net/bbs/content.php?co_id=privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from korean_privacy_links\n",
    "random_korean_privacy_links = random.sample(korean_privacy_links, 30)\n",
    "for link in random_korean_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0         https://newsway.co.kr/company/privacypolicy   \n",
      "1            https://daumee.co.kr/member/privacy.html   \n",
      "2   https://tophub.today/allactivity?privacy_sourc...   \n",
      "3   https://duole.com/privacy/v2/gouji?template_id...   \n",
      "4                  https://xyzcdn.net/privacy#content   \n",
      "5          https://bluepops.co.kr/member/privacy.html   \n",
      "6                     https://fifaro.com/text/privacy   \n",
      "7   https://myprotein.tw/customer-services/privacy...   \n",
      "8         https://nsfwkr.net/bbs/register.php#privacy   \n",
      "9            https://devicemart.co.kr/service/privacy   \n",
      "10                   https://gck99.com.tw/privacy.php   \n",
      "11           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "12          https://hk-bestcasino.com/privacy-policy/   \n",
      "13         https://sanmin.com.tw/static/termspprivacy   \n",
      "14                  https://lolchess.gg/about/privacy   \n",
      "15       http://www.creme21.co.kr/member/privacy.html   \n",
      "16              https://ajou.ac.kr/kr/ajou/privacy.do   \n",
      "17             https://balletnmodel.com/?mode=privacy   \n",
      "18                           https://roo.cash/privacy   \n",
      "19           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "20     https://m.cnhnb.com/html/zhuanti/privacy/?id=1   \n",
      "21             https://privacy.kakao.com/main?lang=ko   \n",
      "22       https://htisec.com/zh-hk/data-privacy-policy   \n",
      "23               https://cagongtv.com/content/privacy   \n",
      "24                      https://godamanga.com/privacy   \n",
      "25  https://nikke-kr.com/privacypolicy#_Pursuant_t...   \n",
      "26  https://store.steamchina.com/privacy_agreement...   \n",
      "27        https://nikke-kr.com/privacypolicy#_Changes   \n",
      "28     https://www.pcone.com.tw/service/privacyPolicy   \n",
      "29   https://qquing.net/bbs/content.php?co_id=privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   76f8d00688ce5ee4129d2cfe7810c53c1b81d4c5d82809...  \n",
      "1   4441eea2dddb934443cac7616ded62ace14dbbe17db651...  \n",
      "2   c9effc5fde671cafb31a85969f944b22ad7b6775b254cc...  \n",
      "3   4d2472373011c6f66d1a53123afd473dc329bfb630b9c4...  \n",
      "4   4f64b7cbec077f0b235583598a40a9b63d470cfa3f1b9b...  \n",
      "5   0caf8650720351b11304dcee1993f55cbb741047ed40d8...  \n",
      "6   9b5b2c7347c49766fa80a9a1188f05ce3d4a8ffcf61a04...  \n",
      "7   5144d03775d8eb736b523d33a4f8ce47d6a9d5303c6942...  \n",
      "8   753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...  \n",
      "9   5f6d20a69b3005d6025dc37ebf4c4ead341e002de46958...  \n",
      "10  1f30698e7b51975a02398b3b2348c99ccb113797a52a59...  \n",
      "11  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "12  4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...  \n",
      "13  046e039e7063fbdbe44fd3ebb9fef9ee05b4a956a3d1bd...  \n",
      "14  9b59d857a77fb34026b08fb304430966c38bd53202b6aa...  \n",
      "15  690873a0195dd4c6636f6bcf98d18d19d419e603268a3d...  \n",
      "16  4670708b7d0f2e741a6397f518921b630a6b91a67a96d9...  \n",
      "17  364a237f7cba05b069bee8c6a1054e5d8bf78bdbdb189e...  \n",
      "18  e768bf3eda7dffeaa89387396650e9206183a83a7a2cb6...  \n",
      "19  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "20  376f97bbd9cceed3b9f88be3aea686ec20826759df45ac...  \n",
      "21  dc17e872fe7bfe10149e37d265ab2bc485f9f6798dce79...  \n",
      "22  8734940804041e2df20ad8d0434db13c926ac56a3361e0...  \n",
      "23  881f0b3bd55fac152cd92092707570af34c86246ffe1f5...  \n",
      "24  76c470a3b4e996819762864767991b497a417e45be54e5...  \n",
      "25  83ea3f88c2ce742bc760a6d82e9777ccb8b6ea6e3f5ecd...  \n",
      "26  226fd14916a7377abc449b4c504336ab5b2cb9986e99e1...  \n",
      "27  721f807ed7e33469141926186f4bed9ca1813845a8ff18...  \n",
      "28  332d68596b80569706353ba450360c159bb25012c6420f...  \n",
      "29  8c04b043562fe037dde46590353e1ebaa7b6a97757d0a8...  \n"
     ]
    }
   ],
   "source": [
    "# random_korean_privacy_links\n",
    "# create a dataframe with random_korean_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_korean_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_korean_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_korean_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_korean_privacy_links.to_csv(\"policies/sample_policies_korean/sample_policies_korean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_korean_privacy_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Main Function\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom_korean_privacy_links\u001b[49m:\n\u001b[0;32m     29\u001b[0m     text_content \u001b[38;5;241m=\u001b[39m fetch_and_convert_website(link)\n\u001b[0;32m     30\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m df_random_korean_privacy_links[df_random_korean_privacy_links[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m link][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_korean_privacy_links' is not defined"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_korean_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_korean_privacy_links[df_random_korean_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ko\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_korean/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Websites Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank            domain\n",
      "0            1        google.com\n",
      "1            2      facebook.com\n",
      "2            3     microsoft.com\n",
      "3            4    googleapis.com\n",
      "4            5         apple.com\n",
      "...        ...               ...\n",
      "578155  578156  lurkerlounge.com\n",
      "578156  578157          528y.com\n",
      "578157  578158     garybartz.com\n",
      "578158  578159          fsagq.cn\n",
      "578159  578160    spottyd10.buzz\n",
      "\n",
      "[578160 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# read \"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\" into dataframe. the csv has no headers\n",
    "tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\", header=None, index_col=False)\n",
    "# tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\")\n",
    "\n",
    "# add headers to the dataframe\n",
    "tranco_df.columns = [\"rank\", \"domain\"]\n",
    "\n",
    "print(tranco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_domains(country_code):\n",
    "    db = TinyDB('websites_by_language.json')\n",
    "\n",
    "\n",
    "    websites_table = db.table('websites')\n",
    "    Website = Query()\n",
    "\n",
    "    korean_websites = websites_table.search(Website.language == country_code)\n",
    "\n",
    "    all_korean_websites = [website['url'] for website in korean_websites]\n",
    "    print(f\"Total {country_code} websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "    # # websites_to_process = korean_websites[:100]\n",
    "    # # websites_to_process = [website['url'] for website in websites_to_process]\n",
    "    # # print(websites_to_process)\n",
    "\n",
    "    # policy_links_table = db.table('policy_links')\n",
    "    # existing_policy_links = policy_links_table.all()\n",
    "    # # only korean domains which has policy_link['country'] == \"Korea\"\n",
    "\n",
    "    # if country_code == \"ko\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "    # elif country_code == \"zh-cn\" or country_code == \"zh\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "    # elif country_code == \"ja\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "\n",
    "    # print(f\"Number of {country_code} domains in policy links table: {len(set(existing_policy_links))}\")\n",
    "\n",
    "    # remaining_websites = []\n",
    "\n",
    "    # for each_website in all_korean_websites:\n",
    "    #     if each_website not in existing_policy_links:\n",
    "    #         remaining_websites.append(each_website)\n",
    "    # print(f\"Total remaining website {len(remaining_websites)}\")\n",
    "\n",
    "    return all_korean_websites\n",
    "\n",
    "    # return remaining_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total zh-cn websites: 16652\n",
      "Total zh-tw websites: 15\n",
      "16667\n"
     ]
    }
   ],
   "source": [
    "chinese_websites_to_process = get_all_domains(\"zh-cn\")\n",
    "mandarin_websites_to_process = get_all_domains(\"zh-tw\")\n",
    "chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese_websites_to_process = get_remaining_domains(\"zh-cn\")\n",
    "# mandarin_websites_to_process = get_remaining_domains(\"zh\")\n",
    "# chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "# print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# top3K_chinese_mandarin_domains store to file top3K_chinese_mandarin_domains.txt\n",
    "with open(\"top3K_chinese_mandarin_domains.txt\", \"w\") as f:\n",
    "    for domain in top3K_chinese_mandarin_domains:\n",
    "        f.write(domain + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank           domain\n",
      "64          65           qq.com\n",
      "316        317     bilibili.com\n",
      "1396      1397           360.cn\n",
      "1697      1698         amap.com\n",
      "1924      1925       alipay.com\n",
      "...        ...              ...\n",
      "326603  326604      dlwafuu.com\n",
      "326610  326611     zhengxsy.com\n",
      "326612  326613  huiqianbian.com\n",
      "326621  326622   sumflurish.com\n",
      "326622  326623    whdaiqian.com\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# find top 3000 chinese_mandarin_domains_to_process from tranco_df table.\n",
    "\n",
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "# only keep top 3000 domains\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "print(top3K_chinese_mandarin_domains)\n",
    "\n",
    "# only keep domains in a list\n",
    "# top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# print(len(top3K_chinese_mandarin_domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"China\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "522\n",
      "Number of domains with no privacy links: 761\n",
      "Number of domains with privacy links: 560\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "chinese_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            chinese_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(chinese_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://natfrp.cloud/policy/privacy#price\n",
      "https://www.yunzhijia.com/public/agreement/privacy.html\n",
      "https://depot.fenbi.com/fenbi-privacy/index.html\n",
      "https://e.vhall.com/v3/privacyUPo\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#special\n",
      "https://www.xgcartoon.com/privacy\n",
      "https://www.cfp.cn/help/privacy-policy#collapseThree\n",
      "https://www.ooopic.com/intro/about/privacyPolicy/\n",
      "https://kaspersky.com.cn/web-privacy-policy\n",
      "https://accounts.growingio.com/user-privacy\n",
      "https://www.horoscopetruth.com/privacy#cookie_policy\n",
      "https://e.vhall.com/v3/privacyPolicy\n",
      "http://www.testplus.cn/privacy\n",
      "https://corp.pcbaby.com.cn/privacyPolicy.html\n",
      "https://www.cfp.cn/help/privacy-policy\n",
      "https://help.fanli.com/a/about/privacy.html\n",
      "https://www.horoscopetruth.com/privacy\n",
      "https://juejin.im/privacy\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#contactus\n",
      "https://agreement.gaoxiaojob.com/docs/privacy-policy/\n",
      "https://natfrp.cloud/policy/privacy\n",
      "https://qweather.com/terms/privacy\n",
      "https://corp.pcbaby.com.cn/privacyPolicy.html\n",
      "https://teambition.com/privacy\n",
      "http://www.camera360.com/privacy_zh.html\n",
      "https://www.backchina.com/special/privacy/\n",
      "https://hsbc.com.cn/en-cn/help/mandatory-info/privacy-and-security/\n",
      "https://fun88asia8.com/cn/help/policy-privacy.htm\n",
      "https://dmttang.com/label/privacy.html\n",
      "https://pop800.com/privacy_policy.html\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from \n",
    "random_chinese_privacy_links = random.sample(chinese_privacy_links, 30)\n",
    "for link in random_chinese_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Chinese Sample Privacy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0   https://www.ooopic.com/intro/about/privacyPolicy/   \n",
      "1              https://ijinshan.com/function/privacy/   \n",
      "2                        https://gotokeep.com/privacy   \n",
      "3           https://www.babytree.com/app/privacy.html   \n",
      "4                https://www.sumy.org.cn/privacy.html   \n",
      "5              https://pop800.com/privacy_policy.html   \n",
      "6                          https://www.smm.cn/privacy   \n",
      "7                https://e.vhall.com/v3/privacyPolicy   \n",
      "8   https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "9              https://dmttang.com/label/privacy.html   \n",
      "10             https://www.horoscopetruth.com/privacy   \n",
      "11                https://lianxinapp.com/privacy.html   \n",
      "12           http://www.camera360.com/privacy_zh.html   \n",
      "13                https://freereceivesms.com/privacy/   \n",
      "14                http://www.sumy.org.cn/privacy.html   \n",
      "15        https://www.shanbay.com/help/about/privacy/   \n",
      "16                              https://pp.cn/privacy   \n",
      "17  https://ppio.work/privacy-policy-20240524-v1.html   \n",
      "18             https://www.dianxiaomi.com/privacy.htm   \n",
      "19        https://learnku.com/docs/guide/privacy/4994   \n",
      "20        https://zhibo8.com/web/privacyPolicyPc.html   \n",
      "21             https://pop800.com/privacy_policy.html   \n",
      "22                              https://pp.cn/privacy   \n",
      "23                 https://qweather.com/terms/privacy   \n",
      "24  https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "25  https://passport.migu.cn/portal/privacy/protoc...   \n",
      "26              https://www.bytem.com/privacy-policy/   \n",
      "27       https://mubu.com/mubu-simple-privacy-policy/   \n",
      "28  https://agreementservice.svs.nike.com.cn/rest/...   \n",
      "29   https://akspeedy.com/html/privacy-agreement.html   \n",
      "\n",
      "                                                 hash  \n",
      "0   8d962324b48bcb726942bf7680cdd5040194308634d07a...  \n",
      "1   d4a1e978f5dffa9582f54472aa18bdd66d1877e7b7e274...  \n",
      "2   a8f8b53118ef694ea6875d6ee92e2777f6c10d33a9ff1d...  \n",
      "3   0255d8eab3c9cca5fd508b0e53619dd182324876327466...  \n",
      "4   b2545fb9903fc3e5eb1017c0dcd4a45a7f6e4489304f4d...  \n",
      "5   8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "6   2183f58b06af982b21730f60cdc8c663635ee1c8b4fe7b...  \n",
      "7   e494cd8c6a67e37867349ac09a1f9213fbe7df0b61d8cd...  \n",
      "8   9b4f04e5f22470e874b919e7a32fcc099d2effac6568e5...  \n",
      "9   f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...  \n",
      "10  d269dc0aee86f1b3ec76c500799f621873caa7cbc4f138...  \n",
      "11  b5688f5362d3adc3c8f280b0e3a9f35b7ece9f3675717e...  \n",
      "12  5157c09ae62f74e520014e2ed663eb11d09d09e61e2ccb...  \n",
      "13  4d8146865fe3ad7eccb95832e3c9d41523bc632e57ea75...  \n",
      "14  110088c2194dfac2d6f06036caf9c406767390765e0643...  \n",
      "15  5b6bdf689f9d2032e5f568bf9fcf39ec00b3b5cc43ba19...  \n",
      "16  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "17  19a1e09a202516bf1bd71cc7b66f5fca56bac5a5fd39d6...  \n",
      "18  24b23e040b3fa35aae0499bd009090ea829c3775280326...  \n",
      "19  bd3d38f95ee2147c84a0884f90148bfeac6174362f6681...  \n",
      "20  887c0920886d2263a96ab93a7eec9f944f6251511bd30c...  \n",
      "21  8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "22  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "23  f9c7bf29552b15866235771d00876a0238df455031d876...  \n",
      "24  4d06405ba73ec8f20c172acb745d01bc2e448050340ac0...  \n",
      "25  e211de0e09e38a56f73a46dac3cfdda6e6beb89537410b...  \n",
      "26  acad12dcc280f6c44c42c109fb0bec4d9c82b4db3d8051...  \n",
      "27  d704126b9f686c3e070adba47cb94485c0f89eb723022d...  \n",
      "28  4c46cf5e8f005eea1c129536c1bf1ab26984b6ff27cde2...  \n",
      "29  299aa5a241ba54d526fcfa6ee7fa755315f331e02f6942...  \n"
     ]
    }
   ],
   "source": [
    "# random_chinese_privacy_links\n",
    "\n",
    "# create a dataframe with random_chinese_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_chinese_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_chinese_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_chinese_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_chinese_privacy_links.to_csv(\"policies/sample_policies_chinese/sample_policies_chinese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching the website: 403 Client Error: Forbidden for url: https://www.ooopic.com/intro/about/privacyPolicy/\n",
      "Error fetching the website: 403 Client Error: Forbidden for url: https://www.freereceivesms.com/privacy/\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_chinese_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_chinese_privacy_links[df_random_chinese_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        with open(f\"policies/sample_policies_chinese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Progress of Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Korean websites: 2730\n",
      "Number of Korean domains in policy links table: 2729, percentage: 0.9996336996336996\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "korean_websites = websites_table.search(Website.language == \"ko\")\n",
    "\n",
    "all_korean_websites = [website['url'] for website in korean_websites]\n",
    "print(f\"Total Korean websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "# websites_to_process = korean_websites[:100]\n",
    "# websites_to_process = [website['url'] for website in websites_to_process]\n",
    "# print(websites_to_process)\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "# only korean domains which has policy_link['country'] == \"Korea\"\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "\n",
    "print(f\"Number of Korean domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_korean_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_korean_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Japanese websites: 3380\n",
      "Number of Japan domains in policy links table: 3352, percentage: 0.991715976331361\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "all_japanese_websites = [website['url'] for website in japanese_website]\n",
    "print(f\"Total Japanese websites: {len(set(all_japanese_websites))}\")\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "print(f\"Number of Japan domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_japanese_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_japanese_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chinese websites: 3000\n",
      "Number of China domains in policy links table: 585, percentage: 0.195\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language2.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "# japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# # websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "print(f\"Total Chinese websites: 3000\")\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "\n",
    "print(f\"Number of China domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/3000}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "# for each_website in all_japanese_websites:\n",
    "#     if each_website not in existing_policy_links:\n",
    "#         remaining_websites.append(each_website)\n",
    "# print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Link Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_links_df_v1 = pd.read_csv(\"../policy_corpus/privacy_links_df.csv\")\n",
    "policy_links_df_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chinese_privacy_links_df = pd.DataFrame(columns=[\"domain\", \"privacy_link\", \"hash\", \"init_language\", \"valid_language\", \"text_content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "db2 = TinyDB('websites_by_language2.json')\n",
    "\n",
    "Website = Query()\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "policy_links_table2 = db.table('policy_links')\n",
    "# filter where country is China\n",
    "existing_policy_links = policy_links_table.search(Query().country == \"China\")\n",
    "existing_policy_links2 = policy_links_table2.search(Query().country == \"China\")\n",
    "\n",
    "\n",
    "for domain in existing_policy_links:\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            hash = hashlib.sha256(link.encode()).hexdigest()\n",
    "            chinese_privacy_links_df = chinese_privacy_links_df._append({\"domain\": domain['domain'], \"privacy_link\": link, \"hash\": hash, \"init_language\": domain['country'], \"valid_language\": \"\", \"text_content\": \"\"}, ignore_index=True)            \n",
    "\n",
    "\n",
    "for domain in existing_policy_links2:\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            hash = hashlib.sha256(link.encode()).hexdigest()\n",
    "            chinese_privacy_links_df = chinese_privacy_links_df._append({\"domain\": domain['domain'], \"privacy_link\": link, \"hash\": hash, \"init_language\": domain['country'], \"valid_language\": \"\", \"text_content\": \"\"}, ignore_index=True)            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>privacy_link</th>\n",
       "      <th>hash</th>\n",
       "      <th>init_language</th>\n",
       "      <th>valid_language</th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quark.cn</td>\n",
       "      <td>https://www.amazon.cn/gp/help/customer/display...</td>\n",
       "      <td>d817382b418886458aee75f492f3a35e44f0f9bb23d4b7...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yy.com</td>\n",
       "      <td>https://emuserh5.eastmoney.com/useragreement/yszc</td>\n",
       "      <td>b941b84035d3611a1500a8b61353559c0ff324f06ce5a6...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yy.com</td>\n",
       "      <td>https://docs.getui.com/privacy/</td>\n",
       "      <td>12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zuimeitianqi.com</td>\n",
       "      <td>https://new.qq.com/sv1/agreement/jubaoxuzhi.html</td>\n",
       "      <td>aac6ba34e1e6bf8c653d06da31c132dafdb06ee496a92f...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tudou.com</td>\n",
       "      <td>https://docs.getui.com/privacy/</td>\n",
       "      <td>12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>digitaling.com</td>\n",
       "      <td>https://www.digitaling.com/help/agreement</td>\n",
       "      <td>75e76eedc56647c7cb9b6bd34e06cd53b9e3e8e1e16043...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>vipleanda.com</td>\n",
       "      <td>https://dmttang.com/label/privacy.html</td>\n",
       "      <td>f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>buzixin.com</td>\n",
       "      <td>https://dmttang.com/label/privacy.html</td>\n",
       "      <td>f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>falv-tv.com</td>\n",
       "      <td>https://dmttang.com/label/privacy.html</td>\n",
       "      <td>f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>x1fu.com</td>\n",
       "      <td>https://dmttang.com/label/privacy.html</td>\n",
       "      <td>f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3214 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                domain                                       privacy_link  \\\n",
       "0             quark.cn  https://www.amazon.cn/gp/help/customer/display...   \n",
       "1               yy.com  https://emuserh5.eastmoney.com/useragreement/yszc   \n",
       "2               yy.com                    https://docs.getui.com/privacy/   \n",
       "3     zuimeitianqi.com   https://new.qq.com/sv1/agreement/jubaoxuzhi.html   \n",
       "4            tudou.com                    https://docs.getui.com/privacy/   \n",
       "...                ...                                                ...   \n",
       "3209    digitaling.com          https://www.digitaling.com/help/agreement   \n",
       "3210     vipleanda.com             https://dmttang.com/label/privacy.html   \n",
       "3211       buzixin.com             https://dmttang.com/label/privacy.html   \n",
       "3212       falv-tv.com             https://dmttang.com/label/privacy.html   \n",
       "3213          x1fu.com             https://dmttang.com/label/privacy.html   \n",
       "\n",
       "                                                   hash init_language  \\\n",
       "0     d817382b418886458aee75f492f3a35e44f0f9bb23d4b7...         China   \n",
       "1     b941b84035d3611a1500a8b61353559c0ff324f06ce5a6...         China   \n",
       "2     12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...         China   \n",
       "3     aac6ba34e1e6bf8c653d06da31c132dafdb06ee496a92f...         China   \n",
       "4     12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...         China   \n",
       "...                                                 ...           ...   \n",
       "3209  75e76eedc56647c7cb9b6bd34e06cd53b9e3e8e1e16043...         China   \n",
       "3210  f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...         China   \n",
       "3211  f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...         China   \n",
       "3212  f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...         China   \n",
       "3213  f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...         China   \n",
       "\n",
       "     valid_language text_content  \n",
       "0                                 \n",
       "1                                 \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 \n",
       "...             ...          ...  \n",
       "3209                              \n",
       "3210                              \n",
       "3211                              \n",
       "3212                              \n",
       "3213                              \n",
       "\n",
       "[3214 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_privacy_links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_language\n",
      "China    3214\n",
      "Name: count, dtype: int64\n",
      "561\n"
     ]
    }
   ],
   "source": [
    "print(chinese_privacy_links_df['init_language'].value_counts())\n",
    "\n",
    "print(len(chinese_privacy_links_df['domain'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>microsoft.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>googleapis.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>apple.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578155</th>\n",
       "      <td>578156</td>\n",
       "      <td>lurkerlounge.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578156</th>\n",
       "      <td>578157</td>\n",
       "      <td>528y.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578157</th>\n",
       "      <td>578158</td>\n",
       "      <td>garybartz.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578158</th>\n",
       "      <td>578159</td>\n",
       "      <td>fsagq.cn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578159</th>\n",
       "      <td>578160</td>\n",
       "      <td>spottyd10.buzz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          rank            domain\n",
       "0            1        google.com\n",
       "1            2      facebook.com\n",
       "2            3     microsoft.com\n",
       "3            4    googleapis.com\n",
       "4            5         apple.com\n",
       "...        ...               ...\n",
       "578155  578156  lurkerlounge.com\n",
       "578156  578157          528y.com\n",
       "578157  578158     garybartz.com\n",
       "578158  578159          fsagq.cn\n",
       "578159  578160    spottyd10.buzz\n",
       "\n",
       "[578160 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webRanking_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\", header=None, index_col=False)\n",
    "# tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\")\n",
    "\n",
    "# add headers to the dataframe\n",
    "webRanking_df.columns = [\"rank\", \"domain\"]\n",
    "webRanking_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every domain in chinese_privacy_links_df find its rank in WebRanking_df using domain value and add a new column to chinese_privacy_links_df\n",
    "chinese_privacy_links_df['rank'] = chinese_privacy_links_df['domain'].map(webRanking_df.set_index('domain')['rank'])\n",
    "\n",
    "# sort chinese_privacy_links_df by rank column and reset index\n",
    "chinese_privacy_links_df = chinese_privacy_links_df.sort_values(by='rank').reset_index(drop=True)\n",
    "# chinese_privacy_links_df index reset\n",
    "# chinese_privacy_links_df = chinese_privacy_links_df.reset_index(drop=True)\n",
    "\n",
    "# keep only 1000 rows from top based on top ranking\n",
    "chinese_privacy_links_df = chinese_privacy_links_df[:1000]\n",
    "# chinese_privacy_links_df save to policy_corpus\n",
    "chinese_privacy_links_df.to_csv(\"../policy_corpus/chinese_privacy_links_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>privacy_link</th>\n",
       "      <th>hash</th>\n",
       "      <th>init_language</th>\n",
       "      <th>valid_language</th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.us-onlinestore.com</td>\n",
       "      <td>https://www.us-onlinestore.com/law.html#area-p...</td>\n",
       "      <td>9ac6380d1d17249bf50dd2f9e43c545b25ea820e2c28e5...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enhance.co.jp</td>\n",
       "      <td>https://enhance.co.jp/privacy-policy/</td>\n",
       "      <td>0fb0f105dfa22b83c4c15d0665ce7c97f7c9577c225b5b...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rizapgroup.com</td>\n",
       "      <td>https://rizapgroup.com/privacy</td>\n",
       "      <td>1de0c851c94d9d92fc1ec03635321420fb60c159661ec1...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asajikan.jp</td>\n",
       "      <td>https://asajikan.jp/privacy</td>\n",
       "      <td>724587d0bdf7fa450cb3bf22b397ced291053b4db05377...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mocom.tv</td>\n",
       "      <td>https://mocom.tv/payment_policy.phtml</td>\n",
       "      <td>2e8b98375e71893f56667adcd62d32256b05dbf126571d...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>nsfwkr.net</td>\n",
       "      <td>https://nsfwkr.net/bbs/register.php#privacy</td>\n",
       "      <td>753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>gck99.com.tw</td>\n",
       "      <td>https://gck99.com.tw/privacy.php</td>\n",
       "      <td>1f30698e7b51975a02398b3b2348c99ccb113797a52a59...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>hk-bestcasino.com</td>\n",
       "      <td>https://hk-bestcasino.com/privacy-policy/</td>\n",
       "      <td>4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>nsfwkr.net</td>\n",
       "      <td>https://nsfwkr.net/bbs/page.php?hid=privacy</td>\n",
       "      <td>c1ad8e75c78c554eda26b8c1c577dc13d76132587ca4a6...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>bj.58.com</td>\n",
       "      <td>https://bj.58.com/caishui/54996963653044x.shtm...</td>\n",
       "      <td>953a4aaf51075d3bb37f3ae1053f1a89cb0f646b413416...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6750 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain  \\\n",
       "0     www.us-onlinestore.com   \n",
       "1              enhance.co.jp   \n",
       "2             rizapgroup.com   \n",
       "3                asajikan.jp   \n",
       "4                   mocom.tv   \n",
       "...                      ...   \n",
       "6745              nsfwkr.net   \n",
       "6746            gck99.com.tw   \n",
       "6747       hk-bestcasino.com   \n",
       "6748              nsfwkr.net   \n",
       "6749               bj.58.com   \n",
       "\n",
       "                                           privacy_link  \\\n",
       "0     https://www.us-onlinestore.com/law.html#area-p...   \n",
       "1                 https://enhance.co.jp/privacy-policy/   \n",
       "2                        https://rizapgroup.com/privacy   \n",
       "3                           https://asajikan.jp/privacy   \n",
       "4                 https://mocom.tv/payment_policy.phtml   \n",
       "...                                                 ...   \n",
       "6745        https://nsfwkr.net/bbs/register.php#privacy   \n",
       "6746                   https://gck99.com.tw/privacy.php   \n",
       "6747          https://hk-bestcasino.com/privacy-policy/   \n",
       "6748        https://nsfwkr.net/bbs/page.php?hid=privacy   \n",
       "6749  https://bj.58.com/caishui/54996963653044x.shtm...   \n",
       "\n",
       "                                                   hash init_language  \\\n",
       "0     9ac6380d1d17249bf50dd2f9e43c545b25ea820e2c28e5...         Japan   \n",
       "1     0fb0f105dfa22b83c4c15d0665ce7c97f7c9577c225b5b...         Japan   \n",
       "2     1de0c851c94d9d92fc1ec03635321420fb60c159661ec1...         Japan   \n",
       "3     724587d0bdf7fa450cb3bf22b397ced291053b4db05377...         Japan   \n",
       "4     2e8b98375e71893f56667adcd62d32256b05dbf126571d...         Japan   \n",
       "...                                                 ...           ...   \n",
       "6745  753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...        Korean   \n",
       "6746  1f30698e7b51975a02398b3b2348c99ccb113797a52a59...        Korean   \n",
       "6747  4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...        Korean   \n",
       "6748  c1ad8e75c78c554eda26b8c1c577dc13d76132587ca4a6...        Korean   \n",
       "6749  953a4aaf51075d3bb37f3ae1053f1a89cb0f646b413416...        Korean   \n",
       "\n",
       "      valid_language  text_content  \n",
       "0                NaN           NaN  \n",
       "1                NaN           NaN  \n",
       "2                NaN           NaN  \n",
       "3                NaN           NaN  \n",
       "4                NaN           NaN  \n",
       "...              ...           ...  \n",
       "6745             NaN           NaN  \n",
       "6746             NaN           NaN  \n",
       "6747             NaN           NaN  \n",
       "6748             NaN           NaN  \n",
       "6749             NaN           NaN  \n",
       "\n",
       "[6750 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# policy_links_df_v1 = pd.read_csv(\"../policy_corpus/privacy_links_df.csv\")\n",
    "# policy_links_df_v1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
