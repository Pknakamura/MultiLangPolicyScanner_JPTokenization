{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "      ------------------------------------ 20.5/981.5 kB 330.3 kB/s eta 0:00:03\n",
      "     - ----------------------------------- 41.0/981.5 kB 393.8 kB/s eta 0:00:03\n",
      "     --- -------------------------------- 102.4/981.5 kB 737.3 kB/s eta 0:00:02\n",
      "     ------------ ------------------------- 317.4/981.5 kB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 593.9/981.5 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  962.6/981.5 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=6acdefae4860648e655464f0475bc9a66530c5f1a31ca01ac662ad6bd9e72780\n",
      "  Stored in directory: c:\\users\\mhass\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB of Websites by Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63926, 'zh-cn': 16652, 'id': 8471, 'ru': 4302, 'es': 3924, 'de': 3500, 'ja': 3380, 'pt': 3346, 'ko': 2730, 'fr': 2618, 'vi': 1817, 'it': 1497, 'tr': 1457, 'nl': 1152, 'pl': 1105, 'ar': 1085, 'fa': 1067, 'th': 968, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 336, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 242, 'bn': 228, 'sk': 206, 'so': 171, 'he': 169, 'lt': 131, 'sl': 108, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'el', 'he', 'sw', 'zh-cn', 'et', 'sl', 'nl', 'it', 'cs', 'es', 'te', 'ja', 'ta', 'de', 'ko', 'kn', 'ru', 'ro', 'ar', 'da', 'ca', 'hu', 'fi', 'tl', 'lt', 'hr', 'so', 'pl', 'pt', 'id', 'hi', 'ne', 'sq', 'ml', 'th', 'bg', 'mk', 'sv', 'af', 'ur', 'lv', 'fa', 'tr', 'fr', 'no', 'zh-tw', 'vi', 'en', 'sk', 'cy', 'pa', 'bn', 'gu', 'uk', 'mr'}\n",
      "Total number of unique languages: 55\n"
     ]
    }
   ],
   "source": [
    "# list all unique languages on a new line and total number of unique languages\n",
    "unique_languages = set(languages)\n",
    "print(unique_languages)\n",
    "print(f\"Total number of unique languages: {len(unique_languages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total websites in English: 63926\n",
      "Total websites in Chinese(simplified): 16652\n",
      "Total websites in Chinese(traditional): 15\n",
      "Total websites in Korean: 2730\n",
      "Total websites in Japanese: 3380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese(simplified): {len(websites)}\")\n",
    "\n",
    "# zh-tw\n",
    "websites = websites_table.search(Website.language == \"zh-tw\")\n",
    "print(f\"Total websites in Chinese(traditional): {len(websites)}\")\n",
    "\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Random Websites from the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 random domains for eng, zh-cn, ko, ja languages\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "# get 25 random websites\n",
    "random_en_websites = random.sample(websites, 25)\n",
    "print(\"Random websites in English:\")\n",
    "for website in random_en_websites:\n",
    "    print(website['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Chinese(simplified):\n",
      "znhnj.com\n",
      "c7575tp.com\n",
      "zgbainian.com\n",
      "vzvmpqzu.cn\n",
      "jzyy365.com\n",
      "ayhnfs.com\n",
      "wenliang2019.com\n",
      "qczxyn.com\n",
      "tvywf.com\n",
      "taupal.com\n",
      "tengfei2020.cn\n",
      "hongtaihy888.com\n",
      "307985.com\n",
      "lnjianyuan.com\n",
      "360kuai.com\n",
      "zhxrty.com\n",
      "hzgt26.com\n",
      "fslianyin.com\n",
      "szzhuokong.com\n",
      "yuexianghutong.com\n",
      "gzyclkj.com\n",
      "inshenke.com\n",
      "xsys14.com\n",
      "usbbk.com\n",
      "fc2id.com\n",
      "Random websites in Chinese(simplified):\n",
      "010cfzx.com\n",
      "szyunlai.com\n",
      "rh258.cn\n",
      "wxyege.com\n",
      "hzlvbang.com\n",
      "lrf1688.com\n",
      "douqu02.com\n",
      "cqbaipiao.com\n",
      "gsxnj.cn\n",
      "bjztjhjy.com\n",
      "weiyehj.com\n",
      "zhengyujz.com\n",
      "shxflt.com\n",
      "jishangliu.com\n",
      "fantanggame.com\n",
      "yuhe-investment.com\n",
      "517ygyw.com\n",
      "jiashiscreen.com\n",
      "zyzjpf.com\n",
      "ssswu168.com\n",
      "duoduojupin.com\n",
      "jsrongbao.com\n",
      "scwlk.com\n",
      "cloudlakenet.com\n",
      "hxdcost.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "# get 25 random websites\n",
    "random_ZhCn_websites1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for zh-cn language which is different from the previous 25 websites in random_ZhCn_websites1\n",
    "random_ZhCn_websites2 = random.sample(websites, 25)\n",
    "# check random_ZhCn_websites2 is different from random_ZhCn_websites1\n",
    "for website in random_ZhCn_websites2:\n",
    "    if website in random_ZhCn_websites1:\n",
    "        print(\"Random websites in Chinese(simplified) are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ZhCn_websites2.remove(website)\n",
    "        random_ZhCn_websites2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean:\n",
      "banxianovle.com\n",
      "tzosk.cn\n",
      "comicabc.com\n",
      "hnsmall.com\n",
      "ppt.cc\n",
      "kcycle.or.kr\n",
      "ace.io\n",
      "pmangvegasmoney.com\n",
      "sokoyo-mj.com\n",
      "juyuejiapin.com\n",
      "zztaoji.com\n",
      "hle.com.tw\n",
      "tlogcorp.com\n",
      "minbozy.com\n",
      "baokefangzhi.com\n",
      "njwucheng.com\n",
      "huiyupom.com\n",
      "mindsopen.com.tw\n",
      "nmg.com.hk\n",
      "qlrc.com\n",
      "hbtianzhi.com\n",
      "daehangreenpower.com\n",
      "dox87.com\n",
      "gxsaiying.com\n",
      "largeal.com\n",
      "Random websites in Korean are not unique\n",
      "Random websites in Korean:\n",
      "wtwt227.com\n",
      "mysealib.com\n",
      "ifreesite.com\n",
      "tianfuents.com\n",
      "bqg996.com\n",
      "hamimall.com.tw\n",
      "fxfx207.com\n",
      "515ym.com\n",
      "codeup.kr\n",
      "zggongchengjishuzixun.com\n",
      "latimes.kr\n",
      "fjmeixin.com\n",
      "sinronlee.kr\n",
      "xcy5551.com\n",
      "hongwha21.net\n",
      "075367.com\n",
      "sguwl.com\n",
      "koreatech.ac.kr\n",
      "xxymask.com\n",
      "kcar.com\n",
      "mt-guide01.com\n",
      "amghbwp.cn\n",
      "redvi63.com\n",
      "hlbam13.com\n",
      "996wap.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "# get 25 random websites\n",
    "random__ko_websites_1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for korean language which is different from the previous 25 websites in random__ko_websites_1\n",
    "random__ko_websites_2 = random.sample(websites, 25)\n",
    "# check random__ko_websites_2 is different from random__ko_websites_1\n",
    "for website in random__ko_websites_2:\n",
    "    if website in random__ko_websites_1:\n",
    "        print(\"Random websites in Korean are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random__ko_websites_2.remove(website)\n",
    "        random__ko_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_2:\n",
    "    print(website['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean: [{'url': 'yxzwlkj.com', 'language': 'ko', 'timestamp': '2024-06-08T08:34:31.376923'}, {'url': 'jshaoou.com', 'language': 'ko', 'timestamp': '2024-06-08T07:06:15.695528'}]\n"
     ]
    }
   ],
   "source": [
    "# give two more random korean websites\n",
    "random__ko_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Korean:\", random__ko_websites_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359198.com\n",
      "toonkor326.com\n",
      "dbcnews.co.kr\n",
      "hbwocheng.com\n",
      "whichav.video\n",
      "pinksisly.com\n",
      "chenzhongtech.com\n",
      "bluezz.com.tw\n",
      "gdzhukou.com\n",
      "oplove16.com\n",
      "yamoa3.site\n",
      "zhongfa1688.com\n",
      "douyuanxiuhe.com\n",
      "yp.com.hk\n",
      "imendon.com\n",
      "fxfx217.com\n",
      "htwhbook.com\n",
      "yedam.com\n",
      "homeplus.co.kr\n",
      "newhua99.xyz\n",
      "88p2p.com\n",
      "shyuwangfangshui.com\n",
      "fenghemp.com\n",
      "daoom.co.kr\n",
      "nfqlife.com\n",
      "evolutionplaynow.com\n",
      "limeitianhe.com\n",
      "yebigun1.mil.kr\n",
      "anpservice.net\n",
      "trfsgs.com\n"
     ]
    }
   ],
   "source": [
    "# give 30 random websites from the database for korean, chinese and japanese languages\n",
    "# get 30 random websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "random_websites = random.sample(websites, 30)\n",
    "# just list urls\n",
    "for website in random_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bu-light.com\n",
      "calulu-dogwear.jp\n",
      "jrelife.jp\n",
      "ksb.co.jp\n",
      "costcotuu.com\n",
      "nissui-kenko.com\n",
      "adalysis.tech\n",
      "kagukuro.com\n",
      "eco-ring.com\n",
      "blogoon.net\n",
      "zero.jp\n",
      "sunmold.com\n",
      "kodomotokurashi.com\n",
      "spimo.net\n",
      "venus-walker.com\n",
      "webike.net\n",
      "sublimestore.jp\n",
      "sustaina-mall.com\n",
      "d-arms-shop.jp\n",
      "crx7601.com\n",
      "cosp.jp\n",
      "miyaji.co.jp\n",
      "194964.com\n",
      "mediamix.ne.jp\n",
      "estnation.co.jp\n",
      "Random websites in Japanese:\n",
      "geo-mobile.jp\n",
      "branshes.jp\n",
      "stransa.co.jp\n",
      "belllabell.com\n",
      "regza.com\n",
      "ko-co.jp\n",
      "harmony-products.jp\n",
      "press-crew.com\n",
      "tokyo-recycle.net\n",
      "enjoytokyo.jp\n",
      "sbux.jp\n",
      "videomarket.jp\n",
      "nogikoi.jp\n",
      "zakzak.co.jp\n",
      "it-hojo.jp\n",
      "paidy.com\n",
      "movie-rush.com\n",
      "signmall.jp\n",
      "entrevida.com\n",
      "golfsapuri.com\n",
      "rekisiru.com\n",
      "sharousi-kakomon.com\n",
      "supleks.jp\n",
      "mamoru-k.com\n",
      "lordsofchaos.jp\n"
     ]
    }
   ],
   "source": [
    "# get 25 random websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "random_ja_websites = random.sample(websites, 25)\n",
    "# just list urls\n",
    "for website in random_ja_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "# get 25 more random japanese websites\n",
    "random_ja_websites_2 = random.sample(websites, 25)\n",
    "# check random_ja_websites_2 is different from random_ja_websites\n",
    "for website in random_ja_websites_2:\n",
    "    if website in random_ja_websites:\n",
    "        # print(\"Random websites in Japanese are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ja_websites_2.remove(website)\n",
    "        random_ja_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Japanese:\")\n",
    "for website in random_ja_websites_2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Japanese: [{'url': 'comiful.net', 'language': 'ja', 'timestamp': '2024-06-08T09:51:24.302366'}, {'url': 'hentaiasmr.moe', 'language': 'ja', 'timestamp': '2024-06-08T02:31:00.175990'}]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 more random japanese websites\n",
    "random_ja_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Japanese:\", random_ja_websites_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of user agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# get free proxies from online\n",
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # fetch proxy list from online\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})\n",
    "\n",
    "\n",
    "    # print(proxy_table)\n",
    "\n",
    "    # # Extract proxy IPs and ports\n",
    "    proxies = []\n",
    "    proxy_table = proxy_table.find(\"tbody\")\n",
    "    # print(proxy_table)\n",
    "    for row in proxy_table.find_all(\"tr\"):\n",
    "        proxies.append({\n",
    "        \"ip\":   row.find_all(\"td\")[0].string,\n",
    "        \"port\": row.find_all(\"td\")[1].string\n",
    "        })\n",
    "        # print(row)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "recursive_extract() missing 1 required positional argument: 'original_domain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m start_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.naver.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the starting URL\u001b[39;00m\n\u001b[0;32m     58\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaver.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mrecursive_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_domain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll extracted links:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m all_links:\n",
      "Cell \u001b[1;32mIn[75], line 53\u001b[0m, in \u001b[0;36mrecursive_extract\u001b[1;34m(url, original_domain)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m link \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_links:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# if original_domain in link:\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         all_links\u001b[38;5;241m.\u001b[39mappend(link)\n\u001b[1;32m---> 53\u001b[0m         \u001b[43mrecursive_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: recursive_extract() missing 1 required positional argument: 'original_domain'"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://www.naver.com/',  # Replace with your URLs\n",
    "    'https://www.997788.com/',\t\n",
    "]\n",
    "\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Add a list of user agents here\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    # proxies = get_proxies()\n",
    "    # proxy = random.choice(proxies)\n",
    "    # print(f\"Using proxy: {proxy}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        if original_domain in url:\n",
    "            links = extract_links(url)\n",
    "        for link in links:\n",
    "            if link not in visited_links:\n",
    "                # if original_domain in link:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    original_domain = \"naver.com\"\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_links2 =all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original domain: www.naver.com, Start URL: https://www.naver.com/\n",
      "Extracting links from: https://www.naver.com/\n",
      "All extracted links:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        print(f\"Extracting links from: {url}\")\n",
    "        links = extract_links(url)\n",
    "        for link in links:\n",
    "            parsed_link = urlparse(link)\n",
    "            if link not in visited_links:\n",
    "                # check if original domain is in the link\n",
    "                if original_domain in parsed_link.netloc:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    original_domain = parsed_start_url.netloc\n",
    "    \n",
    "    print(f\"Original domain: {original_domain}, Start URL: {start_url}\")\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
