{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "      ------------------------------------ 20.5/981.5 kB 330.3 kB/s eta 0:00:03\n",
      "     - ----------------------------------- 41.0/981.5 kB 393.8 kB/s eta 0:00:03\n",
      "     --- -------------------------------- 102.4/981.5 kB 737.3 kB/s eta 0:00:02\n",
      "     ------------ ------------------------- 317.4/981.5 kB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 593.9/981.5 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  962.6/981.5 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=6acdefae4860648e655464f0475bc9a66530c5f1a31ca01ac662ad6bd9e72780\n",
      "  Stored in directory: c:\\users\\mhass\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63882, 'zh-cn': 16642, 'id': 8465, 'ru': 4299, 'es': 3921, 'de': 3499, 'ja': 3375, 'pt': 3345, 'ko': 2727, 'fr': 2615, 'vi': 1817, 'it': 1497, 'tr': 1456, 'nl': 1149, 'pl': 1103, 'ar': 1084, 'fa': 1066, 'th': 967, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 334, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 241, 'bn': 228, 'sk': 205, 'so': 171, 'he': 169, 'lt': 130, 'sl': 107, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n",
      "Total websites in English: 63883\n",
      "Total websites in Chinese: 16642\n",
      "Total websites in Korean: 2727\n",
      "Total websites in Japanese: 3375\n",
      "[{'url': 'express.pk', 'language': 'ur', 'timestamp': '2024-06-08T03:33:17.411030'}, {'url': 'banuri.edu.pk', 'language': 'ur', 'timestamp': '2024-06-08T03:47:45.194243'}, {'url': 'alafaf.net', 'language': 'ur', 'timestamp': '2024-06-08T06:21:37.792315'}, {'url': 'dailypakistan.com.pk', 'language': 'ur', 'timestamp': '2024-06-08T07:23:38.145698'}, {'url': 'real-madrid.ir', 'language': 'ur', 'timestamp': '2024-06-08T08:11:13.102494'}, {'url': 'ummat.net', 'language': 'ur', 'timestamp': '2024-06-08T09:34:57.735313'}, {'url': 'aaj.tv', 'language': 'ur', 'timestamp': '2024-06-08T11:32:30.603010'}, {'url': 'nawaiwaqt.com.pk', 'language': 'ur', 'timestamp': '2024-06-08T13:16:22.684826'}, {'url': 'humnews.pk', 'language': 'ur', 'timestamp': '2024-06-08T16:06:20.918828'}, {'url': 'upera.tv', 'language': 'ur', 'timestamp': '2024-06-08T17:36:34.191293'}, {'url': 'courts.gov.ps', 'language': 'ur', 'timestamp': '2024-06-08T19:38:12.772151'}, {'url': 'shoppingish.pk', 'language': 'ur', 'timestamp': '2024-06-08T20:13:04.066436'}, {'url': 'independenturdu.com', 'language': 'ur', 'timestamp': '2024-06-08T21:31:00.150861'}, {'url': 'karachigo.com', 'language': 'ur', 'timestamp': '2024-06-08T22:23:32.956375'}, {'url': 'urdunews.com', 'language': 'ur', 'timestamp': '2024-06-09T01:13:14.432560'}, {'url': 'urduleaks.com', 'language': 'ur', 'timestamp': '2024-06-09T03:04:46.599691'}]\n"
     ]
    }
   ],
   "source": [
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag\n",
    "\n",
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "# Print all entries\n",
    "# for item in websites_table.all():\n",
    "#     print(item)\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n",
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese: {len(websites)}\")\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n",
    "# print names of urdu website using ur \n",
    "websites = websites_table.search(Website.language == \"ur\")\n",
    "print(websites)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_list_from_file(file_path):\n",
    "    if 'cloudflare' in file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df['domain'].tolist()\n",
    "\n",
    "    if 'ahref' in file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df['url'].tolist()\n",
    "    \n",
    "    if 'tranco' in file_path:\n",
    "        # there is no header in the file\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        return df[1].tolist()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        url_list = file.readlines()\n",
    "    return [url.strip() for url in url_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs before filtering: 578180\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 2947: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m error_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_urls.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 20\u001b[0m     error_urls \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#  remaining urls = total urls - error urls - urls from tinydb\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# get all urls from tinydb\u001b[39;00m\n\u001b[0;32m     24\u001b[0m all_urls \u001b[38;5;241m=\u001b[39m websites_table\u001b[38;5;241m.\u001b[39mall()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 2947: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "url_list = [\n",
    "        # 'https://www.example.com',\n",
    "        # 'https://www.anotherexample.com',\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "\n",
    "# List all files in the data folder\n",
    "for file in os.listdir(\"../data\"):\n",
    "    file_list = create_url_list_from_file(\"../data/\" + file)\n",
    "    url_list.extend(file_list)\n",
    "    # print(file)\n",
    "\n",
    "# Remove duplicates\n",
    "url_list = list(set(url_list))\n",
    "print(f\"Total URLs before filtering: {len(url_list)}\")\n",
    "\n",
    "# read error urls from file\n",
    "error_urls = []\n",
    "with open(\"error_urls.txt\", \"r\") as file:\n",
    "    error_urls = file.readlines()\n",
    "\n",
    "#  remaining urls = total urls - error urls - urls from tinydb\n",
    "# get all urls from tinydb\n",
    "all_urls = websites_table.all()\n",
    "all_urls = [url['url'] for url in all_urls]\n",
    "print(f\"Total URLs in TinyDB: {len(all_urls)}\")\n",
    "\n",
    "# remaining urls\n",
    "remaining_urls = list(set(url_list) - set(error_urls) - set(all_urls))\n",
    "print(f\"Total URLs after filtering: {len(remaining_urls)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
