{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "      ------------------------------------ 20.5/981.5 kB 330.3 kB/s eta 0:00:03\n",
      "     - ----------------------------------- 41.0/981.5 kB 393.8 kB/s eta 0:00:03\n",
      "     --- -------------------------------- 102.4/981.5 kB 737.3 kB/s eta 0:00:02\n",
      "     ------------ ------------------------- 317.4/981.5 kB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------- --------------- 593.9/981.5 kB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  962.6/981.5 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993254 sha256=6acdefae4860648e655464f0475bc9a66530c5f1a31ca01ac662ad6bd9e72780\n",
      "  Stored in directory: c:\\users\\mhass\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB of Websites by Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63926, 'zh-cn': 16652, 'id': 8471, 'ru': 4302, 'es': 3924, 'de': 3500, 'ja': 3380, 'pt': 3346, 'ko': 2730, 'fr': 2618, 'vi': 1817, 'it': 1497, 'tr': 1457, 'nl': 1152, 'pl': 1105, 'ar': 1085, 'fa': 1067, 'th': 968, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 336, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 242, 'bn': 228, 'sk': 206, 'so': 171, 'he': 169, 'lt': 131, 'sl': 108, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sw', 'et', 'id', 'te', 'af', 'ca', 'ru', 'cy', 'bg', 'hi', 'so', 'de', 'gu', 'ml', 'no', 'sl', 'ja', 'zh-cn', 'ne', 'mk', 'fi', 'fr', 'ko', 'uk', 'pl', 'it', 'es', 'he', 'sq', 'th', 'ta', 'zh-tw', 'mr', 'kn', 'tr', 'vi', 'da', 'pt', 'en', 'lt', 'hr', 'ro', 'sv', 'cs', 'bn', 'tl', 'sk', 'pa', 'fa', 'nl', 'ur', 'lv', 'hu', 'el', 'ar'}\n",
      "Total number of unique languages: 55\n"
     ]
    }
   ],
   "source": [
    "# list all unique languages on a new line and total number of unique languages\n",
    "unique_languages = set(languages)\n",
    "print(unique_languages)\n",
    "print(f\"Total number of unique languages: {len(unique_languages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total websites in English: 63926\n",
      "Total websites in Chinese(simplified): 16652\n",
      "Total websites in Chinese(traditional): 15\n",
      "Total websites in Korean: 2730\n",
      "Total websites in Japanese: 3380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese(simplified): {len(websites)}\")\n",
    "\n",
    "# zh-tw\n",
    "websites = websites_table.search(Website.language == \"zh-tw\")\n",
    "print(f\"Total websites in Chinese(traditional): {len(websites)}\")\n",
    "\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Random Websites from the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 random domains for eng, zh-cn, ko, ja languages\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "# get 25 random websites\n",
    "random_en_websites = random.sample(websites, 25)\n",
    "print(\"Random websites in English:\")\n",
    "for website in random_en_websites:\n",
    "    print(website['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Chinese(simplified):\n",
      "znhnj.com\n",
      "c7575tp.com\n",
      "zgbainian.com\n",
      "vzvmpqzu.cn\n",
      "jzyy365.com\n",
      "ayhnfs.com\n",
      "wenliang2019.com\n",
      "qczxyn.com\n",
      "tvywf.com\n",
      "taupal.com\n",
      "tengfei2020.cn\n",
      "hongtaihy888.com\n",
      "307985.com\n",
      "lnjianyuan.com\n",
      "360kuai.com\n",
      "zhxrty.com\n",
      "hzgt26.com\n",
      "fslianyin.com\n",
      "szzhuokong.com\n",
      "yuexianghutong.com\n",
      "gzyclkj.com\n",
      "inshenke.com\n",
      "xsys14.com\n",
      "usbbk.com\n",
      "fc2id.com\n",
      "Random websites in Chinese(simplified):\n",
      "010cfzx.com\n",
      "szyunlai.com\n",
      "rh258.cn\n",
      "wxyege.com\n",
      "hzlvbang.com\n",
      "lrf1688.com\n",
      "douqu02.com\n",
      "cqbaipiao.com\n",
      "gsxnj.cn\n",
      "bjztjhjy.com\n",
      "weiyehj.com\n",
      "zhengyujz.com\n",
      "shxflt.com\n",
      "jishangliu.com\n",
      "fantanggame.com\n",
      "yuhe-investment.com\n",
      "517ygyw.com\n",
      "jiashiscreen.com\n",
      "zyzjpf.com\n",
      "ssswu168.com\n",
      "duoduojupin.com\n",
      "jsrongbao.com\n",
      "scwlk.com\n",
      "cloudlakenet.com\n",
      "hxdcost.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "# get 25 random websites\n",
    "random_ZhCn_websites1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for zh-cn language which is different from the previous 25 websites in random_ZhCn_websites1\n",
    "random_ZhCn_websites2 = random.sample(websites, 25)\n",
    "# check random_ZhCn_websites2 is different from random_ZhCn_websites1\n",
    "for website in random_ZhCn_websites2:\n",
    "    if website in random_ZhCn_websites1:\n",
    "        print(\"Random websites in Chinese(simplified) are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ZhCn_websites2.remove(website)\n",
    "        random_ZhCn_websites2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean:\n",
      "banxianovle.com\n",
      "tzosk.cn\n",
      "comicabc.com\n",
      "hnsmall.com\n",
      "ppt.cc\n",
      "kcycle.or.kr\n",
      "ace.io\n",
      "pmangvegasmoney.com\n",
      "sokoyo-mj.com\n",
      "juyuejiapin.com\n",
      "zztaoji.com\n",
      "hle.com.tw\n",
      "tlogcorp.com\n",
      "minbozy.com\n",
      "baokefangzhi.com\n",
      "njwucheng.com\n",
      "huiyupom.com\n",
      "mindsopen.com.tw\n",
      "nmg.com.hk\n",
      "qlrc.com\n",
      "hbtianzhi.com\n",
      "daehangreenpower.com\n",
      "dox87.com\n",
      "gxsaiying.com\n",
      "largeal.com\n",
      "Random websites in Korean are not unique\n",
      "Random websites in Korean:\n",
      "wtwt227.com\n",
      "mysealib.com\n",
      "ifreesite.com\n",
      "tianfuents.com\n",
      "bqg996.com\n",
      "hamimall.com.tw\n",
      "fxfx207.com\n",
      "515ym.com\n",
      "codeup.kr\n",
      "zggongchengjishuzixun.com\n",
      "latimes.kr\n",
      "fjmeixin.com\n",
      "sinronlee.kr\n",
      "xcy5551.com\n",
      "hongwha21.net\n",
      "075367.com\n",
      "sguwl.com\n",
      "koreatech.ac.kr\n",
      "xxymask.com\n",
      "kcar.com\n",
      "mt-guide01.com\n",
      "amghbwp.cn\n",
      "redvi63.com\n",
      "hlbam13.com\n",
      "996wap.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "# get 25 random websites\n",
    "random__ko_websites_1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for korean language which is different from the previous 25 websites in random__ko_websites_1\n",
    "random__ko_websites_2 = random.sample(websites, 25)\n",
    "# check random__ko_websites_2 is different from random__ko_websites_1\n",
    "for website in random__ko_websites_2:\n",
    "    if website in random__ko_websites_1:\n",
    "        print(\"Random websites in Korean are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random__ko_websites_2.remove(website)\n",
    "        random__ko_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_2:\n",
    "    print(website['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean: [{'url': 'yxzwlkj.com', 'language': 'ko', 'timestamp': '2024-06-08T08:34:31.376923'}, {'url': 'jshaoou.com', 'language': 'ko', 'timestamp': '2024-06-08T07:06:15.695528'}]\n"
     ]
    }
   ],
   "source": [
    "# give two more random korean websites\n",
    "random__ko_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Korean:\", random__ko_websites_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359198.com\n",
      "toonkor326.com\n",
      "dbcnews.co.kr\n",
      "hbwocheng.com\n",
      "whichav.video\n",
      "pinksisly.com\n",
      "chenzhongtech.com\n",
      "bluezz.com.tw\n",
      "gdzhukou.com\n",
      "oplove16.com\n",
      "yamoa3.site\n",
      "zhongfa1688.com\n",
      "douyuanxiuhe.com\n",
      "yp.com.hk\n",
      "imendon.com\n",
      "fxfx217.com\n",
      "htwhbook.com\n",
      "yedam.com\n",
      "homeplus.co.kr\n",
      "newhua99.xyz\n",
      "88p2p.com\n",
      "shyuwangfangshui.com\n",
      "fenghemp.com\n",
      "daoom.co.kr\n",
      "nfqlife.com\n",
      "evolutionplaynow.com\n",
      "limeitianhe.com\n",
      "yebigun1.mil.kr\n",
      "anpservice.net\n",
      "trfsgs.com\n"
     ]
    }
   ],
   "source": [
    "# give 30 random websites from the database for korean, chinese and japanese languages\n",
    "# get 30 random websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "random_websites = random.sample(websites, 30)\n",
    "# just list urls\n",
    "for website in random_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bu-light.com\n",
      "calulu-dogwear.jp\n",
      "jrelife.jp\n",
      "ksb.co.jp\n",
      "costcotuu.com\n",
      "nissui-kenko.com\n",
      "adalysis.tech\n",
      "kagukuro.com\n",
      "eco-ring.com\n",
      "blogoon.net\n",
      "zero.jp\n",
      "sunmold.com\n",
      "kodomotokurashi.com\n",
      "spimo.net\n",
      "venus-walker.com\n",
      "webike.net\n",
      "sublimestore.jp\n",
      "sustaina-mall.com\n",
      "d-arms-shop.jp\n",
      "crx7601.com\n",
      "cosp.jp\n",
      "miyaji.co.jp\n",
      "194964.com\n",
      "mediamix.ne.jp\n",
      "estnation.co.jp\n",
      "Random websites in Japanese:\n",
      "geo-mobile.jp\n",
      "branshes.jp\n",
      "stransa.co.jp\n",
      "belllabell.com\n",
      "regza.com\n",
      "ko-co.jp\n",
      "harmony-products.jp\n",
      "press-crew.com\n",
      "tokyo-recycle.net\n",
      "enjoytokyo.jp\n",
      "sbux.jp\n",
      "videomarket.jp\n",
      "nogikoi.jp\n",
      "zakzak.co.jp\n",
      "it-hojo.jp\n",
      "paidy.com\n",
      "movie-rush.com\n",
      "signmall.jp\n",
      "entrevida.com\n",
      "golfsapuri.com\n",
      "rekisiru.com\n",
      "sharousi-kakomon.com\n",
      "supleks.jp\n",
      "mamoru-k.com\n",
      "lordsofchaos.jp\n"
     ]
    }
   ],
   "source": [
    "# get 25 random websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "random_ja_websites = random.sample(websites, 25)\n",
    "# just list urls\n",
    "for website in random_ja_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "# get 25 more random japanese websites\n",
    "random_ja_websites_2 = random.sample(websites, 25)\n",
    "# check random_ja_websites_2 is different from random_ja_websites\n",
    "for website in random_ja_websites_2:\n",
    "    if website in random_ja_websites:\n",
    "        # print(\"Random websites in Japanese are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ja_websites_2.remove(website)\n",
    "        random_ja_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Japanese:\")\n",
    "for website in random_ja_websites_2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Japanese: [{'url': 'comiful.net', 'language': 'ja', 'timestamp': '2024-06-08T09:51:24.302366'}, {'url': 'hentaiasmr.moe', 'language': 'ja', 'timestamp': '2024-06-08T02:31:00.175990'}]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 more random japanese websites\n",
    "random_ja_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Japanese:\", random_ja_websites_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of user agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# get free proxies from online\n",
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # fetch proxy list from online\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})\n",
    "\n",
    "\n",
    "    # print(proxy_table)\n",
    "\n",
    "    # # Extract proxy IPs and ports\n",
    "    proxies = []\n",
    "    proxy_table = proxy_table.find(\"tbody\")\n",
    "    # print(proxy_table)\n",
    "    for row in proxy_table.find_all(\"tr\"):\n",
    "        proxies.append({\n",
    "        \"ip\":   row.find_all(\"td\")[0].string,\n",
    "        \"port\": row.find_all(\"td\")[1].string\n",
    "        })\n",
    "        # print(row)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_35.naver\n",
      "Extracting links from: https://policy.naver.com/rules/youthpolicy.html\n",
      "Extracting links from: http://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_16.naver\n",
      "Extracting links from: https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login\n",
      "Extracting links from: https://www.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_17.naver\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_18.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "Extracting links from: https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "Extracting links from: https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "Extracting links from: http://naver.com\n",
      "Extracting links from: https://policy.naver.com/policy/service.html\n",
      "Extracting links from: http://www.naver.com\n",
      "Extracting links from: https://help.naver.com\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "Extracting links from: https://right.naver.com\n",
      "Extracting links from: https://help.naver.com/\n",
      "Extracting links from: http://help.naver.com/\n",
      "Extracting links from: https://help.naver.com/support/home.nhn\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "Extracting links from: http://policy.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "Extracting links from: http://policy.naver.com/rules/service_location.html\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063\n",
      "Extracting links from: https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "Extracting links from: https://tv.naver.com/v/13644277/list/594825\n",
      "Extracting links from: https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/privacyinfo\n",
      "Extracting links from: http://www.naver.com/rules/service.html\n",
      "Extracting links from: http://www.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "Extracting links from: https://www.naver.com/more.html\n",
      "Extracting links from: https://www.naver.com/policy/service.html\n",
      "Extracting links from: https://www.naver.com/policy/privacy.html\n",
      "Extracting links from: https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "Extracting links from: http://www.naver.com/rules/disclaimer.html\n",
      "Extracting links from: https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "Extracting links from: https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "Extracting links from: https://gam.naver.com/optout/main \n",
      "Error accessing https://gam.naver.com/optout/main : 404 Client Error: 404 for url: https://gam.naver.com/optout/main%20\n",
      "Extracting links from: https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "Extracting links from: https://blog.naver.com/n_privacy\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://notice.naver.com/notices/privacypolicy/13880\n",
      "Extracting links from: https://www.naver.com/policy/youthpolicy.html\n",
      "Extracting links from: https://help.naver.com/alias/report/Protection_report.naver\n",
      "Extracting links from: https://policy.naver.com/policy/service_group.html\n",
      "Extracting links from: https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://blog.naver.com/blogpeople/220823707644\n",
      "Extracting links from: https://help.naver.com/support/reportCenter/home.help\n",
      "Extracting links from: https://green.naver.com/\n",
      "Extracting links from: https://privacy.naver.com\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14997\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14236\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14235\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14234\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14233\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223471678731\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223463199093\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223455075484\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223446809582\n",
      "Extracting links from: http://study.jr.naver.com/privacy/\n",
      "Extracting links from: https://jr.naver.com/\n",
      "Extracting links from: https://help.naver.com/service/5636/category/bookmark\n",
      "Extracting links from: https://jr.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/report/report_m_3.naver\n",
      "Extracting links from: https://right.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html#a4\n",
      "Extracting links from: https://inoti.naver.com/inoti/main.nhn\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "Extracting links from: http://policy.naver.com/policy/disclaimer.html\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "Extracting links from: https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html#a2_3\n",
      "Extracting links from: http://blog.naver.com/n_privacy/80143119849\n",
      "Extracting links from: https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "Extracting links from: https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n",
      "All extracted links:\n",
      "https://help.naver.com/alias/search/word/word_35.naver\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "http://www.naver.com/\n",
      "https://help.naver.com/alias/search/word/word_16.naver\n",
      "https://www.navercorp.com\n",
      "https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "https://nid.naver.com/nidlogin.login\n",
      "https://www.naver.com\n",
      "https://help.naver.com/alias/search/word/word_17.naver\n",
      "https://help.naver.com/alias/search/word/word_18.naver\n",
      "https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "https://www.navercorp.com/\n",
      "https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "http://naver.com\n",
      "https://policy.naver.com/policy/service.html\n",
      "http://www.naver.com\n",
      "https://help.naver.com\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "https://right.naver.com\n",
      "https://help.naver.com/\n",
      "http://www.navercorp.com/\n",
      "http://recruit.navercorp.com/\n",
      "https://www.navercorp.com/nhn/company/proposalGuide.nhn\n",
      "http://help.naver.com/\n",
      "https://help.naver.com/support/home.nhn\n",
      "http://policy.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "http://policy.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "http://policy.naver.com/rules/service_location.html\n",
      "https://notice.naver.com/notices/LBS/14063\n",
      "https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "https://tv.naver.com/v/13644277/list/594825\n",
      "https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "https://policy.naver.com/policy/privacy.html\n",
      "https://www.law.go.kr/LSW/lsInfoP.do?efYd=20200805&lsiSeq=213857#0000\n",
      "http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm\n",
      "https://centerforplainlanguage.org/learning-training/five-steps-plain-language/\n",
      "https://privacy.naver.com/privacyinfo\n",
      "http://www.naver.com/rules/service.html\n",
      "http://www.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "http://www.law.go.kr/법령/형사소송법/(13454,20150731)\n",
      "http://www.law.go.kr/lsInfoP.do?lsiSeq=160962&efYd=20141015#0000\n",
      "http://www.law.go.kr/법령/전기통신사업법/(13011,20150120)\n",
      "https://www.naver.com/more.html\n",
      "https://recruit.navercorp.com/\n",
      "https://www.navercorp.com/naver/proposalInquire\n",
      "https://www.naver.com/policy/service.html\n",
      "https://www.naver.com/policy/privacy.html\n",
      "https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "http://www.naver.com/rules/disclaimer.html\n",
      "https://www.navercorp.com/ko/company/proposalRegister.nhn\n",
      "https://s.pstatic.net/static/www/rules/naverBrandRequest.doc\n",
      "https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "https://gam.naver.com/optout/main \n",
      "https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "http://www.navercorp.com/ko/index.nhn\n",
      "https://blog.naver.com/n_privacy\n",
      "https://privacy.kisa.or.kr/main.do\n",
      "https://www.spo.go.kr/site/spo/main.do\n",
      "http://www.police.go.kr/www/security/cyber.jsp\n",
      "https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://notice.naver.com/notices/privacypolicy/13880\n",
      "https://www.naver.com/policy/youthpolicy.html\n",
      "https://help.naver.com/alias/report/Protection_report.naver\n",
      "https://policy.naver.com/policy/service_group.html\n",
      "https://www.kiso.or.kr/%EC%95%8C%EB%A6%BC%EB%A7%88%EB%8B%B9/%EC%A3%BC%EC%9A%94-%EA%B3%B5%EA%B0%9C%EC%82%AC%ED%95%AD/%EC%A0%95%EC%B1%85%EA%B2%B0%EC%A0%95/\n",
      "https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://blog.naver.com/blogpeople/220823707644\n",
      "https://www.kiso.or.kr/%EC%A0%95%EC%B1%85%EC%9C%84%EC%9B%90%ED%9A%8C/%EC%A0%95%EC%B1%85%EA%B7%9C%EC%A0%95/\n",
      "https://www.kiso.or.kr/%EC%A0%95%EB%B3%B4%EC%84%BC%ED%84%B0/kiso-%EC%A0%95%EC%B1%85/guideline/\n",
      "https://help.naver.com/support/reportCenter/home.help\n",
      "https://green.naver.com/\n",
      "https://privacy.naver.com\n",
      "https://notice.naver.com/notices/privacynid/14997\n",
      "https://notice.naver.com/notices/privacynid/14236\n",
      "https://notice.naver.com/notices/privacynid/14235\n",
      "https://notice.naver.com/notices/privacynid/14234\n",
      "https://notice.naver.com/notices/privacynid/14233\n",
      "https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "https://blog.naver.com//n_privacy/223471678731\n",
      "https://blog.naver.com//n_privacy/223463199093\n",
      "https://blog.naver.com//n_privacy/223455075484\n",
      "https://blog.naver.com//n_privacy/223446809582\n",
      "http://study.jr.naver.com/privacy/\n",
      "https://jr.naver.com/\n",
      "https://help.naver.com/service/5636/category/bookmark\n",
      "https://jr.naver.com\n",
      "https://help.naver.com/alias/report/report_m_3.naver\n",
      "https://right.naver.com/\n",
      "https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "https://policy.naver.com/policy/privacy.html#a4\n",
      "https://www.facebook.com/naverprivacy\n",
      "https://inoti.naver.com/inoti/main.nhn\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "http://policy.naver.com/policy/disclaimer.html\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "http://policy.naver.com/policy/privacy.html#a2_3\n",
      "http://blog.naver.com/n_privacy/80143119849\n",
      "http://ko.wikipedia.org/wiki/HTTP_Cookie\n",
      "http://cookiecentral.com/\n",
      "http://www.howstuffworks.com/cookie.htm\n",
      "https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://www.naver.com/',  # Replace with your URLs\n",
    "    'https://www.997788.com/',\t\n",
    "]\n",
    "\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Add a list of user agents here\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    # proxies = get_proxies()\n",
    "    # proxy = random.choice(proxies)\n",
    "    # print(f\"Using proxy: {proxy}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        if original_domain in url:\n",
    "            print(f\"Extracting links from: {url}\")\n",
    "            links = extract_links(url)\n",
    "            for link in links:\n",
    "                if link not in visited_links:\n",
    "                    # if original_domain in link:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    original_domain = \"naver.com\"\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privacy-related links:\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "https://policy.naver.com/policy/service.html\n",
      "http://policy.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "http://policy.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "http://policy.naver.com/rules/service_location.html\n",
      "https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "https://policy.naver.com/policy/privacy.html\n",
      "http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm\n",
      "https://privacy.naver.com/privacyinfo\n",
      "http://www.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "https://www.naver.com/policy/service.html\n",
      "https://www.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "https://blog.naver.com/n_privacy\n",
      "https://privacy.kisa.or.kr/main.do\n",
      "https://notice.naver.com/notices/privacypolicy/13880\n",
      "https://www.naver.com/policy/youthpolicy.html\n",
      "https://policy.naver.com/policy/service_group.html\n",
      "https://privacy.naver.com\n",
      "https://notice.naver.com/notices/privacynid/14997\n",
      "https://notice.naver.com/notices/privacynid/14236\n",
      "https://notice.naver.com/notices/privacynid/14235\n",
      "https://notice.naver.com/notices/privacynid/14234\n",
      "https://notice.naver.com/notices/privacynid/14233\n",
      "https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "https://blog.naver.com//n_privacy/223471678731\n",
      "https://blog.naver.com//n_privacy/223463199093\n",
      "https://blog.naver.com//n_privacy/223455075484\n",
      "https://blog.naver.com//n_privacy/223446809582\n",
      "http://study.jr.naver.com/privacy/\n",
      "https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "https://policy.naver.com/policy/privacy.html#a4\n",
      "https://www.facebook.com/naverprivacy\n",
      "http://policy.naver.com/policy/disclaimer.html\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "http://policy.naver.com/policy/privacy.html#a2_3\n",
      "http://blog.naver.com/n_privacy/80143119849\n",
      "https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n"
     ]
    }
   ],
   "source": [
    "# from all_links list, get all urls which might be related to privacy policy, terms of service, policy, cookies, etc\n",
    "# extract all urls which might be related to privacy policy, terms of service, policy, cookies, etc\n",
    "# from the list of all links extracted from the website\n",
    "\n",
    "search_terms = [\"privacy\", \"policy\", \"terms\", \"cookies\", \"gdpr\", \"data\", \"protection\", \"security\" \"legal\", \"agreement\"]\n",
    "privacy_links = []\n",
    "for link in all_links:\n",
    "    for term in search_terms:\n",
    "        if term in link:\n",
    "            privacy_links.append(link)\n",
    "            break\n",
    "\n",
    "print(\"Privacy-related links:\")\n",
    "for link in privacy_links:\n",
    "    print(link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://help.naver.com/alias/search/word/word_35.naver\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "http://www.naver.com/\n",
      "https://help.naver.com/alias/search/word/word_16.naver\n",
      "https://d2.naver.com/home\n",
      "https://developers.naver.com/\n"
     ]
    }
   ],
   "source": [
    "# all_links2 =all_links\n",
    "# print all links which has naver.com in it\n",
    "for link in all_links2:\n",
    "    if \"naver.com\" in link:\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original domain: www.naver.com, Start URL: https://www.naver.com/\n",
      "Extracting links from: https://www.naver.com/\n",
      "All extracted links:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        print(f\"Extracting links from: {url}\")\n",
    "        links = extract_links(url)\n",
    "        for link in links:\n",
    "            parsed_link = urlparse(link)\n",
    "            if link not in visited_links:\n",
    "                # check if original domain is in the link\n",
    "                if original_domain in parsed_link.netloc:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    original_domain = parsed_start_url.netloc\n",
    "    \n",
    "    print(f\"Original domain: {original_domain}, Start URL: {start_url}\")\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "eyny.com 5\n",
      "0\n",
      "tianfuents.com 29\n",
      "0\n",
      "cdjingrun.com 30\n",
      "0\n",
      "mib19.co.kr 69\n",
      "2\n",
      "kvuuv.com 155\n",
      "0\n",
      "ytjingji.com 25\n",
      "0\n",
      "tj-bdqn.com 0\n",
      "0\n",
      "qhyjjk.com 178\n",
      "0\n",
      "yawentv.com 0\n",
      "0\n",
      "mxs2.com 0\n",
      "0\n",
      "gougou600.top 0\n",
      "0\n",
      "997788.com 0\n",
      "0\n",
      "homeplus.co.kr 0\n",
      "0\n",
      "dashixun.com 189\n",
      "0\n",
      "tlogcorp.com 6\n",
      "0\n",
      "jelly-beansshop.com 22\n",
      "0\n",
      "lishixzs.com 2372\n",
      "0\n",
      "lndaomou.com 27\n",
      "0\n",
      "indischool.com 11\n",
      "4\n",
      "xunjiepdf.com 2934\n",
      "0\n",
      "oift.net 0\n",
      "0\n",
      "jishuangtech.com 170\n",
      "0\n",
      "dragonest.com 21\n",
      "1\n",
      "mhghotel.com 0\n",
      "0\n",
      "codeup.kr 0\n",
      "0\n",
      "kurogames.com 4\n",
      "3\n",
      "xtlgf.com 172\n",
      "0\n",
      "smartproxy.cn 1\n",
      "0\n",
      "zocbo.com 49\n",
      "1\n",
      "hm7ew6c.cc 2\n",
      "0\n",
      "yunsom.com 4\n",
      "0\n",
      "ttu.edu.tw 1881\n",
      "3\n",
      "realclick.co.kr 771\n",
      "6\n",
      "726786.com 2359\n",
      "19\n",
      "ymc616.com 2568\n",
      "28\n",
      "sissykorea2.com 0\n",
      "0\n",
      "torrentsee220.com 0\n",
      "0\n",
      "wtwt250.com 1\n",
      "0\n",
      "wtwt208.com 7\n",
      "0\n",
      "univ100.kr 1140\n",
      "11\n",
      "jscjx.cn 1717\n",
      "8\n",
      "octopus.com.hk 1480\n",
      "11\n",
      "ny10086.com 4658\n",
      "13\n",
      "anpservice.net 43\n",
      "0\n",
      "bestone-work.com 2124\n",
      "10\n",
      "lcd1004.co.kr 6592\n",
      "5\n",
      "gnjoy.com.tw 2443\n",
      "23\n",
      "jointapply.com 18650\n",
      "47\n",
      "mywhh.com 110\n",
      "7\n",
      "11toon96.com 6567\n",
      "252\n",
      "ytn.co.kr 24473\n",
      "105\n",
      "hnzxscp.com 3041\n",
      "23\n",
      "dgxinshun168.com 4257\n",
      "58\n",
      "fzxwrsguyy.xyz 19\n",
      "0\n",
      "dak.gg 22722\n",
      "51\n",
      "bikesell.co.kr 24\n",
      "0\n",
      "gwfence.co.kr 4305\n",
      "28\n",
      "fxfx240.com 2\n",
      "0\n",
      "toosadfun.com 8585\n",
      "21\n",
      "mangoboard.net 28292\n",
      "67\n",
      "hongniuzy3.com 12\n",
      "0\n",
      "xn--939au0g3vw1iaq8a469c.kr 19\n",
      "0\n",
      "ei.go.kr 47\n",
      "0\n",
      "clean-clean-peru.com 1385\n",
      "9\n",
      "manatoki339.net 1411\n",
      "14\n",
      "schoolbell-e.com 45\n",
      "3\n",
      "mlxtrip.com 2327\n",
      "15\n",
      "shyldj.com 465\n",
      "1\n",
      "sunybeckbj.com 298\n",
      "0\n",
      "ruitu02.com 66\n",
      "0\n",
      "yasyadong.com 121\n",
      "0\n",
      "instiz.net 1707\n",
      "14\n",
      "cypher-dark-market.com 2046\n",
      "12\n",
      "xstxt.cc 366\n",
      "3\n",
      "cafe24.co.kr 215\n",
      "6\n",
      "lz3305.com 610\n",
      "5\n",
      "ahjundun.com 1503\n",
      "10\n",
      "yylow.com 27\n",
      "0\n",
      "sytianxia.com 80\n",
      "3\n",
      "peanutoon.com 163\n",
      "3\n",
      "thekpm.com 1111\n",
      "8\n",
      "lscmlt.com 2560\n",
      "14\n",
      "xiuwushidai.com 2867\n",
      "15\n",
      "x1kzhi.com 3463\n",
      "15\n",
      "urovo.com 3528\n",
      "15\n",
      "businessweekly.com.tw 3770\n",
      "15\n",
      "linqihuoyuan.com 4134\n",
      "15\n",
      "peoplenjob.com 4268\n",
      "15\n",
      "aii.life 4587\n",
      "16\n",
      "fxxz.com 4625\n",
      "16\n",
      "solarbe.com 4691\n",
      "16\n",
      "metakr.co.kr 4863\n",
      "17\n",
      "poni25.net 4902\n",
      "17\n",
      "avsubs.co.kr 5042\n",
      "18\n",
      "jnu.ac.kr 5162\n",
      "21\n",
      "bingfeng.tw 5717\n",
      "22\n",
      "gxrgwl.com 6045\n",
      "22\n",
      "youboy.com 6613\n",
      "22\n",
      "univstore.com 6614\n",
      "22\n",
      "jc001.cn 6613\n",
      "22\n",
      "srbang.cn 0\n",
      "0\n",
      "wfwf301.com 14\n",
      "0\n",
      "tianjinzhaofa.cn 42\n",
      "0\n",
      "itpison.com 688\n",
      "7\n",
      "tkr314.com 1527\n",
      "11\n",
      "wink.co.kr 1520\n",
      "11\n",
      "udongman.cn 205\n",
      "0\n",
      "shouxiuwang.com 211\n",
      "3\n",
      "miaojiek.com 18\n",
      "0\n",
      "nkbuluo.com 28\n",
      "0\n",
      "sonohotelsresorts.com 4\n",
      "0\n",
      "dabangapp.com 18\n",
      "0\n",
      "sguwl.com 20\n",
      "0\n",
      "lnjiate.com 78\n",
      "1\n",
      "vip-ugo.com 16\n",
      "1\n",
      "shwenguo.com 153\n",
      "1\n",
      "wftoon125.com 25\n",
      "0\n",
      "ryrong.com 151\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "policy_links = policy_links_table.all()\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "for domain in policy_links:\n",
    "    print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"terms\" in link or \"cookies\" in link or \"gdpr\" in link or \"data\" in link or \"protection\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            privacy_domains[domain['domain']] = privacy_links\n",
    "            # print(link)\n",
    "            # break\n",
    "    print(len(privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "mib19.co.kr 2\n",
      "indischool.com 4\n",
      "dragonest.com 1\n",
      "kurogames.com 3\n",
      "zocbo.com 1\n",
      "ttu.edu.tw 3\n",
      "realclick.co.kr 6\n",
      "726786.com 19\n",
      "ymc616.com 28\n",
      "univ100.kr 11\n",
      "jscjx.cn 8\n",
      "octopus.com.hk 11\n",
      "ny10086.com 13\n",
      "bestone-work.com 10\n",
      "lcd1004.co.kr 5\n",
      "gnjoy.com.tw 23\n",
      "jointapply.com 47\n",
      "mywhh.com 7\n",
      "11toon96.com 252\n",
      "ytn.co.kr 105\n",
      "hnzxscp.com 23\n",
      "dgxinshun168.com 58\n",
      "dak.gg 51\n",
      "gwfence.co.kr 28\n",
      "toosadfun.com 21\n",
      "mangoboard.net 67\n",
      "clean-clean-peru.com 9\n",
      "manatoki339.net 14\n",
      "schoolbell-e.com 3\n",
      "mlxtrip.com 15\n",
      "shyldj.com 1\n",
      "instiz.net 14\n",
      "cypher-dark-market.com 12\n",
      "xstxt.cc 3\n",
      "cafe24.co.kr 6\n",
      "lz3305.com 5\n",
      "ahjundun.com 10\n",
      "sytianxia.com 3\n",
      "peanutoon.com 3\n",
      "thekpm.com 8\n",
      "lscmlt.com 14\n",
      "xiuwushidai.com 15\n",
      "x1kzhi.com 15\n",
      "urovo.com 15\n",
      "businessweekly.com.tw 15\n",
      "linqihuoyuan.com 15\n",
      "peoplenjob.com 15\n",
      "aii.life 16\n",
      "fxxz.com 16\n",
      "solarbe.com 16\n",
      "metakr.co.kr 17\n",
      "poni25.net 17\n",
      "avsubs.co.kr 18\n",
      "jnu.ac.kr 21\n",
      "bingfeng.tw 22\n",
      "gxrgwl.com 22\n",
      "youboy.com 22\n",
      "univstore.com 22\n",
      "jc001.cn 22\n"
     ]
    }
   ],
   "source": [
    "print(len(privacy_domains))\n",
    "\n",
    "# give number of privacy related links for each domain\n",
    "for domain, links in privacy_domains.items():\n",
    "    print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Korean websites: 2730\n",
      "Number of Korean domains in policy links table: 81\n",
      "81\n",
      "yes\n",
      "2649\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "korean_websites = websites_table.search(Website.language == \"ko\")\n",
    "\n",
    "all_korean_websites = [website['url'] for website in korean_websites]\n",
    "print(f\"Total Korean websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "# websites_to_process = korean_websites[:100]\n",
    "# websites_to_process = [website['url'] for website in websites_to_process]\n",
    "# print(websites_to_process)\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "\n",
    "print(f\"Number of Korean domains in policy links table: {len(set(existing_policy_links))}\")\n",
    "\n",
    "print(len(set(existing_policy_links)))\n",
    "\n",
    "if (\"eyny.com\" in existing_policy_links) and (\"eyny.com\" in all_korean_websites):\n",
    "    print(\"yes\")\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_korean_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Korean websites: 2730\n",
      "Number of Korean domains in policy links table: 122\n",
      "122\n",
      "yes\n",
      "2608\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "korean_websites = websites_table.search(Website.language == \"ko\")\n",
    "\n",
    "all_korean_websites = [website['url'] for website in korean_websites]\n",
    "print(f\"Total Korean websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "# websites_to_process = korean_websites[:100]\n",
    "# websites_to_process = [website['url'] for website in websites_to_process]\n",
    "# print(websites_to_process)\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "# only korean domains which has policy_link['country'] == \"Korea\"\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "\n",
    "print(f\"Number of Korean domains in policy links table: {len(set(existing_policy_links))}\")\n",
    "\n",
    "print(len(set(existing_policy_links)))\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_korean_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
