{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: C:\\Users\\mhass\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag\n",
    "\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB of Websites by Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63926, 'zh-cn': 16652, 'id': 8471, 'ru': 4302, 'es': 3924, 'de': 3500, 'ja': 3380, 'pt': 3346, 'ko': 2730, 'fr': 2618, 'vi': 1817, 'it': 1497, 'tr': 1457, 'nl': 1152, 'pl': 1105, 'ar': 1085, 'fa': 1067, 'th': 968, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 336, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 242, 'bn': 228, 'sk': 206, 'so': 171, 'he': 169, 'lt': 131, 'sl': 108, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n",
    "\n",
    "# list all unique languages on a new line and total number of unique languages\n",
    "unique_languages = set(languages)\n",
    "print(unique_languages)\n",
    "print(f\"Total number of unique languages: {len(unique_languages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total websites in English: 63926\n",
      "Total websites in Chinese(simplified): 16652\n",
      "Total websites in Chinese(traditional): 15\n",
      "Total websites in Korean: 2730\n",
      "Total websites in Japanese: 3380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese(simplified): {len(websites)}\")\n",
    "\n",
    "# zh-tw\n",
    "websites = websites_table.search(Website.language == \"zh-tw\")\n",
    "print(f\"Total websites in Chinese(traditional): {len(websites)}\")\n",
    "\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Random Websites from the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 random domains for eng, zh-cn, ko, ja languages\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "# get 25 random websites\n",
    "random_en_websites = random.sample(websites, 25)\n",
    "print(\"Random websites in English:\")\n",
    "for website in random_en_websites:\n",
    "    print(website['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Chinese(simplified):\n",
      "piggymates.com\n",
      "xiningchina.com\n",
      "qianbangjiaoyu.com\n",
      "dnbbm.com\n",
      "sanygroup.com\n",
      "qxjf-art.com\n",
      "xiamiaoyangzhi.com\n",
      "cqkuaisu.com\n",
      "cshuaqun.com\n",
      "gongyeqg.com\n",
      "18avx.com\n",
      "chinabrx.com\n",
      "rendaikuan.com\n",
      "szyueshan.com\n",
      "qdzhiruitong.com\n",
      "freereceivesms.com\n",
      "gggoodgame.com\n",
      "hellobike.com\n",
      "allstar-era.com\n",
      "kxunchina.com\n",
      "zgrtcm.com\n",
      "yumerzx.com\n",
      "leg1678.com\n",
      "shenzhen-nanning.com\n",
      "262196.cn\n",
      "Random websites in Chinese(simplified):\n",
      "fytlsm.com\n",
      "hapclock.com\n",
      "jtk100.com\n",
      "iduduapp.com\n",
      "sdbxqy.com\n",
      "njchangxue.com\n",
      "daimonchina.com\n",
      "zhaoshimy.com\n",
      "jutu360.com\n",
      "wodessay.com\n",
      "yataixuanhao.com\n",
      "hztaiyi.com\n",
      "sujienk.com\n",
      "liusuliusu.com\n",
      "cmdjdkj.com\n",
      "hycmzc.com\n",
      "ytshenhong.com\n",
      "znote8899.com\n",
      "jxlesong.com\n",
      "jax-china.com\n",
      "51haotou.com\n",
      "xinglistqy.com\n",
      "mtsbjy.com\n",
      "ysu.edu.cn\n",
      "tianqingshiyin.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "# get 25 random websites\n",
    "random_ZhCn_websites1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for zh-cn language which is different from the previous 25 websites in random_ZhCn_websites1\n",
    "random_ZhCn_websites2 = random.sample(websites, 25)\n",
    "# check random_ZhCn_websites2 is different from random_ZhCn_websites1\n",
    "for website in random_ZhCn_websites2:\n",
    "    if website in random_ZhCn_websites1:\n",
    "        print(\"Random websites in Chinese(simplified) are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ZhCn_websites2.remove(website)\n",
    "        random_ZhCn_websites2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean:\n",
      "21stcbc.org\n",
      "lcd1004.co.kr\n",
      "pvxywg.com\n",
      "wtwt248.com\n",
      "papalah.pw\n",
      "jusoya10.com\n",
      "netpro.co.kr\n",
      "bestone-work.com\n",
      "kassashair.com\n",
      "chroscience.com\n",
      "studypatent.com\n",
      "ovotv.com\n",
      "chosong.co.kr\n",
      "xsmzjc.com\n",
      "daehangreenpower.com\n",
      "xingyueboke.com\n",
      "womaneconomy.co.kr\n",
      "keyixs.com\n",
      "xn--939au0g3vw1iaq8a469c.kr\n",
      "19878719.com\n",
      "scshangting.com\n",
      "rongbaodianmo.com\n",
      "mgyqw.com\n",
      "dabangapp.com\n",
      "qiutianxia29.com\n",
      "Random websites in Korean:\n",
      "uqcjvpk.cn\n",
      "mobilitytv.co.kr\n",
      "whichav.video\n",
      "jxcgyl.com\n",
      "interpark.com\n",
      "jiexunec.com\n",
      "1234567.com.cn\n",
      "neworbis.com\n",
      "heywakeup.com.tw\n",
      "wozai-travel.com\n",
      "smdv.kr\n",
      "ezalba.co.kr\n",
      "haobofangshui.com\n",
      "torrentsee217.com\n",
      "ruantongzhi.com\n",
      "cmuma.xyz\n",
      "ttlock.com\n",
      "jmdoor.com.tw\n",
      "newtoki.help\n",
      "sxwlz.com\n",
      "sdmeixiusy.com\n",
      "optisun.vip\n",
      "clean-clean-peru.com\n",
      "11toon112.com\n",
      "aniweek.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "# get 25 random websites\n",
    "random__ko_websites_1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for korean language which is different from the previous 25 websites in random__ko_websites_1\n",
    "random__ko_websites_2 = random.sample(websites, 25)\n",
    "# check random__ko_websites_2 is different from random__ko_websites_1\n",
    "for website in random__ko_websites_2:\n",
    "    if website in random__ko_websites_1:\n",
    "        print(\"Random websites in Korean are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random__ko_websites_2.remove(website)\n",
    "        random__ko_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_2:\n",
    "    print(website['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean: [{'url': 'yxzwlkj.com', 'language': 'ko', 'timestamp': '2024-06-08T08:34:31.376923'}, {'url': 'jshaoou.com', 'language': 'ko', 'timestamp': '2024-06-08T07:06:15.695528'}]\n"
     ]
    }
   ],
   "source": [
    "# give two more random korean websites\n",
    "random__ko_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Korean:\", random__ko_websites_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359198.com\n",
      "toonkor326.com\n",
      "dbcnews.co.kr\n",
      "hbwocheng.com\n",
      "whichav.video\n",
      "pinksisly.com\n",
      "chenzhongtech.com\n",
      "bluezz.com.tw\n",
      "gdzhukou.com\n",
      "oplove16.com\n",
      "yamoa3.site\n",
      "zhongfa1688.com\n",
      "douyuanxiuhe.com\n",
      "yp.com.hk\n",
      "imendon.com\n",
      "fxfx217.com\n",
      "htwhbook.com\n",
      "yedam.com\n",
      "homeplus.co.kr\n",
      "newhua99.xyz\n",
      "88p2p.com\n",
      "shyuwangfangshui.com\n",
      "fenghemp.com\n",
      "daoom.co.kr\n",
      "nfqlife.com\n",
      "evolutionplaynow.com\n",
      "limeitianhe.com\n",
      "yebigun1.mil.kr\n",
      "anpservice.net\n",
      "trfsgs.com\n"
     ]
    }
   ],
   "source": [
    "# give 30 random websites from the database for korean, chinese and japanese languages\n",
    "# get 30 random websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "random_websites = random.sample(websites, 30)\n",
    "# just list urls\n",
    "for website in random_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nobori-mart.net\n",
      "keirin-mobile.jp\n",
      "sesto.jp\n",
      "eigeki.com\n",
      "valor-luvitapp.com\n",
      "kuzen.io\n",
      "enechange.co.jp\n",
      "hanako.tokyo\n",
      "saiyasune.com\n",
      "gogojungle.co.jp\n",
      "madamefigaro.jp\n",
      "reil.co.jp\n",
      "koichidomoto-fc.net\n",
      "ganbalegends.com\n",
      "jal.co.jp\n",
      "dengekionline.com\n",
      "bribaby.jp\n",
      "hankyu.co.jp\n",
      "yutasan.co\n",
      "laxd.com\n",
      "karakubuy.com\n",
      "hellouniweb.com\n",
      "android4front.jp\n",
      "nichiga.net\n",
      "cardrush-pokemon.jp\n",
      "Random websites in Japanese:\n",
      "domonet.jp\n",
      "sabory-blog.com\n",
      "tsurisuke.com\n",
      "hero-news.com\n",
      "halmek.co.jp\n",
      "monotaro.com\n",
      "homes.co.jp\n",
      "ai-eye.jp\n",
      "pushcode.jp\n",
      "tyuemon.com\n",
      "ifdef.jp\n",
      "mangakoma.net\n",
      "sangacio.com\n",
      "jobop.jp\n",
      "rere.jp\n",
      "thp-shop.co.jp\n",
      "brandnavi-online.com\n",
      "ranking.net\n",
      "edesk.jp\n",
      "kimuratan.jp\n",
      "xn--pckua2a7gp15o89zb.com\n",
      "kawashima-ya.jp\n",
      "nippon-foundation.or.jp\n",
      "boxingnews.jp\n",
      "axa-direct.co.jp\n"
     ]
    }
   ],
   "source": [
    "# get 25 random websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "random_ja_websites = random.sample(websites, 25)\n",
    "# just list urls\n",
    "for website in random_ja_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "# get 25 more random japanese websites\n",
    "random_ja_websites_2 = random.sample(websites, 25)\n",
    "# check random_ja_websites_2 is different from random_ja_websites\n",
    "for website in random_ja_websites_2:\n",
    "    if website in random_ja_websites:\n",
    "        # print(\"Random websites in Japanese are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ja_websites_2.remove(website)\n",
    "        random_ja_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Japanese:\")\n",
    "for website in random_ja_websites_2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Japanese: [{'url': 'comiful.net', 'language': 'ja', 'timestamp': '2024-06-08T09:51:24.302366'}, {'url': 'hentaiasmr.moe', 'language': 'ja', 'timestamp': '2024-06-08T02:31:00.175990'}]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 more random japanese websites\n",
    "random_ja_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Japanese:\", random_ja_websites_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of user agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# get free proxies from online\n",
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # fetch proxy list from online\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})\n",
    "\n",
    "\n",
    "    # print(proxy_table)\n",
    "\n",
    "    # # Extract proxy IPs and ports\n",
    "    proxies = []\n",
    "    proxy_table = proxy_table.find(\"tbody\")\n",
    "    # print(proxy_table)\n",
    "    for row in proxy_table.find_all(\"tr\"):\n",
    "        proxies.append({\n",
    "        \"ip\":   row.find_all(\"td\")[0].string,\n",
    "        \"port\": row.find_all(\"td\")[1].string\n",
    "        })\n",
    "        # print(row)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_35.naver\n",
      "Extracting links from: https://policy.naver.com/rules/youthpolicy.html\n",
      "Extracting links from: http://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_16.naver\n",
      "Extracting links from: https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login\n",
      "Extracting links from: https://www.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_17.naver\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_18.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "Extracting links from: https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "Extracting links from: https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "Extracting links from: http://naver.com\n",
      "Extracting links from: https://policy.naver.com/policy/service.html\n",
      "Extracting links from: http://www.naver.com\n",
      "Extracting links from: https://help.naver.com\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "Extracting links from: https://right.naver.com\n",
      "Extracting links from: https://help.naver.com/\n",
      "Extracting links from: http://help.naver.com/\n",
      "Extracting links from: https://help.naver.com/support/home.nhn\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "Extracting links from: http://policy.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "Extracting links from: http://policy.naver.com/rules/service_location.html\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063\n",
      "Extracting links from: https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "Extracting links from: https://tv.naver.com/v/13644277/list/594825\n",
      "Extracting links from: https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/privacyinfo\n",
      "Extracting links from: http://www.naver.com/rules/service.html\n",
      "Extracting links from: http://www.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "Extracting links from: https://www.naver.com/more.html\n",
      "Extracting links from: https://www.naver.com/policy/service.html\n",
      "Extracting links from: https://www.naver.com/policy/privacy.html\n",
      "Extracting links from: https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "Extracting links from: http://www.naver.com/rules/disclaimer.html\n",
      "Extracting links from: https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "Extracting links from: https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "Extracting links from: https://gam.naver.com/optout/main \n",
      "Error accessing https://gam.naver.com/optout/main : 404 Client Error: 404 for url: https://gam.naver.com/optout/main%20\n",
      "Extracting links from: https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "Extracting links from: https://blog.naver.com/n_privacy\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://notice.naver.com/notices/privacypolicy/13880\n",
      "Extracting links from: https://www.naver.com/policy/youthpolicy.html\n",
      "Extracting links from: https://help.naver.com/alias/report/Protection_report.naver\n",
      "Extracting links from: https://policy.naver.com/policy/service_group.html\n",
      "Extracting links from: https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://blog.naver.com/blogpeople/220823707644\n",
      "Extracting links from: https://help.naver.com/support/reportCenter/home.help\n",
      "Extracting links from: https://green.naver.com/\n",
      "Extracting links from: https://privacy.naver.com\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14997\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14236\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14235\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14234\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14233\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223471678731\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223463199093\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223455075484\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223446809582\n",
      "Extracting links from: http://study.jr.naver.com/privacy/\n",
      "Extracting links from: https://jr.naver.com/\n",
      "Extracting links from: https://help.naver.com/service/5636/category/bookmark\n",
      "Extracting links from: https://jr.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/report/report_m_3.naver\n",
      "Extracting links from: https://right.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html#a4\n",
      "Extracting links from: https://inoti.naver.com/inoti/main.nhn\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "Extracting links from: http://policy.naver.com/policy/disclaimer.html\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "Extracting links from: https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html#a2_3\n",
      "Extracting links from: http://blog.naver.com/n_privacy/80143119849\n",
      "Extracting links from: https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "Extracting links from: https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n",
      "All extracted links:\n",
      "https://help.naver.com/alias/search/word/word_35.naver\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "http://www.naver.com/\n",
      "https://help.naver.com/alias/search/word/word_16.naver\n",
      "https://www.navercorp.com\n",
      "https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "https://nid.naver.com/nidlogin.login\n",
      "https://www.naver.com\n",
      "https://help.naver.com/alias/search/word/word_17.naver\n",
      "https://help.naver.com/alias/search/word/word_18.naver\n",
      "https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "https://www.navercorp.com/\n",
      "https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "http://naver.com\n",
      "https://policy.naver.com/policy/service.html\n",
      "http://www.naver.com\n",
      "https://help.naver.com\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "https://right.naver.com\n",
      "https://help.naver.com/\n",
      "http://www.navercorp.com/\n",
      "http://recruit.navercorp.com/\n",
      "https://www.navercorp.com/nhn/company/proposalGuide.nhn\n",
      "http://help.naver.com/\n",
      "https://help.naver.com/support/home.nhn\n",
      "http://policy.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "http://policy.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "http://policy.naver.com/rules/service_location.html\n",
      "https://notice.naver.com/notices/LBS/14063\n",
      "https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "https://tv.naver.com/v/13644277/list/594825\n",
      "https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "https://policy.naver.com/policy/privacy.html\n",
      "https://www.law.go.kr/LSW/lsInfoP.do?efYd=20200805&lsiSeq=213857#0000\n",
      "http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm\n",
      "https://centerforplainlanguage.org/learning-training/five-steps-plain-language/\n",
      "https://privacy.naver.com/privacyinfo\n",
      "http://www.naver.com/rules/service.html\n",
      "http://www.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "http://www.law.go.kr/법령/형사소송법/(13454,20150731)\n",
      "http://www.law.go.kr/lsInfoP.do?lsiSeq=160962&efYd=20141015#0000\n",
      "http://www.law.go.kr/법령/전기통신사업법/(13011,20150120)\n",
      "https://www.naver.com/more.html\n",
      "https://recruit.navercorp.com/\n",
      "https://www.navercorp.com/naver/proposalInquire\n",
      "https://www.naver.com/policy/service.html\n",
      "https://www.naver.com/policy/privacy.html\n",
      "https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "http://www.naver.com/rules/disclaimer.html\n",
      "https://www.navercorp.com/ko/company/proposalRegister.nhn\n",
      "https://s.pstatic.net/static/www/rules/naverBrandRequest.doc\n",
      "https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "https://gam.naver.com/optout/main \n",
      "https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "http://www.navercorp.com/ko/index.nhn\n",
      "https://blog.naver.com/n_privacy\n",
      "https://privacy.kisa.or.kr/main.do\n",
      "https://www.spo.go.kr/site/spo/main.do\n",
      "http://www.police.go.kr/www/security/cyber.jsp\n",
      "https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://notice.naver.com/notices/privacypolicy/13880\n",
      "https://www.naver.com/policy/youthpolicy.html\n",
      "https://help.naver.com/alias/report/Protection_report.naver\n",
      "https://policy.naver.com/policy/service_group.html\n",
      "https://www.kiso.or.kr/%EC%95%8C%EB%A6%BC%EB%A7%88%EB%8B%B9/%EC%A3%BC%EC%9A%94-%EA%B3%B5%EA%B0%9C%EC%82%AC%ED%95%AD/%EC%A0%95%EC%B1%85%EA%B2%B0%EC%A0%95/\n",
      "https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://blog.naver.com/blogpeople/220823707644\n",
      "https://www.kiso.or.kr/%EC%A0%95%EC%B1%85%EC%9C%84%EC%9B%90%ED%9A%8C/%EC%A0%95%EC%B1%85%EA%B7%9C%EC%A0%95/\n",
      "https://www.kiso.or.kr/%EC%A0%95%EB%B3%B4%EC%84%BC%ED%84%B0/kiso-%EC%A0%95%EC%B1%85/guideline/\n",
      "https://help.naver.com/support/reportCenter/home.help\n",
      "https://green.naver.com/\n",
      "https://privacy.naver.com\n",
      "https://notice.naver.com/notices/privacynid/14997\n",
      "https://notice.naver.com/notices/privacynid/14236\n",
      "https://notice.naver.com/notices/privacynid/14235\n",
      "https://notice.naver.com/notices/privacynid/14234\n",
      "https://notice.naver.com/notices/privacynid/14233\n",
      "https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "https://blog.naver.com//n_privacy/223471678731\n",
      "https://blog.naver.com//n_privacy/223463199093\n",
      "https://blog.naver.com//n_privacy/223455075484\n",
      "https://blog.naver.com//n_privacy/223446809582\n",
      "http://study.jr.naver.com/privacy/\n",
      "https://jr.naver.com/\n",
      "https://help.naver.com/service/5636/category/bookmark\n",
      "https://jr.naver.com\n",
      "https://help.naver.com/alias/report/report_m_3.naver\n",
      "https://right.naver.com/\n",
      "https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "https://policy.naver.com/policy/privacy.html#a4\n",
      "https://www.facebook.com/naverprivacy\n",
      "https://inoti.naver.com/inoti/main.nhn\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "http://policy.naver.com/policy/disclaimer.html\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "http://policy.naver.com/policy/privacy.html#a2_3\n",
      "http://blog.naver.com/n_privacy/80143119849\n",
      "http://ko.wikipedia.org/wiki/HTTP_Cookie\n",
      "http://cookiecentral.com/\n",
      "http://www.howstuffworks.com/cookie.htm\n",
      "https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://www.naver.com/',  # Replace with your URLs\n",
    "    'https://www.997788.com/',\t\n",
    "]\n",
    "\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Add a list of user agents here\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    # proxies = get_proxies()\n",
    "    # proxy = random.choice(proxies)\n",
    "    # print(f\"Using proxy: {proxy}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        if original_domain in url:\n",
    "            print(f\"Extracting links from: {url}\")\n",
    "            links = extract_links(url)\n",
    "            for link in links:\n",
    "                if link not in visited_links:\n",
    "                    # if original_domain in link:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    original_domain = \"naver.com\"\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        print(f\"Extracting links from: {url}\")\n",
    "        links = extract_links(url)\n",
    "        for link in links:\n",
    "            parsed_link = urlparse(link)\n",
    "            if link not in visited_links:\n",
    "                # check if original domain is in the link\n",
    "                if original_domain in parsed_link.netloc:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    original_domain = parsed_start_url.netloc\n",
    "    \n",
    "    print(f\"Original domain: {original_domain}, Start URL: {start_url}\")\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bn', 'de', 'en', 'fa', 'pt', 'websites', 'id', 'it', 'vi', 'fr', 'ja', 'zh-cn', 'ru', 'es', 'pl', 'ko', 'policy_links', 'sv'}\n"
     ]
    }
   ],
   "source": [
    "# read tinydb database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "\n",
    "# read all tables names\n",
    "tables = db.tables()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6088\n"
     ]
    }
   ],
   "source": [
    "# read policy_links table\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "policy_links = policy_links_table.all()\n",
    "print(len(policy_links))\n",
    "\n",
    "# find thirty domains for Japan and Korea which only has 1 privacy policy related link links\n",
    "\n",
    "# find thirty random domains for each country\n",
    "random_korean_domains = []\n",
    "random_japanese_domains = []\n",
    "\n",
    "for domain in policy_links:\n",
    "    if domain['country'] == \"Korea\":\n",
    "        random_korean_domains.append(domain['domain'])\n",
    "    elif domain['country'] == \"Japan\":\n",
    "        random_japanese_domains.append(domain['domain'])\n",
    "\n",
    "random_korean_domains = random.sample(random_korean_domains, 30)\n",
    "random_japanese_domains = random.sample(random_japanese_domains, 30)\n",
    "\n",
    "# find all_links for each domain and filter only privacy related links\n",
    "# for korean domains\n",
    "korean_privacy_links = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Extracted Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "# policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "5341\n",
      "Number of domains with no privacy links: 470\n",
      "Number of domains with privacy links: 2889\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "2383\n"
     ]
    }
   ],
   "source": [
    "print(len(privacy_domains))\n",
    "\n",
    "# give number of privacy related links for each domain\n",
    "japan_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            japan_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(japan_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tumi.co.jp/privacyPolicy.html\n",
      "https://mocom.tv/wo/privacy.phtml\n",
      "https://locondo.jp/shop/contents/privacy\n",
      "https://anzen.ne.jp/html/info.html#about_privacy\n",
      "https://gumpla.jp/privacy\n",
      "https://support.ac-pocketcamp.com/en-AU/privacy_policy\n",
      "https://ths-fooduniform.jp/html/privacy.html\n",
      "https://tanosu.com/privacy/\n",
      "https://buffaloes.co.jp/company/privacy.html\n",
      "https://cdn.kenko-mileage.jp/policy/privacy_policy.html\n",
      "https://sen-n.com/privacy-policy/\n",
      "https://crescendoalle.com/pages/privacy\n",
      "https://atomtech.co.jp/policies/privacy-policy\n",
      "https://wadatsumi.co/policies/privacy-policy\n",
      "https://danmachi-danchro.com/privacy-policy/\n",
      "https://tryt-worker.jp/company/privacypolicy/\n",
      "https://privacy.tver.jp/tver-id-external-data-integration/\n",
      "https://talk.jp/privacy\n",
      "https://ball-goods.com/?mode=privacy\n",
      "https://izumo-pbx.jp/contact.html#privacy\n",
      "https://sug-web.jp/privacy-policy/\n",
      "https://denkohome.com/privacy/\n",
      "https://freeblog-video.com/privacy-policy/\n",
      "https://irotsuku.com/info/privacy\n",
      "https://www.convention.co.jp/privacy/\n",
      "https://emmafrancis.jp/contents/terms#privacy_content\n",
      "https://www.evastore.jp/shop/pages/privacy.aspx#anchor-cookie_block\n",
      "https://www.bornfreegroup.jp/privacypolicy\n",
      "https://car-repo.jp/privacy-policy\n",
      "https://howsie-shop.jp/pages/privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from japan_privacy_links\n",
    "random_japan_privacy_links = random.sample(japan_privacy_links, 30)\n",
    "for link in random_japan_privacy_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0           https://www.tumi.co.jp/privacyPolicy.html   \n",
      "1                   https://mocom.tv/wo/privacy.phtml   \n",
      "2            https://locondo.jp/shop/contents/privacy   \n",
      "3    https://anzen.ne.jp/html/info.html#about_privacy   \n",
      "4                           https://gumpla.jp/privacy   \n",
      "5   https://support.ac-pocketcamp.com/en-AU/privac...   \n",
      "6        https://ths-fooduniform.jp/html/privacy.html   \n",
      "7                         https://tanosu.com/privacy/   \n",
      "8        https://buffaloes.co.jp/company/privacy.html   \n",
      "9   https://cdn.kenko-mileage.jp/policy/privacy_po...   \n",
      "10                  https://sen-n.com/privacy-policy/   \n",
      "11            https://crescendoalle.com/pages/privacy   \n",
      "12     https://atomtech.co.jp/policies/privacy-policy   \n",
      "13       https://wadatsumi.co/policies/privacy-policy   \n",
      "14       https://danmachi-danchro.com/privacy-policy/   \n",
      "15      https://tryt-worker.jp/company/privacypolicy/   \n",
      "16  https://privacy.tver.jp/tver-id-external-data-...   \n",
      "17                            https://talk.jp/privacy   \n",
      "18               https://ball-goods.com/?mode=privacy   \n",
      "19          https://izumo-pbx.jp/contact.html#privacy   \n",
      "20                 https://sug-web.jp/privacy-policy/   \n",
      "21                     https://denkohome.com/privacy/   \n",
      "22         https://freeblog-video.com/privacy-policy/   \n",
      "23                  https://irotsuku.com/info/privacy   \n",
      "24              https://www.convention.co.jp/privacy/   \n",
      "25  https://emmafrancis.jp/contents/terms#privacy_...   \n",
      "26  https://www.evastore.jp/shop/pages/privacy.asp...   \n",
      "27         https://www.bornfreegroup.jp/privacypolicy   \n",
      "28                 https://car-repo.jp/privacy-policy   \n",
      "29               https://howsie-shop.jp/pages/privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   3aa81fddc11f27554b348c4bacbdd6892201fd52f8316c...  \n",
      "1   f93003101a1dc35a8f84f77d5de4e50349401e6d7c4a68...  \n",
      "2   0604ad08c6272bc865c58070051960ddfd5c300f184d37...  \n",
      "3   731e8b4b89538c01b1b76420dff82480750bca42dd4d0f...  \n",
      "4   5fe338baacbd7f12d15613ae812baf4dd208b4d7d366a1...  \n",
      "5   4cfbdc8279ca6745bd1f58ee304f2664d80e8c439f2ffc...  \n",
      "6   0463f7194b0d3656e2354bc5b665044706b80a1cf22807...  \n",
      "7   2903c5b5b7fdc1debad2519b112f6336a7fbb35b4dc2bf...  \n",
      "8   b8693e71c58177ea524837d0503a4cfe207b5c1255ee0b...  \n",
      "9   024a526957f6b4637b126fdf22972e336da0f19c26ae27...  \n",
      "10  ee602a8262c3146e5b85c8a2ccec7334771e3d4b642e43...  \n",
      "11  f9e29796d4e6e2805030d3152ebb73b1297b26cc566892...  \n",
      "12  f3d0f3111dea5cc64f168f9927c3484b1232ef225dec01...  \n",
      "13  114031d73fe94b624387a9a61be1963621eba2cee79d04...  \n",
      "14  97eac313480d61761278cd181640503989412bbdf93637...  \n",
      "15  d5287ba8731acaf5aa06af1d0efd71f154713c7db69b6f...  \n",
      "16  60290a97a2bc3830100a40686c30336eaec60205c08944...  \n",
      "17  17a2b0fa9e931351b5581078c36f0aed9cf152c18bee47...  \n",
      "18  be860561cbe09928fd8369165d5220cd49506cf5acc2fd...  \n",
      "19  d0a945673bbae0fded01e61ad5d67fff842eca1011ff07...  \n",
      "20  d6bfe39b0eb99223ef4d81f7055e581ec683cae3a2dd4b...  \n",
      "21  0edaf50f6ee2ab3d38009a753d3105fd3a1f677d433efa...  \n",
      "22  ab47d235fbfc20f22d0ccdd16d0be74d7a7254a39f8d33...  \n",
      "23  c628917e715e52f6e9006e016a9aa09175b5b0e7aedc4c...  \n",
      "24  babc3060c024455ee2ade4741f5d8e49be716a1f783bd4...  \n",
      "25  e2e3885ef0abb740caf804b640a1bcd73bba652f5aaaa9...  \n",
      "26  a85a4da4288ffc49316c36e8cfdc7d0e1eb92dab2f1e6c...  \n",
      "27  2a82c56ac400c385da4d277ae8c0d4c9c120b596b10921...  \n",
      "28  0646142af80e14eabc4a880466468e493ed1e8485f24b9...  \n",
      "29  a92c2f2f86f61a7897c95c6634357081e55e91ce38dd6f...  \n"
     ]
    }
   ],
   "source": [
    "# random_japan_privacy_links\n",
    "# create a dataframe with random_japan_privacy_links and each url's hash value using sha256\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_japan_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df.to_csv(\"policies/sample_policies_japanese/sample_policies_japanese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Error fetching the website: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_japan_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df[df['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ja\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_japanese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n",
      "1409\n",
      "Number of domains with no privacy links: 1292\n",
      "Number of domains with privacy links: 1437\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "korean_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            korean_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(korean_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://newsway.co.kr/company/privacypolicy\n",
      "https://daumee.co.kr/member/privacy.html\n",
      "https://tophub.today/allactivity?privacy_source=activity_log_top_menu\n",
      "https://duole.com/privacy/v2/gouji?template_id=1#nav6\n",
      "https://xyzcdn.net/privacy#content\n",
      "https://bluepops.co.kr/member/privacy.html\n",
      "https://fifaro.com/text/privacy\n",
      "https://myprotein.tw/customer-services/privacy-and-security.list\n",
      "https://nsfwkr.net/bbs/register.php#privacy\n",
      "https://devicemart.co.kr/service/privacy\n",
      "https://gck99.com.tw/privacy.php\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://hk-bestcasino.com/privacy-policy/\n",
      "https://sanmin.com.tw/static/termspprivacy\n",
      "https://lolchess.gg/about/privacy\n",
      "http://www.creme21.co.kr/member/privacy.html\n",
      "https://ajou.ac.kr/kr/ajou/privacy.do\n",
      "https://balletnmodel.com/?mode=privacy\n",
      "https://roo.cash/privacy\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://m.cnhnb.com/html/zhuanti/privacy/?id=1\n",
      "https://privacy.kakao.com/main?lang=ko\n",
      "https://htisec.com/zh-hk/data-privacy-policy\n",
      "https://cagongtv.com/content/privacy\n",
      "https://godamanga.com/privacy\n",
      "https://nikke-kr.com/privacypolicy#_Pursuant_to_our\n",
      "https://store.steamchina.com/privacy_agreement?snr=1_60_4__global-responsive-menu\n",
      "https://nikke-kr.com/privacypolicy#_Changes\n",
      "https://www.pcone.com.tw/service/privacyPolicy\n",
      "https://qquing.net/bbs/content.php?co_id=privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from korean_privacy_links\n",
    "random_korean_privacy_links = random.sample(korean_privacy_links, 30)\n",
    "for link in random_korean_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0         https://newsway.co.kr/company/privacypolicy   \n",
      "1            https://daumee.co.kr/member/privacy.html   \n",
      "2   https://tophub.today/allactivity?privacy_sourc...   \n",
      "3   https://duole.com/privacy/v2/gouji?template_id...   \n",
      "4                  https://xyzcdn.net/privacy#content   \n",
      "5          https://bluepops.co.kr/member/privacy.html   \n",
      "6                     https://fifaro.com/text/privacy   \n",
      "7   https://myprotein.tw/customer-services/privacy...   \n",
      "8         https://nsfwkr.net/bbs/register.php#privacy   \n",
      "9            https://devicemart.co.kr/service/privacy   \n",
      "10                   https://gck99.com.tw/privacy.php   \n",
      "11           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "12          https://hk-bestcasino.com/privacy-policy/   \n",
      "13         https://sanmin.com.tw/static/termspprivacy   \n",
      "14                  https://lolchess.gg/about/privacy   \n",
      "15       http://www.creme21.co.kr/member/privacy.html   \n",
      "16              https://ajou.ac.kr/kr/ajou/privacy.do   \n",
      "17             https://balletnmodel.com/?mode=privacy   \n",
      "18                           https://roo.cash/privacy   \n",
      "19           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "20     https://m.cnhnb.com/html/zhuanti/privacy/?id=1   \n",
      "21             https://privacy.kakao.com/main?lang=ko   \n",
      "22       https://htisec.com/zh-hk/data-privacy-policy   \n",
      "23               https://cagongtv.com/content/privacy   \n",
      "24                      https://godamanga.com/privacy   \n",
      "25  https://nikke-kr.com/privacypolicy#_Pursuant_t...   \n",
      "26  https://store.steamchina.com/privacy_agreement...   \n",
      "27        https://nikke-kr.com/privacypolicy#_Changes   \n",
      "28     https://www.pcone.com.tw/service/privacyPolicy   \n",
      "29   https://qquing.net/bbs/content.php?co_id=privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   76f8d00688ce5ee4129d2cfe7810c53c1b81d4c5d82809...  \n",
      "1   4441eea2dddb934443cac7616ded62ace14dbbe17db651...  \n",
      "2   c9effc5fde671cafb31a85969f944b22ad7b6775b254cc...  \n",
      "3   4d2472373011c6f66d1a53123afd473dc329bfb630b9c4...  \n",
      "4   4f64b7cbec077f0b235583598a40a9b63d470cfa3f1b9b...  \n",
      "5   0caf8650720351b11304dcee1993f55cbb741047ed40d8...  \n",
      "6   9b5b2c7347c49766fa80a9a1188f05ce3d4a8ffcf61a04...  \n",
      "7   5144d03775d8eb736b523d33a4f8ce47d6a9d5303c6942...  \n",
      "8   753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...  \n",
      "9   5f6d20a69b3005d6025dc37ebf4c4ead341e002de46958...  \n",
      "10  1f30698e7b51975a02398b3b2348c99ccb113797a52a59...  \n",
      "11  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "12  4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...  \n",
      "13  046e039e7063fbdbe44fd3ebb9fef9ee05b4a956a3d1bd...  \n",
      "14  9b59d857a77fb34026b08fb304430966c38bd53202b6aa...  \n",
      "15  690873a0195dd4c6636f6bcf98d18d19d419e603268a3d...  \n",
      "16  4670708b7d0f2e741a6397f518921b630a6b91a67a96d9...  \n",
      "17  364a237f7cba05b069bee8c6a1054e5d8bf78bdbdb189e...  \n",
      "18  e768bf3eda7dffeaa89387396650e9206183a83a7a2cb6...  \n",
      "19  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "20  376f97bbd9cceed3b9f88be3aea686ec20826759df45ac...  \n",
      "21  dc17e872fe7bfe10149e37d265ab2bc485f9f6798dce79...  \n",
      "22  8734940804041e2df20ad8d0434db13c926ac56a3361e0...  \n",
      "23  881f0b3bd55fac152cd92092707570af34c86246ffe1f5...  \n",
      "24  76c470a3b4e996819762864767991b497a417e45be54e5...  \n",
      "25  83ea3f88c2ce742bc760a6d82e9777ccb8b6ea6e3f5ecd...  \n",
      "26  226fd14916a7377abc449b4c504336ab5b2cb9986e99e1...  \n",
      "27  721f807ed7e33469141926186f4bed9ca1813845a8ff18...  \n",
      "28  332d68596b80569706353ba450360c159bb25012c6420f...  \n",
      "29  8c04b043562fe037dde46590353e1ebaa7b6a97757d0a8...  \n"
     ]
    }
   ],
   "source": [
    "# random_korean_privacy_links\n",
    "# create a dataframe with random_korean_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_korean_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_korean_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_korean_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_korean_privacy_links.to_csv(\"policies/sample_policies_korean/sample_policies_korean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_korean_privacy_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Main Function\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom_korean_privacy_links\u001b[49m:\n\u001b[0;32m     29\u001b[0m     text_content \u001b[38;5;241m=\u001b[39m fetch_and_convert_website(link)\n\u001b[0;32m     30\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m df_random_korean_privacy_links[df_random_korean_privacy_links[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m link][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_korean_privacy_links' is not defined"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_korean_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_korean_privacy_links[df_random_korean_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ko\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_korean/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Websites Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank            domain\n",
      "0            1        google.com\n",
      "1            2      facebook.com\n",
      "2            3     microsoft.com\n",
      "3            4    googleapis.com\n",
      "4            5         apple.com\n",
      "...        ...               ...\n",
      "578155  578156  lurkerlounge.com\n",
      "578156  578157          528y.com\n",
      "578157  578158     garybartz.com\n",
      "578158  578159          fsagq.cn\n",
      "578159  578160    spottyd10.buzz\n",
      "\n",
      "[578160 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# read \"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\" into dataframe. the csv has no headers\n",
    "tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\", header=None, index_col=False)\n",
    "# tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\")\n",
    "\n",
    "# add headers to the dataframe\n",
    "tranco_df.columns = [\"rank\", \"domain\"]\n",
    "\n",
    "print(tranco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_domains(country_code):\n",
    "    db = TinyDB('websites_by_language.json')\n",
    "\n",
    "\n",
    "    websites_table = db.table('websites')\n",
    "    Website = Query()\n",
    "\n",
    "    korean_websites = websites_table.search(Website.language == country_code)\n",
    "\n",
    "    all_korean_websites = [website['url'] for website in korean_websites]\n",
    "    print(f\"Total {country_code} websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "    # # websites_to_process = korean_websites[:100]\n",
    "    # # websites_to_process = [website['url'] for website in websites_to_process]\n",
    "    # # print(websites_to_process)\n",
    "\n",
    "    # policy_links_table = db.table('policy_links')\n",
    "    # existing_policy_links = policy_links_table.all()\n",
    "    # # only korean domains which has policy_link['country'] == \"Korea\"\n",
    "\n",
    "    # if country_code == \"ko\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "    # elif country_code == \"zh-cn\" or country_code == \"zh\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "    # elif country_code == \"ja\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "\n",
    "    # print(f\"Number of {country_code} domains in policy links table: {len(set(existing_policy_links))}\")\n",
    "\n",
    "    # remaining_websites = []\n",
    "\n",
    "    # for each_website in all_korean_websites:\n",
    "    #     if each_website not in existing_policy_links:\n",
    "    #         remaining_websites.append(each_website)\n",
    "    # print(f\"Total remaining website {len(remaining_websites)}\")\n",
    "\n",
    "    return all_korean_websites\n",
    "\n",
    "    # return remaining_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total zh-cn websites: 16652\n",
      "Total zh-tw websites: 15\n",
      "16667\n"
     ]
    }
   ],
   "source": [
    "chinese_websites_to_process = get_all_domains(\"zh-cn\")\n",
    "mandarin_websites_to_process = get_all_domains(\"zh-tw\")\n",
    "chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese_websites_to_process = get_remaining_domains(\"zh-cn\")\n",
    "# mandarin_websites_to_process = get_remaining_domains(\"zh\")\n",
    "# chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "# print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# top3K_chinese_mandarin_domains store to file top3K_chinese_mandarin_domains.txt\n",
    "with open(\"top3K_chinese_mandarin_domains.txt\", \"w\") as f:\n",
    "    for domain in top3K_chinese_mandarin_domains:\n",
    "        f.write(domain + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank           domain\n",
      "64          65           qq.com\n",
      "316        317     bilibili.com\n",
      "1396      1397           360.cn\n",
      "1697      1698         amap.com\n",
      "1924      1925       alipay.com\n",
      "...        ...              ...\n",
      "326603  326604      dlwafuu.com\n",
      "326610  326611     zhengxsy.com\n",
      "326612  326613  huiqianbian.com\n",
      "326621  326622   sumflurish.com\n",
      "326622  326623    whdaiqian.com\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# find top 3000 chinese_mandarin_domains_to_process from tranco_df table.\n",
    "\n",
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "# only keep top 3000 domains\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "print(top3K_chinese_mandarin_domains)\n",
    "\n",
    "# only keep domains in a list\n",
    "# top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# print(len(top3K_chinese_mandarin_domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"China\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "522\n",
      "Number of domains with no privacy links: 761\n",
      "Number of domains with privacy links: 560\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "chinese_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            chinese_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(chinese_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://natfrp.cloud/policy/privacy#price\n",
      "https://www.yunzhijia.com/public/agreement/privacy.html\n",
      "https://depot.fenbi.com/fenbi-privacy/index.html\n",
      "https://e.vhall.com/v3/privacyUPo\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#special\n",
      "https://www.xgcartoon.com/privacy\n",
      "https://www.cfp.cn/help/privacy-policy#collapseThree\n",
      "https://www.ooopic.com/intro/about/privacyPolicy/\n",
      "https://kaspersky.com.cn/web-privacy-policy\n",
      "https://accounts.growingio.com/user-privacy\n",
      "https://www.horoscopetruth.com/privacy#cookie_policy\n",
      "https://e.vhall.com/v3/privacyPolicy\n",
      "http://www.testplus.cn/privacy\n",
      "https://corp.pcbaby.com.cn/privacyPolicy.html\n",
      "https://www.cfp.cn/help/privacy-policy\n",
      "https://help.fanli.com/a/about/privacy.html\n",
      "https://www.horoscopetruth.com/privacy\n",
      "https://juejin.im/privacy\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#contactus\n",
      "https://agreement.gaoxiaojob.com/docs/privacy-policy/\n",
      "https://natfrp.cloud/policy/privacy\n",
      "https://qweather.com/terms/privacy\n",
      "https://corp.pcbaby.com.cn/privacyPolicy.html\n",
      "https://teambition.com/privacy\n",
      "http://www.camera360.com/privacy_zh.html\n",
      "https://www.backchina.com/special/privacy/\n",
      "https://hsbc.com.cn/en-cn/help/mandatory-info/privacy-and-security/\n",
      "https://fun88asia8.com/cn/help/policy-privacy.htm\n",
      "https://dmttang.com/label/privacy.html\n",
      "https://pop800.com/privacy_policy.html\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from \n",
    "random_chinese_privacy_links = random.sample(chinese_privacy_links, 30)\n",
    "for link in random_chinese_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Chinese Sample Privacy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0   https://www.ooopic.com/intro/about/privacyPolicy/   \n",
      "1              https://ijinshan.com/function/privacy/   \n",
      "2                        https://gotokeep.com/privacy   \n",
      "3           https://www.babytree.com/app/privacy.html   \n",
      "4                https://www.sumy.org.cn/privacy.html   \n",
      "5              https://pop800.com/privacy_policy.html   \n",
      "6                          https://www.smm.cn/privacy   \n",
      "7                https://e.vhall.com/v3/privacyPolicy   \n",
      "8   https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "9              https://dmttang.com/label/privacy.html   \n",
      "10             https://www.horoscopetruth.com/privacy   \n",
      "11                https://lianxinapp.com/privacy.html   \n",
      "12           http://www.camera360.com/privacy_zh.html   \n",
      "13                https://freereceivesms.com/privacy/   \n",
      "14                http://www.sumy.org.cn/privacy.html   \n",
      "15        https://www.shanbay.com/help/about/privacy/   \n",
      "16                              https://pp.cn/privacy   \n",
      "17  https://ppio.work/privacy-policy-20240524-v1.html   \n",
      "18             https://www.dianxiaomi.com/privacy.htm   \n",
      "19        https://learnku.com/docs/guide/privacy/4994   \n",
      "20        https://zhibo8.com/web/privacyPolicyPc.html   \n",
      "21             https://pop800.com/privacy_policy.html   \n",
      "22                              https://pp.cn/privacy   \n",
      "23                 https://qweather.com/terms/privacy   \n",
      "24  https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "25  https://passport.migu.cn/portal/privacy/protoc...   \n",
      "26              https://www.bytem.com/privacy-policy/   \n",
      "27       https://mubu.com/mubu-simple-privacy-policy/   \n",
      "28  https://agreementservice.svs.nike.com.cn/rest/...   \n",
      "29   https://akspeedy.com/html/privacy-agreement.html   \n",
      "\n",
      "                                                 hash  \n",
      "0   8d962324b48bcb726942bf7680cdd5040194308634d07a...  \n",
      "1   d4a1e978f5dffa9582f54472aa18bdd66d1877e7b7e274...  \n",
      "2   a8f8b53118ef694ea6875d6ee92e2777f6c10d33a9ff1d...  \n",
      "3   0255d8eab3c9cca5fd508b0e53619dd182324876327466...  \n",
      "4   b2545fb9903fc3e5eb1017c0dcd4a45a7f6e4489304f4d...  \n",
      "5   8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "6   2183f58b06af982b21730f60cdc8c663635ee1c8b4fe7b...  \n",
      "7   e494cd8c6a67e37867349ac09a1f9213fbe7df0b61d8cd...  \n",
      "8   9b4f04e5f22470e874b919e7a32fcc099d2effac6568e5...  \n",
      "9   f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...  \n",
      "10  d269dc0aee86f1b3ec76c500799f621873caa7cbc4f138...  \n",
      "11  b5688f5362d3adc3c8f280b0e3a9f35b7ece9f3675717e...  \n",
      "12  5157c09ae62f74e520014e2ed663eb11d09d09e61e2ccb...  \n",
      "13  4d8146865fe3ad7eccb95832e3c9d41523bc632e57ea75...  \n",
      "14  110088c2194dfac2d6f06036caf9c406767390765e0643...  \n",
      "15  5b6bdf689f9d2032e5f568bf9fcf39ec00b3b5cc43ba19...  \n",
      "16  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "17  19a1e09a202516bf1bd71cc7b66f5fca56bac5a5fd39d6...  \n",
      "18  24b23e040b3fa35aae0499bd009090ea829c3775280326...  \n",
      "19  bd3d38f95ee2147c84a0884f90148bfeac6174362f6681...  \n",
      "20  887c0920886d2263a96ab93a7eec9f944f6251511bd30c...  \n",
      "21  8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "22  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "23  f9c7bf29552b15866235771d00876a0238df455031d876...  \n",
      "24  4d06405ba73ec8f20c172acb745d01bc2e448050340ac0...  \n",
      "25  e211de0e09e38a56f73a46dac3cfdda6e6beb89537410b...  \n",
      "26  acad12dcc280f6c44c42c109fb0bec4d9c82b4db3d8051...  \n",
      "27  d704126b9f686c3e070adba47cb94485c0f89eb723022d...  \n",
      "28  4c46cf5e8f005eea1c129536c1bf1ab26984b6ff27cde2...  \n",
      "29  299aa5a241ba54d526fcfa6ee7fa755315f331e02f6942...  \n"
     ]
    }
   ],
   "source": [
    "# random_chinese_privacy_links\n",
    "\n",
    "# create a dataframe with random_chinese_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_chinese_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_chinese_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_chinese_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_chinese_privacy_links.to_csv(\"policies/sample_policies_chinese/sample_policies_chinese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching the website: 403 Client Error: Forbidden for url: https://www.ooopic.com/intro/about/privacyPolicy/\n",
      "Error fetching the website: 403 Client Error: Forbidden for url: https://www.freereceivesms.com/privacy/\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_chinese_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_chinese_privacy_links[df_random_chinese_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        with open(f\"policies/sample_policies_chinese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Progress of Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Korean websites: 2730\n",
      "Number of Korean domains in policy links table: 2729, percentage: 0.9996336996336996\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "korean_websites = websites_table.search(Website.language == \"ko\")\n",
    "\n",
    "all_korean_websites = [website['url'] for website in korean_websites]\n",
    "print(f\"Total Korean websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "# websites_to_process = korean_websites[:100]\n",
    "# websites_to_process = [website['url'] for website in websites_to_process]\n",
    "# print(websites_to_process)\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "# only korean domains which has policy_link['country'] == \"Korea\"\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "\n",
    "print(f\"Number of Korean domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_korean_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_korean_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Japanese websites: 3380\n",
      "Number of Japan domains in policy links table: 3352, percentage: 0.991715976331361\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "all_japanese_websites = [website['url'] for website in japanese_website]\n",
    "print(f\"Total Japanese websites: {len(set(all_japanese_websites))}\")\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "print(f\"Number of Japan domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_japanese_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_japanese_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7416\n"
     ]
    }
   ],
   "source": [
    "db1 = TinyDB('websites_by_language2.json')\n",
    "# print total items in 'policy_links' table\n",
    "\n",
    "domains = []\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "policy_links = policy_links_table.all()\n",
    "print(len(policy_links))\n",
    "\n",
    "for policy_link in policy_links:\n",
    "    domains.append(policy_link['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chinese websites: 3000\n",
      "Number of China domains in policy links table: 1328, percentage: 0.44266666666666665\n"
     ]
    }
   ],
   "source": [
    "db1 = TinyDB('websites_by_language2.json')\n",
    "Website = Query()\n",
    "\n",
    "# japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# # websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "print(f\"Total Chinese websites: 3000\")\n",
    "\n",
    "policy_links_table1 = db.table('policy_links')\n",
    "existing_policy_links1 = policy_links_table.all()\n",
    "existing_policy_links1 = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "\n",
    "print(f\"Number of China domains in policy links table: {len(set(existing_policy_links1))}, percentage: {len(set(existing_policy_links1))/3000}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 5001978 (char 5001977)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[238], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print total items in 'policy_links' table\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# read json into dictionary\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebsites_by_language2.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m----> 6\u001b[0m     chinese_json2 \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chinese_json2))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 5001978 (char 5001977)"
     ]
    }
   ],
   "source": [
    "db1 = TinyDB('websites_by_language2.json')\n",
    "# print total items in 'policy_links' table\n",
    "\n",
    "# read json into dictionary\n",
    "with open('websites_by_language2.json') as json_file:\n",
    "    chinese_json2 = json.load(json_file)\n",
    "\n",
    "print(len(chinese_json2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Link Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>privacy_link</th>\n",
       "      <th>hash</th>\n",
       "      <th>init_language</th>\n",
       "      <th>valid_language</th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.us-onlinestore.com</td>\n",
       "      <td>https://www.us-onlinestore.com/law.html#area-p...</td>\n",
       "      <td>9ac6380d1d17249bf50dd2f9e43c545b25ea820e2c28e5...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enhance.co.jp</td>\n",
       "      <td>https://enhance.co.jp/privacy-policy/</td>\n",
       "      <td>0fb0f105dfa22b83c4c15d0665ce7c97f7c9577c225b5b...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rizapgroup.com</td>\n",
       "      <td>https://rizapgroup.com/privacy</td>\n",
       "      <td>1de0c851c94d9d92fc1ec03635321420fb60c159661ec1...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asajikan.jp</td>\n",
       "      <td>https://asajikan.jp/privacy</td>\n",
       "      <td>724587d0bdf7fa450cb3bf22b397ced291053b4db05377...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mocom.tv</td>\n",
       "      <td>https://mocom.tv/payment_policy.phtml</td>\n",
       "      <td>2e8b98375e71893f56667adcd62d32256b05dbf126571d...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6745</th>\n",
       "      <td>nsfwkr.net</td>\n",
       "      <td>https://nsfwkr.net/bbs/register.php#privacy</td>\n",
       "      <td>753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>gck99.com.tw</td>\n",
       "      <td>https://gck99.com.tw/privacy.php</td>\n",
       "      <td>1f30698e7b51975a02398b3b2348c99ccb113797a52a59...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>hk-bestcasino.com</td>\n",
       "      <td>https://hk-bestcasino.com/privacy-policy/</td>\n",
       "      <td>4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>nsfwkr.net</td>\n",
       "      <td>https://nsfwkr.net/bbs/page.php?hid=privacy</td>\n",
       "      <td>c1ad8e75c78c554eda26b8c1c577dc13d76132587ca4a6...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6749</th>\n",
       "      <td>bj.58.com</td>\n",
       "      <td>https://bj.58.com/caishui/54996963653044x.shtm...</td>\n",
       "      <td>953a4aaf51075d3bb37f3ae1053f1a89cb0f646b413416...</td>\n",
       "      <td>Korean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6750 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain  \\\n",
       "0     www.us-onlinestore.com   \n",
       "1              enhance.co.jp   \n",
       "2             rizapgroup.com   \n",
       "3                asajikan.jp   \n",
       "4                   mocom.tv   \n",
       "...                      ...   \n",
       "6745              nsfwkr.net   \n",
       "6746            gck99.com.tw   \n",
       "6747       hk-bestcasino.com   \n",
       "6748              nsfwkr.net   \n",
       "6749               bj.58.com   \n",
       "\n",
       "                                           privacy_link  \\\n",
       "0     https://www.us-onlinestore.com/law.html#area-p...   \n",
       "1                 https://enhance.co.jp/privacy-policy/   \n",
       "2                        https://rizapgroup.com/privacy   \n",
       "3                           https://asajikan.jp/privacy   \n",
       "4                 https://mocom.tv/payment_policy.phtml   \n",
       "...                                                 ...   \n",
       "6745        https://nsfwkr.net/bbs/register.php#privacy   \n",
       "6746                   https://gck99.com.tw/privacy.php   \n",
       "6747          https://hk-bestcasino.com/privacy-policy/   \n",
       "6748        https://nsfwkr.net/bbs/page.php?hid=privacy   \n",
       "6749  https://bj.58.com/caishui/54996963653044x.shtm...   \n",
       "\n",
       "                                                   hash init_language  \\\n",
       "0     9ac6380d1d17249bf50dd2f9e43c545b25ea820e2c28e5...         Japan   \n",
       "1     0fb0f105dfa22b83c4c15d0665ce7c97f7c9577c225b5b...         Japan   \n",
       "2     1de0c851c94d9d92fc1ec03635321420fb60c159661ec1...         Japan   \n",
       "3     724587d0bdf7fa450cb3bf22b397ced291053b4db05377...         Japan   \n",
       "4     2e8b98375e71893f56667adcd62d32256b05dbf126571d...         Japan   \n",
       "...                                                 ...           ...   \n",
       "6745  753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...        Korean   \n",
       "6746  1f30698e7b51975a02398b3b2348c99ccb113797a52a59...        Korean   \n",
       "6747  4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...        Korean   \n",
       "6748  c1ad8e75c78c554eda26b8c1c577dc13d76132587ca4a6...        Korean   \n",
       "6749  953a4aaf51075d3bb37f3ae1053f1a89cb0f646b413416...        Korean   \n",
       "\n",
       "      valid_language  text_content  \n",
       "0                NaN           NaN  \n",
       "1                NaN           NaN  \n",
       "2                NaN           NaN  \n",
       "3                NaN           NaN  \n",
       "4                NaN           NaN  \n",
       "...              ...           ...  \n",
       "6745             NaN           NaN  \n",
       "6746             NaN           NaN  \n",
       "6747             NaN           NaN  \n",
       "6748             NaN           NaN  \n",
       "6749             NaN           NaN  \n",
       "\n",
       "[6750 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_links_df_v1 = pd.read_csv(\"../policy_corpus/privacy_links_df.csv\")\n",
    "policy_links_df_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328\n",
      "1328\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "\n",
    "Website = Query()\n",
    "chinese_policy_links = []\n",
    "policy_links_table = db.table('policy_links')\n",
    "# filter where country is China\n",
    "chinese_policy_links = policy_links_table.search(Query().country == \"China\")\n",
    "print(len(chinese_policy_links))\n",
    "\n",
    "chinese_policy_links = [dict(val) for val in chinese_policy_links]\n",
    "print(len(chinese_policy_links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3008\n"
     ]
    }
   ],
   "source": [
    "db1 = TinyDB('websites_by_language2.json')\n",
    "# print total items in 'policy_links' table\n",
    "\n",
    "# read json into dictionary\n",
    "with open('websites_by_language2.json') as json_file:\n",
    "    chinese_policy_domains2 = json.load(json_file)\n",
    "\n",
    "len(chinese_policy_domains2[\"policy_links\"])\n",
    "\n",
    "for _, value in chinese_policy_domains2[\"policy_links\"].items():\n",
    "    # print(type(value))\n",
    "    # domain_name = value['domain']\n",
    "    # add value to chinese_policy_links list\n",
    "    chinese_policy_links.append(value)\n",
    "    # break\n",
    "\n",
    "print(len(chinese_policy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining domains: 1021\n"
     ]
    }
   ],
   "source": [
    "# read top3K_chinese_mandarin_domains.txt into a list\n",
    "top3K_chinese_mandarin_domains = []\n",
    "with open(\"top3K_chinese_mandarin_domains.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        top3K_chinese_mandarin_domains.append(line.strip())\n",
    "\n",
    "for val in chinese_policy_links:\n",
    "    # print(val['domain'])\n",
    "    if val['domain'] in top3K_chinese_mandarin_domains:\n",
    "        top3K_chinese_mandarin_domains.remove(val['domain'])\n",
    "\n",
    "print(f\"Total remaining domains: {len(top3K_chinese_mandarin_domains)}\") \n",
    "\n",
    "# rewrite remaining_domains to top3K_chinese_mandarin_domains.txt only\n",
    "with open(\"top3K_chinese_mandarin_domains.txt\", \"w\") as f:\n",
    "    for domain in top3K_chinese_mandarin_domains:\n",
    "        f.write(domain + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_privacy_links_df = pd.DataFrame(columns=[\"domain\", \"privacy_link\", \"hash\", \"init_language\", \"valid_language\", \"text_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>privacy_link</th>\n",
       "      <th>hash</th>\n",
       "      <th>init_language</th>\n",
       "      <th>valid_language</th>\n",
       "      <th>text_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quark.cn</td>\n",
       "      <td>https://www.amazon.cn/gp/help/customer/display...</td>\n",
       "      <td>d817382b418886458aee75f492f3a35e44f0f9bb23d4b7...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yy.com</td>\n",
       "      <td>https://emuserh5.eastmoney.com/useragreement/yszc</td>\n",
       "      <td>b941b84035d3611a1500a8b61353559c0ff324f06ce5a6...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yy.com</td>\n",
       "      <td>https://docs.getui.com/privacy/</td>\n",
       "      <td>12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zuimeitianqi.com</td>\n",
       "      <td>https://new.qq.com/sv1/agreement/jubaoxuzhi.html</td>\n",
       "      <td>aac6ba34e1e6bf8c653d06da31c132dafdb06ee496a92f...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tudou.com</td>\n",
       "      <td>https://docs.getui.com/privacy/</td>\n",
       "      <td>12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>yancong1.com</td>\n",
       "      <td>https://www.hexsen.com/nginx-common-security-s...</td>\n",
       "      <td>f65a3ccdbe984d4e68c6cdb84e7d0048ca8d526339d8d1...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>weilaimanhua.com</td>\n",
       "      <td>https://sanygroup.com/privacy/</td>\n",
       "      <td>3674d16895658cc558e65774f686b9ebef49177433ed5c...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>moterse.com</td>\n",
       "      <td>https://sanygroup.com/privacy/</td>\n",
       "      <td>3674d16895658cc558e65774f686b9ebef49177433ed5c...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>dafajidian.com</td>\n",
       "      <td>https://sanygroup.com/privacy/</td>\n",
       "      <td>3674d16895658cc558e65774f686b9ebef49177433ed5c...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>hbgd518.com</td>\n",
       "      <td>https://sanygroup.com/privacy/</td>\n",
       "      <td>3674d16895658cc558e65774f686b9ebef49177433ed5c...</td>\n",
       "      <td>China</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1685 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                domain                                       privacy_link  \\\n",
       "0             quark.cn  https://www.amazon.cn/gp/help/customer/display...   \n",
       "1               yy.com  https://emuserh5.eastmoney.com/useragreement/yszc   \n",
       "2               yy.com                    https://docs.getui.com/privacy/   \n",
       "3     zuimeitianqi.com   https://new.qq.com/sv1/agreement/jubaoxuzhi.html   \n",
       "4            tudou.com                    https://docs.getui.com/privacy/   \n",
       "...                ...                                                ...   \n",
       "1680      yancong1.com  https://www.hexsen.com/nginx-common-security-s...   \n",
       "1681  weilaimanhua.com                     https://sanygroup.com/privacy/   \n",
       "1682       moterse.com                     https://sanygroup.com/privacy/   \n",
       "1683    dafajidian.com                     https://sanygroup.com/privacy/   \n",
       "1684       hbgd518.com                     https://sanygroup.com/privacy/   \n",
       "\n",
       "                                                   hash init_language  \\\n",
       "0     d817382b418886458aee75f492f3a35e44f0f9bb23d4b7...         China   \n",
       "1     b941b84035d3611a1500a8b61353559c0ff324f06ce5a6...         China   \n",
       "2     12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...         China   \n",
       "3     aac6ba34e1e6bf8c653d06da31c132dafdb06ee496a92f...         China   \n",
       "4     12cb3886763269f6692e864b7c30d11e31b68cb3b2ec4b...         China   \n",
       "...                                                 ...           ...   \n",
       "1680  f65a3ccdbe984d4e68c6cdb84e7d0048ca8d526339d8d1...         China   \n",
       "1681  3674d16895658cc558e65774f686b9ebef49177433ed5c...         China   \n",
       "1682  3674d16895658cc558e65774f686b9ebef49177433ed5c...         China   \n",
       "1683  3674d16895658cc558e65774f686b9ebef49177433ed5c...         China   \n",
       "1684  3674d16895658cc558e65774f686b9ebef49177433ed5c...         China   \n",
       "\n",
       "     valid_language text_content  \n",
       "0                                 \n",
       "1                                 \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 \n",
       "...             ...          ...  \n",
       "1680                              \n",
       "1681                              \n",
       "1682                              \n",
       "1683                              \n",
       "1684                              \n",
       "\n",
       "[1685 rows x 6 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for val in chinese_policy_links:\n",
    "    for link in val['all_links']:\n",
    "        if (\"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link):\n",
    "            hash_object = hashlib.sha256(link.encode())\n",
    "            hex_dig = hash_object.hexdigest()\n",
    "            chinese_privacy_links_df = chinese_privacy_links_df._append({\"domain\": val['domain'], \"privacy_link\": link, \"hash\": hex_dig, \"init_language\": \"China\", \"valid_language\":\"\", \"text_content\": \"\"}, ignore_index=True)\n",
    "\n",
    "chinese_privacy_links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "# in chinese_privacy_links_df, check if url domain of privacy link differs from domain in the dataframe for corresponding row. Create another column named \"domain_match\" to store the result of the comparison.\n",
    "chinese_privacy_links_df['domain_match'] = chinese_privacy_links_df.apply(lambda x: urlparse(x['privacy_link']).netloc == x['domain'], axis=1)\n",
    "\n",
    "# check if domain_match is False, then update the domain column with the domain from the privacy link using urlparse\n",
    "chinese_privacy_links_df['domain'] = np.where(chinese_privacy_links_df['domain_match'] == False, chinese_privacy_links_df['privacy_link'].apply(lambda x: urlparse(x).netloc), chinese_privacy_links_df['domain'])\n",
    "\n",
    "# removing duplicate privacy links\n",
    "chinese_privacy_links_df = chinese_privacy_links_df.drop_duplicates(subset=['privacy_link'])\n",
    "print(len(chinese_privacy_links_df))\n",
    "\n",
    "chinese_privacy_links_df.to_csv(\"../policy_corpus/chinese_privacy_links_df_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e51359a949953ebbd2bbeb8d97bd60677a5b611778e662aa1cbc8a61c0aa47d0\n"
     ]
    }
   ],
   "source": [
    "temp_privacy_link = 'https://products.s.kaspersky-labs.com/homeuser/kpmwin9.0/24.1.0.258/chinese_simplified-20240703_104538/3834353037377c44454c7c4e554c4c/eula_zh-Hans.html'\n",
    "hash_temp_privacy_link = hashlib.sha256(temp_privacy_link.encode()).hexdigest()\n",
    "print(hash_temp_privacy_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "威胁管理和防御解决方案 |  卡巴斯基 跳到主体内容 转到主页 家庭用户 卡巴斯基 反病毒软件 卡巴斯基 安全软件 卡巴斯基 全方位安全软件 卡巴斯基密码管理器 续费授权许可 支持 试用和下载 企业 KSOS 门户 试用和下载 续费 SMB 授权许可 查找合作伙伴 公司帐户 卡巴斯基TIP 支持 云控制台 合作伙伴 查找经销商 查找分销商 合作伙伴计划 关于我们 认识我们 联系我们 新闻中心 新闻稿 赞助项目 招贤纳士 家庭用户 企业 合作伙伴 关于我们 我的账户 美洲 América Latina Brasil United States Canada - English Canada - Français 非洲 Afrique Francophone Algérie Maroc South Africa Tunisie 中东 Middle East (English) الشرق الأوسط  (عربى) 西欧 Belgique & Luxembourg Danmark Deutschland & Schweiz España France Italia & Svizzera Nederland Norge Österreich Portugal Sverige Suomi United Kingdom 东欧 Česká republika Magyarország Polska România Srbija Türkiye Ελλάδα (Greece) България (Bulgaria) Қазақстан - Русский Қазақстан - Қазақша Россия и Белару́сь (Russia & Belarus) Україна (Ukraine) 亚太地区 Australia India Indonesia (Bahasa) New Zealand Việt Nam ไทย (Thailand) 한국 (Korea) 中国 (China) 中国香港 (Hong Kong SAR) 中国台湾 (Taiwan) 日本語 (Japan) 其他地区 全球网站 主页 大型企业安全 解决方案 卡巴斯基威胁管理和防御 解决方案 卡巴斯基威胁管理和防御 由威胁情报提供支持的高级保护 取得联系 下载白皮书* （15 页） 下载产品介绍 （2 页） 了解更多： 概述 适用于 奖项 用途 白皮书 白皮书 与此解决方案相关的内容 威胁管理和防御 数字化转型给 IT 安全带来了隐患。要获得竞争优势并保持客户和合作伙伴的忠诚度，您的组织必须保护业务连续性，并为关键资产、企业数据和整个 IT 基础架构提供可靠的保护。这意味着将您的 IT 安全策略提升到全新水准。 卡巴斯基威胁管理与防御解决方案以独特的方式将领先的技术和服务结合在一起，以支持实施自适应安全策略 - 从而帮助您的安全团队防范攻击、快速检测独特的新威胁、快速准确地响应实时事件并预测未来威胁。 有效防范威胁 强化系统并提高对整体网络安全风险的认知，是实现高效自动化和有效保护的第一步。 迅速检测入侵，避免其造成实际损失 检测新型独特威胁（恶意和非恶意软件）的能力是网络安全成熟度的关键决定因素。 全方位响应事件 在有人工指导的情况下针对最复杂的威胁集中做出快速响应对于有效的事件管理至关重要。 预测未来网络威胁 将内部调查结果与最新威胁情报对照比较，帮助准确预测和应对潜在的未来威胁。 适用目标 卡巴斯基深入了解复杂威胁的内部机制，并凭借这样的认知开发出由一系列技术和服务构成的战略性产品组合，能够提供适用于如下细分领域的完全集成式自适应安全方法： 企业组织 政府 金融服务 能源、天然气和石油 电信 零售 独立测试 卡巴斯基反针对性攻击平台 ICSA Labs：高级威胁防御测试 卡巴斯基反针对性攻击平台 SE 实验室入侵响应检测：AAA 奖项 卡巴斯基端点检测与响应 MITRE 评估：APT 29 第 2 轮 Recognition 卡巴斯基反针对性攻击平台以及卡巴斯基端点检测与响应 Gartner Peer Insights 2020 年端点检测与响应“客户选择奖” 卡巴斯基反针对性攻击平台以及卡巴斯基端点检测与响应 2020 年 Radicati APT 防护市场象限 实际应用 适合您的组织的综合性防御机制 供卓越防御机制的专业产品， 包含卡巴斯基反定向攻击平台 （以卡巴斯基端点检测和响应为核心）， 此外还有一系列的卡巴斯基网络安全服务 。该解决方案可以集成到您当前的企业战略中，以应对复杂威胁、补充现有保护技术并支持与 SIEM/SOC 的交互。这造就了一种帮助企业抵御定向攻击、降低风险、减少网络事件造成的直接损失的整体化方法。 加强您的安全运营中心 为了抵御最复杂的现代网络威胁，并适应动态多变的威胁环境中的持续挑战，您的安全运营中心 (SOC) 应该配备先进的技术、有强大的威胁情报加持，并拥有具备所有必要知识和专业技术的专业人员。这一切相融合，成就了可以抵御最为复杂的 APT 式攻击和定向恶意活动的完整防御周期。在卡巴斯基威胁管理和防御的框架内，我们提供由先进防御技术和服务构成的完善工具库，助您大幅度提升 SOC 效率。 选择您理想的技术与服务平衡点 卡巴斯基威胁管理与防御解决方案提供了安全产品、支持与服务的强大组合。为了提升您的团队的专业水准，卡巴斯基还提供一系列技能培训计划以及威胁情报数据，可有效扩充内部调查结果。我们提供托管式检测和响应服务，因此您可以将与事件相关的处理任务转交给我们，或者依靠卡巴斯基提供专家判断和独特威胁检测专业知识，来保护您的 IT 安全资源。无论贵公司当下和未来有哪些 IT 安全需求，我们都有合适的解决方案。 24/7 全天候高级支持 专业帮助全天候为企业保驾护航。我们的业务遍布全球 200 多个国家/地区，设有 34 个办事处，全天候为企业服务。使用我们的高级支持包，或致电我们的专业服务，确保企业受益于卡巴斯基实验室安全产品。 高级支持 专业服务 白皮书 详细了解我们全球认可的网络安全专家分享的思维领导力 下载 PDF* 企业网络安全解决方案目录 35 页 立即查看* 卡巴斯基网络安全服务为企业保驾护航 27 页 立即查看* 防御高级威胁和降低定向攻击风险 15 页 查看全部 风险 定向攻击（包括高级持久威胁 (APT)）是企业所面临的最危险的风险之一。仅仅防范大多数威胁是不够的，您还需要确保一旦发生入侵，您可以快速调查和响应威胁，并抵御进一步的攻击企图。若没有成熟的网络安全策略，企业则面临持续不断的挑战： 如果事件响应存在短板，就可能意味着处理安全事件造成的间接损失的成本甚至超过直接损失 在对抗高级威胁时，没有任何安全产品的效果可以与高素质专家相比 缺乏可见性和证据会造成重大损失 即使漏失一个事件，也会给身处数字转型过程中的企业造成严重损害 当今独特的攻击从谋划之初就以逃过传统控制和保护解决方案为宗旨 此时，投资于传统安全解决方案的回报可能不尽人意。 与此解决方案相关的内容 卡巴斯基反针对性攻击平台 了解隐藏在边界保护之下的高级网络威胁 了解更多 卡巴斯基端点检测与响应专家版 通过降低高级威胁带来的风险来防止业务中断 了解更多 卡巴斯基专属安全网络 全面的威胁情报数据库，面向隔离的网络和严格限制数据共享的应用 了解更多 卡巴斯基网络安全解决方案 领先的多层级端点保护平台，利用高级的数据科学和全球威胁情报 了解更多 联系我们并与我们的一位专家讨论卡巴斯基如何帮助您保护您的企业。 取得联系 标星号[*]的文件与视频为英语 家用产品 卡巴斯基 反病毒软件 卡巴斯基 安全软件 卡巴斯基 全方位安全软件 所有产品 免费反病毒软件 1-49 名员工的小型企业 卡巴斯基 中小企业安全解决方案 所有产品 50-999 名员工的中型企业 基础版 网络安全解决方案 标准版 网络安全解决方案 高级版 网络安全解决方案 所有产品 1000 名员工以上的大型企业 网络安全服务 卡巴斯基威胁管理和防御 卡巴斯基网络安全 Hybrid Cloud Security Cybersecurity Training Threat Intelligence 所有解决方案 © 2024 AO Kaspersky Lab 自适应安全技术基于专利 CN201821502 “信息设备的自适应安全性”及其在美国、俄罗斯和欧盟地区的同类专利。 京ICP备12053225号 京公网安备 11010102001169号 隐私策略 Cookies 反腐败政策 许可协议 B2C 许可协议 B2B 联系我们 关于我们 合作伙伴 资源中心 新闻稿 网站导航 网站导航 中国 (China) 美洲 América Latina Brasil United States Canada 非洲 Afrique Francophone Algérie Maroc South Africa Tunisie 中东 Middle East (English) الشرق الأوسط  (عربى) 西欧 Belgique & Luxembourg Danmark Deutschland & Schweiz España France Italia & Svizzera Nederland & België Norge Österreich Portugal Sverige Suomi United Kingdom 东欧 Česká republika Magyarország Polska România Srbija Türkiye Ελλάδα (Greece) България (Bulgaria) Россия и Белару́сь (Russia & Belarus) Україна (Ukraine) 亚太地区 Australia India Indonesia (Bahasa) New Zealand Việt Nam ไทย (Thailand) 한국 (Korea) 中国 (China) 中国香港 (Hong Kong) 中国台灣 (Taiwan) 日本語 (Japan) 其他地区 全球网站\n"
     ]
    }
   ],
   "source": [
    "url = \"https://kaspersky.com.cn/enterprise-security/threat-management-defense-solution\"\n",
    "# download html, remove html tags and print\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "text = soup.get_text(separator=' ', strip=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# read all files into a list in the directory ./policy_corpus/policies_manual_add\n",
    "import os\n",
    "\n",
    "all_files = os.listdir(\"../policy_corpus/policies_manual_add\")\n",
    "all_files = [file.split(\".txt\")[0] for file in all_files if file.endswith(\".txt\")]\n",
    "print(len(all_files))\n",
    "\n",
    "# from ../policy_corpus/privacy_links_df_updated_v2.csv, read the csv file into a dataframe\n",
    "privacy_links_df = pd.read_csv(\"../policy_corpus/privacy_links_df_updated_v2.csv\")\n",
    "\n",
    "# check if each value in all_files is in the privacy_links_df['hash'] column\n",
    "# if not, then add the value to a list\n",
    "missing_files = []\n",
    "for file in all_files:\n",
    "    if file not in privacy_links_df['hash'].values:\n",
    "        missing_files.append(file)\n",
    "\n",
    "print(len(missing_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
