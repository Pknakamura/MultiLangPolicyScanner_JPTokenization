{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\mhass\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Do not run this cell if you are running the notebook on your local machine everytimne\n",
    "\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for imitating GET request\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# import libraries for language detection\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# import libraries for web scraping\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tinydb import Query, TinyDB\n",
    "from langcodes import standardize_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Website and Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "NAVER 상단영역 바로가기 서비스 메뉴 바로가기 새소식 블록 바로가기 쇼핑 블록 바로가기 관심사 블록 바로가기 MY 영역 바로가기 위젯 보드 바로가기 보기 설정 바로가기 검색 검색 입력도구 자동완성/최근검색어펼치기 최근 검색어 전체삭제 검색어 저장 기능이 꺼져 있습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 최근 검색어 내역이 없습니다. 설정이 초기화 된다면 도움말 을 확인해주세요. 자동저장 끄기 도움말 닫기 CUE 대화하듯 질문해 보세요 이 정보가 표시된 이유 검색어와 포함된 키워드를 기반으로 AI 기술을 활용하여 연관된 추천 질문을 제공합니다. 레이어 닫기 이전 다음 자세히보기 관심사를 반영한 컨텍스트 자동완성 도움말 컨텍스트 자동완성 컨텍스트 자동완성 ON/OFF 설정은 해당기기(브라우저)에 저장됩니다. 자세히 보기 동일한 시간대・연령대・남녀별 사용자 그룹의 관심사에 맞춰 자동완성을 제공합니다. 자세히 보기 네이버 로그인 컨텍스트 자동완성 레이어 닫기 자동완성 끄기 도움말 신고 닫기\n",
      "\n",
      "Detected language: ko\n"
     ]
    }
   ],
   "source": [
    "website = \"https://www.naver.com/\"\n",
    "\n",
    "\n",
    "# Ensure consistent results from langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    # url = 'https://www.example.com'\n",
    "    \n",
    "    # Fetch and convert the website to text\n",
    "    text_content = fetch_and_convert_website(website)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        print(\"Text content extracted from the website:\")\n",
    "        print(text_content)\n",
    "        \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language:\n",
    "            print(f\"\\nDetected language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sites\": [\n",
      "    {\n",
      "      \"domain\": \"atelos.net\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:00:26\",\n",
      "      \"pagesPerVisit\": 1.64139495235869,\n",
      "      \"bounceRate\": 0.375844431627953,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"babycenter.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\n",
      "      \"rankChange\": 0,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:15\",\n",
      "      \"pagesPerVisit\": 2.6761096536356,\n",
      "      \"bounceRate\": 0.47973039156597,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"stanfordchildrens.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\n",
      "      \"rankChange\": 6,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:02:17\",\n",
      "      \"pagesPerVisit\": 2.25775662691671,\n",
      "      \"bounceRate\": 0.729761362667345,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"parents.com\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\n",
      "      \"rankChange\": 2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:12\",\n",
      "      \"pagesPerVisit\": 1.64635804257872,\n",
      "      \"bounceRate\": 0.731465612975668,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    },\n",
      "    {\n",
      "      \"domain\": \"kidshealth.org\",\n",
      "      \"favicon\": \"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\n",
      "      \"rankChange\": -2,\n",
      "      \"categoryId\": \"health/childrens_health\",\n",
      "      \"visitsAvgDurationFormatted\": \"00:01:03\",\n",
      "      \"pagesPerVisit\": 1.43497873007992,\n",
      "      \"bounceRate\": 0.798248432039549,\n",
      "      \"isBlackListed\": false,\n",
      "      \"isNewRank\": false\n",
      "    }\n",
      "  ],\n",
      "  \"categoryId\": \"health/childrens_health\",\n",
      "  \"countryAlpha2Code\": \"KR\",\n",
      "  \"snapshotDate\": \"2024-05-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rewrite selenium script which open similarweb.com and get top websites for korea-republic-of, health, childrens-health\n",
    "# and save the response to a file\n",
    "\n",
    "\n",
    "def get_top_websites_selenium(country, category, subcategory):\n",
    "    # add user agent to headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.similarweb.com/api/gettopwebsites?country={country}&category={category}&subcategory={subcategory}\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    response = driver.page_source\n",
    "    # extract top websites from response\n",
    "    # <html><head><meta name=\"color-scheme\" content=\"light dark\"><meta charset=\"utf-8\"></head><body><pre>{\"sites\":[{\"domain\":\"atelos.net\",\"favicon\":\"https://site-images.similarcdn.com/image?url=atelos.net&amp;t=2&amp;s=1&amp;h=0c014068c9b4d14ef76a707c2eee40dfbad0c18e281d1a0d68f9bb7b87ea4a14\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:00:26\",\"pagesPerVisit\":1.6413949523586921,\"bounceRate\":0.3758444316279532,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"babycenter.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=babycenter.com&amp;t=2&amp;s=1&amp;h=f3ad9c6f1997a429dd4e140a7c32b5f768d8b23e8a3c9aa8353f3d63af7a1b55\",\"rankChange\":0,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:15\",\"pagesPerVisit\":2.6761096536355957,\"bounceRate\":0.47973039156597036,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"stanfordchildrens.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=stanfordchildrens.org&amp;t=2&amp;s=1&amp;h=cf717fe028b2010f8ad94d01b45db82bce6f6b4887864668d200a311002a42d2\",\"rankChange\":6,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:02:17\",\"pagesPerVisit\":2.257756626916711,\"bounceRate\":0.7297613626673453,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"parents.com\",\"favicon\":\"https://site-images.similarcdn.com/image?url=parents.com&amp;t=2&amp;s=1&amp;h=098e7898a8b60d901990557e20c2bd7012f960a3eaece420c08f40e1ac3a602a\",\"rankChange\":2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:12\",\"pagesPerVisit\":1.6463580425787216,\"bounceRate\":0.7314656129756678,\"isBlackListed\":false,\"isNewRank\":false},{\"domain\":\"kidshealth.org\",\"favicon\":\"https://site-images.similarcdn.com/image?url=kidshealth.org&amp;t=2&amp;s=1&amp;h=28a8bd381e5d8042a077325236b35e6c62d91026ce2b4a160ce4242c4113b329\",\"rankChange\":-2,\"categoryId\":\"health/childrens_health\",\"visitsAvgDurationFormatted\":\"00:01:03\",\"pagesPerVisit\":1.4349787300799237,\"bounceRate\":0.7982484320395493,\"isBlackListed\":false,\"isNewRank\":false}],\"categoryId\":\"health/childrens_health\",\"countryAlpha2Code\":\"KR\",\"snapshotDate\":\"2024-05-01T00:00:00+00:00\"}</pre><div class=\"json-formatter-container\"></div></body></html>\n",
    "    # the response looks like above\n",
    "\n",
    "    response = response.split(\"<pre>\")[1].split(\"</pre>\")[0]\n",
    "    print(response)\n",
    "\n",
    "    # driver.close()\n",
    "    with open(\"top_websites.html\", \"w\") as f:\n",
    "        f.write(response)\n",
    "\n",
    "get_top_websites_selenium(\"korea-republic-of\", \"health\", \"childrens-health\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_website(country):\n",
    "    url = \"https://www.ahrefs.com/top/\" + country\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # look for tbody table\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "\n",
    "\n",
    "    top100 = tables[0]\n",
    "\n",
    "    # create an empty dataframe with columns rank, url, traffic, increase_traffic\n",
    "    # df_website = pd.DataFrame(columns=[\"rank\", \"url\", \"traffic\", \"increase_traffic\"])\n",
    "    # create a dictionary with keys rank, url, traffic, increase_traffic \n",
    "    list_website = []\n",
    "\n",
    "    dict_website = {}\n",
    "\n",
    "    for row in top100.find_all(\"tr\"):\n",
    "        cell_values = [cell.text for cell in row.find_all(\"td\")]\n",
    "        cell_values.pop(1)\n",
    "\n",
    "        url = cell_values[1]\n",
    "        rank = cell_values[0]\n",
    "        traffic = cell_values[2]\n",
    "        increase_traffic = cell_values[3]\n",
    "\n",
    "        # add to dictionary\n",
    "        dict_website[\"rank\"] = rank\n",
    "        dict_website[\"url\"] = url\n",
    "        dict_website[\"traffic\"] = traffic\n",
    "        dict_website[\"increase_traffic\"] = increase_traffic\n",
    "\n",
    "        # add to list\n",
    "        list_website.append(dict_website)\n",
    "\n",
    "\n",
    "        # df_website = df_website._append(pd.Series(cell_values, index=df_website.columns), ignore_index=True)\n",
    "       \n",
    "    return list_website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB of Websites by Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'en': 63926, 'zh-cn': 16652, 'id': 8471, 'ru': 4302, 'es': 3924, 'de': 3500, 'ja': 3380, 'pt': 3346, 'ko': 2730, 'fr': 2618, 'vi': 1817, 'it': 1497, 'tr': 1457, 'nl': 1152, 'pl': 1105, 'ar': 1085, 'fa': 1067, 'th': 968, 'ro': 660, 'uk': 612, 'tl': 589, 'cs': 445, 'el': 384, 'sv': 383, 'hr': 356, 'no': 355, 'hi': 338, 'hu': 337, 'da': 336, 'fi': 271, 'et': 262, 'bg': 253, 'ca': 242, 'bn': 228, 'sk': 206, 'so': 171, 'he': 169, 'lt': 131, 'sl': 108, 'sw': 105, 'af': 95, 'mk': 67, 'lv': 62, 'ta': 52, 'cy': 51, 'mr': 51, 'sq': 50, 'te': 37, 'ml': 30, 'kn': 26, 'ne': 23, 'gu': 18, 'ur': 16, 'zh-tw': 15, 'pa': 2})\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "\n",
    "\n",
    "# list all unique languages and their count of websites\n",
    "languages = websites_table.all()\n",
    "languages = [lang['language'] for lang in languages]\n",
    "# print count of each language along with language\n",
    "from collections import Counter\n",
    "lang_count = Counter(languages)\n",
    "print(lang_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'te', 'ca', 'ru', 'ar', 'zh-cn', 'fa', 'fr', 'id', 'de', 'sl', 'gu', 'fi', 'nl', 'ur', 'ml', 'ne', 'he', 'cs', 'pl', 'en', 'pa', 'hi', 'cy', 'bn', 'hr', 'vi', 'el', 'ta', 'kn', 'tl', 'lv', 'zh-tw', 'af', 'ko', 'sq', 'th', 'ja', 'es', 'et', 'sw', 'lt', 'it', 'bg', 'pt', 'sk', 'mk', 'tr', 'uk', 'hu', 'ro', 'no', 'da', 'mr', 'so', 'sv'}\n",
      "Total number of unique languages: 55\n"
     ]
    }
   ],
   "source": [
    "# list all unique languages on a new line and total number of unique languages\n",
    "unique_languages = set(languages)\n",
    "print(unique_languages)\n",
    "print(f\"Total number of unique languages: {len(unique_languages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total websites in English: 63926\n",
      "Total websites in Chinese(simplified): 16652\n",
      "Total websites in Chinese(traditional): 15\n",
      "Total websites in Korean: 2730\n",
      "Total websites in Japanese: 3380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print count of all website for english, chinese, korean, japanese languages\n",
    "\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "print(f\"Total websites in English: {len(websites)}\")\n",
    "\n",
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "print(f\"Total websites in Chinese(simplified): {len(websites)}\")\n",
    "\n",
    "# zh-tw\n",
    "websites = websites_table.search(Website.language == \"zh-tw\")\n",
    "print(f\"Total websites in Chinese(traditional): {len(websites)}\")\n",
    "\n",
    "\n",
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "print(f\"Total websites in Korean: {len(websites)}\")\n",
    "\n",
    "# get all websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "print(f\"Total websites in Japanese: {len(websites)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Random Websites from the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 random domains for eng, zh-cn, ko, ja languages\n",
    "# get all websites for english language\n",
    "Website = Query()\n",
    "websites = websites_table.search(Website.language == \"en\")\n",
    "# get 25 random websites\n",
    "random_en_websites = random.sample(websites, 25)\n",
    "print(\"Random websites in English:\")\n",
    "for website in random_en_websites:\n",
    "    print(website['domain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Chinese(simplified):\n",
      "piggymates.com\n",
      "xiningchina.com\n",
      "qianbangjiaoyu.com\n",
      "dnbbm.com\n",
      "sanygroup.com\n",
      "qxjf-art.com\n",
      "xiamiaoyangzhi.com\n",
      "cqkuaisu.com\n",
      "cshuaqun.com\n",
      "gongyeqg.com\n",
      "18avx.com\n",
      "chinabrx.com\n",
      "rendaikuan.com\n",
      "szyueshan.com\n",
      "qdzhiruitong.com\n",
      "freereceivesms.com\n",
      "gggoodgame.com\n",
      "hellobike.com\n",
      "allstar-era.com\n",
      "kxunchina.com\n",
      "zgrtcm.com\n",
      "yumerzx.com\n",
      "leg1678.com\n",
      "shenzhen-nanning.com\n",
      "262196.cn\n",
      "Random websites in Chinese(simplified):\n",
      "fytlsm.com\n",
      "hapclock.com\n",
      "jtk100.com\n",
      "iduduapp.com\n",
      "sdbxqy.com\n",
      "njchangxue.com\n",
      "daimonchina.com\n",
      "zhaoshimy.com\n",
      "jutu360.com\n",
      "wodessay.com\n",
      "yataixuanhao.com\n",
      "hztaiyi.com\n",
      "sujienk.com\n",
      "liusuliusu.com\n",
      "cmdjdkj.com\n",
      "hycmzc.com\n",
      "ytshenhong.com\n",
      "znote8899.com\n",
      "jxlesong.com\n",
      "jax-china.com\n",
      "51haotou.com\n",
      "xinglistqy.com\n",
      "mtsbjy.com\n",
      "ysu.edu.cn\n",
      "tianqingshiyin.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for chinese language\n",
    "websites = websites_table.search(Website.language == \"zh-cn\")\n",
    "# get 25 random websites\n",
    "random_ZhCn_websites1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for zh-cn language which is different from the previous 25 websites in random_ZhCn_websites1\n",
    "random_ZhCn_websites2 = random.sample(websites, 25)\n",
    "# check random_ZhCn_websites2 is different from random_ZhCn_websites1\n",
    "for website in random_ZhCn_websites2:\n",
    "    if website in random_ZhCn_websites1:\n",
    "        print(\"Random websites in Chinese(simplified) are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ZhCn_websites2.remove(website)\n",
    "        random_ZhCn_websites2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Chinese(simplified):\")\n",
    "for website in random_ZhCn_websites2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean:\n",
      "21stcbc.org\n",
      "lcd1004.co.kr\n",
      "pvxywg.com\n",
      "wtwt248.com\n",
      "papalah.pw\n",
      "jusoya10.com\n",
      "netpro.co.kr\n",
      "bestone-work.com\n",
      "kassashair.com\n",
      "chroscience.com\n",
      "studypatent.com\n",
      "ovotv.com\n",
      "chosong.co.kr\n",
      "xsmzjc.com\n",
      "daehangreenpower.com\n",
      "xingyueboke.com\n",
      "womaneconomy.co.kr\n",
      "keyixs.com\n",
      "xn--939au0g3vw1iaq8a469c.kr\n",
      "19878719.com\n",
      "scshangting.com\n",
      "rongbaodianmo.com\n",
      "mgyqw.com\n",
      "dabangapp.com\n",
      "qiutianxia29.com\n",
      "Random websites in Korean:\n",
      "uqcjvpk.cn\n",
      "mobilitytv.co.kr\n",
      "whichav.video\n",
      "jxcgyl.com\n",
      "interpark.com\n",
      "jiexunec.com\n",
      "1234567.com.cn\n",
      "neworbis.com\n",
      "heywakeup.com.tw\n",
      "wozai-travel.com\n",
      "smdv.kr\n",
      "ezalba.co.kr\n",
      "haobofangshui.com\n",
      "torrentsee217.com\n",
      "ruantongzhi.com\n",
      "cmuma.xyz\n",
      "ttlock.com\n",
      "jmdoor.com.tw\n",
      "newtoki.help\n",
      "sxwlz.com\n",
      "sdmeixiusy.com\n",
      "optisun.vip\n",
      "clean-clean-peru.com\n",
      "11toon112.com\n",
      "aniweek.com\n"
     ]
    }
   ],
   "source": [
    "# get all websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "# get 25 random websites\n",
    "random__ko_websites_1 = random.sample(websites, 25)\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_1:\n",
    "    print(website['url'])\n",
    "\n",
    "# get anohther 25 random websites for korean language which is different from the previous 25 websites in random__ko_websites_1\n",
    "random__ko_websites_2 = random.sample(websites, 25)\n",
    "# check random__ko_websites_2 is different from random__ko_websites_1\n",
    "for website in random__ko_websites_2:\n",
    "    if website in random__ko_websites_1:\n",
    "        print(\"Random websites in Korean are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random__ko_websites_2.remove(website)\n",
    "        random__ko_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Korean:\")\n",
    "for website in random__ko_websites_2:\n",
    "    print(website['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Korean: [{'url': 'yxzwlkj.com', 'language': 'ko', 'timestamp': '2024-06-08T08:34:31.376923'}, {'url': 'jshaoou.com', 'language': 'ko', 'timestamp': '2024-06-08T07:06:15.695528'}]\n"
     ]
    }
   ],
   "source": [
    "# give two more random korean websites\n",
    "random__ko_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Korean:\", random__ko_websites_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359198.com\n",
      "toonkor326.com\n",
      "dbcnews.co.kr\n",
      "hbwocheng.com\n",
      "whichav.video\n",
      "pinksisly.com\n",
      "chenzhongtech.com\n",
      "bluezz.com.tw\n",
      "gdzhukou.com\n",
      "oplove16.com\n",
      "yamoa3.site\n",
      "zhongfa1688.com\n",
      "douyuanxiuhe.com\n",
      "yp.com.hk\n",
      "imendon.com\n",
      "fxfx217.com\n",
      "htwhbook.com\n",
      "yedam.com\n",
      "homeplus.co.kr\n",
      "newhua99.xyz\n",
      "88p2p.com\n",
      "shyuwangfangshui.com\n",
      "fenghemp.com\n",
      "daoom.co.kr\n",
      "nfqlife.com\n",
      "evolutionplaynow.com\n",
      "limeitianhe.com\n",
      "yebigun1.mil.kr\n",
      "anpservice.net\n",
      "trfsgs.com\n"
     ]
    }
   ],
   "source": [
    "# give 30 random websites from the database for korean, chinese and japanese languages\n",
    "# get 30 random websites for korean language\n",
    "websites = websites_table.search(Website.language == \"ko\")\n",
    "random_websites = random.sample(websites, 30)\n",
    "# just list urls\n",
    "for website in random_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nobori-mart.net\n",
      "keirin-mobile.jp\n",
      "sesto.jp\n",
      "eigeki.com\n",
      "valor-luvitapp.com\n",
      "kuzen.io\n",
      "enechange.co.jp\n",
      "hanako.tokyo\n",
      "saiyasune.com\n",
      "gogojungle.co.jp\n",
      "madamefigaro.jp\n",
      "reil.co.jp\n",
      "koichidomoto-fc.net\n",
      "ganbalegends.com\n",
      "jal.co.jp\n",
      "dengekionline.com\n",
      "bribaby.jp\n",
      "hankyu.co.jp\n",
      "yutasan.co\n",
      "laxd.com\n",
      "karakubuy.com\n",
      "hellouniweb.com\n",
      "android4front.jp\n",
      "nichiga.net\n",
      "cardrush-pokemon.jp\n",
      "Random websites in Japanese:\n",
      "domonet.jp\n",
      "sabory-blog.com\n",
      "tsurisuke.com\n",
      "hero-news.com\n",
      "halmek.co.jp\n",
      "monotaro.com\n",
      "homes.co.jp\n",
      "ai-eye.jp\n",
      "pushcode.jp\n",
      "tyuemon.com\n",
      "ifdef.jp\n",
      "mangakoma.net\n",
      "sangacio.com\n",
      "jobop.jp\n",
      "rere.jp\n",
      "thp-shop.co.jp\n",
      "brandnavi-online.com\n",
      "ranking.net\n",
      "edesk.jp\n",
      "kimuratan.jp\n",
      "xn--pckua2a7gp15o89zb.com\n",
      "kawashima-ya.jp\n",
      "nippon-foundation.or.jp\n",
      "boxingnews.jp\n",
      "axa-direct.co.jp\n"
     ]
    }
   ],
   "source": [
    "# get 25 random websites for japanese language\n",
    "websites = websites_table.search(Website.language == \"ja\")\n",
    "random_ja_websites = random.sample(websites, 25)\n",
    "# just list urls\n",
    "for website in random_ja_websites:\n",
    "    print(website['url'])\n",
    "\n",
    "# get 25 more random japanese websites\n",
    "random_ja_websites_2 = random.sample(websites, 25)\n",
    "# check random_ja_websites_2 is different from random_ja_websites\n",
    "for website in random_ja_websites_2:\n",
    "    if website in random_ja_websites:\n",
    "        # print(\"Random websites in Japanese are not unique\")\n",
    "        # replace the website with a new random website\n",
    "        random_ja_websites_2.remove(website)\n",
    "        random_ja_websites_2.append(random.choice(websites))\n",
    "\n",
    "print(\"Random websites in Japanese:\")\n",
    "for website in random_ja_websites_2:\n",
    "    print(website['url'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random websites in Japanese: [{'url': 'comiful.net', 'language': 'ja', 'timestamp': '2024-06-08T09:51:24.302366'}, {'url': 'hentaiasmr.moe', 'language': 'ja', 'timestamp': '2024-06-08T02:31:00.175990'}]\n"
     ]
    }
   ],
   "source": [
    "# generate 2 more random japanese websites\n",
    "random_ja_websites_3 = random.sample(websites, 2)\n",
    "print(\"Random websites in Japanese:\", random_ja_websites_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Privacy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of user agents\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    # Add more user agents as needed\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# get free proxies from online\n",
    "def get_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # fetch proxy list from online\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    proxy_table = soup.find(\"table\", attrs={\"class\": \"table table-striped table-bordered\"})\n",
    "\n",
    "\n",
    "    # print(proxy_table)\n",
    "\n",
    "    # # Extract proxy IPs and ports\n",
    "    proxies = []\n",
    "    proxy_table = proxy_table.find(\"tbody\")\n",
    "    # print(proxy_table)\n",
    "    for row in proxy_table.find_all(\"tr\"):\n",
    "        proxies.append({\n",
    "        \"ip\":   row.find_all(\"td\")[0].string,\n",
    "        \"port\": row.find_all(\"td\")[1].string\n",
    "        })\n",
    "        # print(row)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_35.naver\n",
      "Extracting links from: https://policy.naver.com/rules/youthpolicy.html\n",
      "Extracting links from: http://www.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_16.naver\n",
      "Extracting links from: https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login\n",
      "Extracting links from: https://www.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_17.naver\n",
      "Extracting links from: https://help.naver.com/alias/search/word/word_18.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "Extracting links from: https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "Extracting links from: https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "Extracting links from: https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "Extracting links from: http://naver.com\n",
      "Extracting links from: https://policy.naver.com/policy/service.html\n",
      "Extracting links from: http://www.naver.com\n",
      "Extracting links from: https://help.naver.com\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "Extracting links from: https://right.naver.com\n",
      "Extracting links from: https://help.naver.com/\n",
      "Extracting links from: http://help.naver.com/\n",
      "Extracting links from: https://help.naver.com/support/home.nhn\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "Extracting links from: http://policy.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "Extracting links from: http://policy.naver.com/rules/service_location.html\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063\n",
      "Extracting links from: https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "Extracting links from: https://tv.naver.com/v/13644277/list/594825\n",
      "Extracting links from: https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/privacyinfo\n",
      "Extracting links from: http://www.naver.com/rules/service.html\n",
      "Extracting links from: http://www.naver.com/rules/privacy.html\n",
      "Extracting links from: https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "Extracting links from: https://www.naver.com/more.html\n",
      "Extracting links from: https://www.naver.com/policy/service.html\n",
      "Extracting links from: https://www.naver.com/policy/privacy.html\n",
      "Extracting links from: https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "Extracting links from: http://www.naver.com/rules/disclaimer.html\n",
      "Extracting links from: https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "Extracting links from: https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "Extracting links from: https://gam.naver.com/optout/main \n",
      "Error accessing https://gam.naver.com/optout/main : 404 Client Error: 404 for url: https://gam.naver.com/optout/main%20\n",
      "Extracting links from: https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "Extracting links from: https://blog.naver.com/n_privacy\n",
      "Extracting links from: https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://notice.naver.com/notices/privacypolicy/13880\n",
      "Extracting links from: https://www.naver.com/policy/youthpolicy.html\n",
      "Extracting links from: https://help.naver.com/alias/report/Protection_report.naver\n",
      "Extracting links from: https://policy.naver.com/policy/service_group.html\n",
      "Extracting links from: https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "Extracting links from: https://blog.naver.com/blogpeople/220823707644\n",
      "Extracting links from: https://help.naver.com/support/reportCenter/home.help\n",
      "Extracting links from: https://green.naver.com/\n",
      "Extracting links from: https://privacy.naver.com\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14997\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14236\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14235\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14234\n",
      "Extracting links from: https://notice.naver.com/notices/privacynid/14233\n",
      "Extracting links from: https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223471678731\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223463199093\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223455075484\n",
      "Extracting links from: https://blog.naver.com//n_privacy/223446809582\n",
      "Extracting links from: http://study.jr.naver.com/privacy/\n",
      "Extracting links from: https://jr.naver.com/\n",
      "Extracting links from: https://help.naver.com/service/5636/category/bookmark\n",
      "Extracting links from: https://jr.naver.com\n",
      "Extracting links from: https://help.naver.com/alias/report/report_m_3.naver\n",
      "Extracting links from: https://right.naver.com/\n",
      "Extracting links from: https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "Extracting links from: https://policy.naver.com/policy/privacy.html#a4\n",
      "Extracting links from: https://inoti.naver.com/inoti/main.nhn\n",
      "Extracting links from: https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "Extracting links from: http://policy.naver.com/policy/disclaimer.html\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "Extracting links from: https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "Extracting links from: http://policy.naver.com/policy/privacy.html#a2_3\n",
      "Extracting links from: http://blog.naver.com/n_privacy/80143119849\n",
      "Extracting links from: https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "Extracting links from: https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "Extracting links from: https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n",
      "All extracted links:\n",
      "https://help.naver.com/alias/search/word/word_35.naver\n",
      "https://policy.naver.com/rules/youthpolicy.html\n",
      "http://www.naver.com/\n",
      "https://help.naver.com/alias/search/word/word_16.naver\n",
      "https://www.navercorp.com\n",
      "https://help.naver.com/support/alias/search/word/word_16.naver\n",
      "https://nid.naver.com/nidlogin.login\n",
      "https://www.naver.com\n",
      "https://help.naver.com/alias/search/word/word_17.naver\n",
      "https://help.naver.com/alias/search/word/word_18.naver\n",
      "https://nid.naver.com/user2/api/route?m=routePwInquiry&lang=ko_KR\n",
      "https://www.navercorp.com/\n",
      "https://help.naver.com/support/alias/membership/p.membership/p.membership_26.naver\n",
      "https://nid.naver.com/user2/api/route?m=routeIdInquiry&lang=ko_KR\n",
      "https://nid.naver.com/user2/V2Join?m=agree&lang=ko_KR&realname=N\n",
      "http://naver.com\n",
      "https://policy.naver.com/policy/service.html\n",
      "http://www.naver.com\n",
      "https://help.naver.com\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=1441\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=532&categoryNo=16952\n",
      "https://right.naver.com\n",
      "https://help.naver.com/\n",
      "http://www.navercorp.com/\n",
      "http://recruit.navercorp.com/\n",
      "https://www.navercorp.com/nhn/company/proposalGuide.nhn\n",
      "http://help.naver.com/\n",
      "https://help.naver.com/support/home.nhn\n",
      "http://policy.naver.com/policy/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/easy_version?menu=policy_personal_information_easyVersion\n",
      "http://policy.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/policy_and_law/infographic?menu=policy_personal_information_infographic\n",
      "http://policy.naver.com/rules/service_location.html\n",
      "https://notice.naver.com/notices/LBS/14063\n",
      "https://help.naver.com/inquiry/input.help?serviceNo=1&categoryNo=15744&lang=ko\n",
      "https://tv.naver.com/v/13644277/list/594825\n",
      "https://policy.naver.com/policy/popup/privacy_agreement.html\n",
      "https://policy.naver.com/policy/privacy.html\n",
      "https://www.law.go.kr/LSW/lsInfoP.do?efYd=20200805&lsiSeq=213857#0000\n",
      "http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm\n",
      "https://centerforplainlanguage.org/learning-training/five-steps-plain-language/\n",
      "https://privacy.naver.com/privacyinfo\n",
      "http://www.naver.com/rules/service.html\n",
      "http://www.naver.com/rules/privacy.html\n",
      "https://privacy.naver.com/transparency/related_act?menu=transparency_report_understand_related_act\n",
      "http://www.law.go.kr/법령/형사소송법/(13454,20150731)\n",
      "http://www.law.go.kr/lsInfoP.do?lsiSeq=160962&efYd=20141015#0000\n",
      "http://www.law.go.kr/법령/전기통신사업법/(13011,20150120)\n",
      "https://www.naver.com/more.html\n",
      "https://recruit.navercorp.com/\n",
      "https://www.navercorp.com/naver/proposalInquire\n",
      "https://www.naver.com/policy/service.html\n",
      "https://www.naver.com/policy/privacy.html\n",
      "https://nid.naver.com/user2/help/changeUserInfo.nhn?lang=&menu=nid\n",
      "http://www.naver.com/rules/disclaimer.html\n",
      "https://www.navercorp.com/ko/company/proposalRegister.nhn\n",
      "https://s.pstatic.net/static/www/rules/naverBrandRequest.doc\n",
      "https://help.naver.com/alias/membership/p.membership/main.naver\n",
      "https://nid.naver.com/user2/help/leaveId.nhn?menu=nid&lang=ko_KR\n",
      "https://gam.naver.com/optout/main \n",
      "https://privacy.naver.com/protection_activity/naver_personal_information_protection?menu=protection_naver_personal_information_protection\n",
      "http://www.navercorp.com/ko/index.nhn\n",
      "https://blog.naver.com/n_privacy\n",
      "https://privacy.kisa.or.kr/main.do\n",
      "https://www.spo.go.kr/site/spo/main.do\n",
      "http://www.police.go.kr/www/security/cyber.jsp\n",
      "https://notice.naver.com/notices/LBS/14063?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://notice.naver.com/notices/privacypolicy/13880\n",
      "https://www.naver.com/policy/youthpolicy.html\n",
      "https://help.naver.com/alias/report/Protection_report.naver\n",
      "https://policy.naver.com/policy/service_group.html\n",
      "https://www.kiso.or.kr/%EC%95%8C%EB%A6%BC%EB%A7%88%EB%8B%B9/%EC%A3%BC%EC%9A%94-%EA%B3%B5%EA%B0%9C%EC%82%AC%ED%95%AD/%EC%A0%95%EC%B1%85%EA%B2%B0%EC%A0%95/\n",
      "https://notice.naver.com/notices/cafe/11558?page=1&pageSize=10&newNoticeHour=168&t=d\n",
      "https://blog.naver.com/blogpeople/220823707644\n",
      "https://www.kiso.or.kr/%EC%A0%95%EC%B1%85%EC%9C%84%EC%9B%90%ED%9A%8C/%EC%A0%95%EC%B1%85%EA%B7%9C%EC%A0%95/\n",
      "https://www.kiso.or.kr/%EC%A0%95%EB%B3%B4%EC%84%BC%ED%84%B0/kiso-%EC%A0%95%EC%B1%85/guideline/\n",
      "https://help.naver.com/support/reportCenter/home.help\n",
      "https://green.naver.com/\n",
      "https://privacy.naver.com\n",
      "https://notice.naver.com/notices/privacynid/14997\n",
      "https://notice.naver.com/notices/privacynid/14236\n",
      "https://notice.naver.com/notices/privacynid/14235\n",
      "https://notice.naver.com/notices/privacynid/14234\n",
      "https://notice.naver.com/notices/privacynid/14233\n",
      "https://nid.naver.com/nidlogin.login?mode=form&url=https://privacy.naver.com/main?menu=home\n",
      "https://blog.naver.com//n_privacy/223471678731\n",
      "https://blog.naver.com//n_privacy/223463199093\n",
      "https://blog.naver.com//n_privacy/223455075484\n",
      "https://blog.naver.com//n_privacy/223446809582\n",
      "http://study.jr.naver.com/privacy/\n",
      "https://jr.naver.com/\n",
      "https://help.naver.com/service/5636/category/bookmark\n",
      "https://jr.naver.com\n",
      "https://help.naver.com/alias/report/report_m_3.naver\n",
      "https://right.naver.com/\n",
      "https://help.naver.com/alias/report/Illegal_userprotection.naver\n",
      "https://policy.naver.com/policy/privacy.html#a4\n",
      "https://www.facebook.com/naverprivacy\n",
      "https://inoti.naver.com/inoti/main.nhn\n",
      "https://help.naver.com/support/contents/contents.nhn?serviceNo=958&categoryNo=3423\n",
      "http://policy.naver.com/policy/disclaimer.html\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinPPkr&v=3\n",
      "https://privacy.naver.com/knowledge/cookie?menu=knowledge_info_relation_cookie\n",
      "http://policy.naver.com/policy/privacy.html#a2_3\n",
      "http://blog.naver.com/n_privacy/80143119849\n",
      "http://ko.wikipedia.org/wiki/HTTP_Cookie\n",
      "http://cookiecentral.com/\n",
      "http://www.howstuffworks.com/cookie.htm\n",
      "https://policy.naver.com/policy-mobile/term.html?type=3\n",
      "https://help.naver.com/alias/privacy/privacy_25.naver\n",
      "https://nid.naver.com/user2/common/terms/terms2?t=viewNaverJoinOPPkr&v=1\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://www.naver.com/',  # Replace with your URLs\n",
    "    'https://www.997788.com/',\t\n",
    "]\n",
    "\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        # Add a list of user agents here\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    # proxies = get_proxies()\n",
    "    # proxy = random.choice(proxies)\n",
    "    # print(f\"Using proxy: {proxy}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        if original_domain in url:\n",
    "            print(f\"Extracting links from: {url}\")\n",
    "            links = extract_links(url)\n",
    "            for link in links:\n",
    "                if link not in visited_links:\n",
    "                    # if original_domain in link:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    original_domain = \"naver.com\"\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "visited_links = set()\n",
    "all_links = []\n",
    "\n",
    "def get_random_user_agent():\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'\n",
    "        # Add more user agents if needed\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def extract_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=5, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = []\n",
    "        \n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            full_link = link.get(\"href\")\n",
    "            if full_link and full_link.startswith(\"http\"):\n",
    "                links.append(full_link)\n",
    "        return links\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def recursive_extract(url, original_domain):\n",
    "    if url not in visited_links:\n",
    "        visited_links.add(url)\n",
    "        print(f\"Extracting links from: {url}\")\n",
    "        links = extract_links(url)\n",
    "        for link in links:\n",
    "            parsed_link = urlparse(link)\n",
    "            if link not in visited_links:\n",
    "                # check if original domain is in the link\n",
    "                if original_domain in parsed_link.netloc:\n",
    "                    all_links.append(link)\n",
    "                    recursive_extract(link, original_domain)\n",
    "                    time.sleep(1)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://www.naver.com/\"  # Replace with the starting URL\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    original_domain = parsed_start_url.netloc\n",
    "    \n",
    "    print(f\"Original domain: {original_domain}, Start URL: {start_url}\")\n",
    "    recursive_extract(start_url, original_domain)\n",
    "    \n",
    "    print(\"All extracted links:\")\n",
    "    for link in all_links:\n",
    "        print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze DB Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bn', 'de', 'en', 'fa', 'pt', 'websites', 'id', 'it', 'vi', 'fr', 'ja', 'zh-cn', 'ru', 'es', 'pl', 'ko', 'policy_links', 'sv'}\n"
     ]
    }
   ],
   "source": [
    "# read tinydb database\n",
    "db = TinyDB('websites_by_language.json')\n",
    "\n",
    "# read all tables names\n",
    "tables = db.tables()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6088\n"
     ]
    }
   ],
   "source": [
    "# read policy_links table\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "policy_links = policy_links_table.all()\n",
    "print(len(policy_links))\n",
    "\n",
    "# find thirty domains for Japan and Korea which only has 1 privacy policy related link links\n",
    "\n",
    "# find thirty random domains for each country\n",
    "random_korean_domains = []\n",
    "random_japanese_domains = []\n",
    "\n",
    "for domain in policy_links:\n",
    "    if domain['country'] == \"Korea\":\n",
    "        random_korean_domains.append(domain['domain'])\n",
    "    elif domain['country'] == \"Japan\":\n",
    "        random_japanese_domains.append(domain['domain'])\n",
    "\n",
    "random_korean_domains = random.sample(random_korean_domains, 30)\n",
    "random_japanese_domains = random.sample(random_japanese_domains, 30)\n",
    "\n",
    "# find all_links for each domain and filter only privacy related links\n",
    "# for korean domains\n",
    "korean_privacy_links = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Extracted Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "# policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "5341\n",
      "Number of domains with no privacy links: 470\n",
      "Number of domains with privacy links: 2889\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3359\n",
      "2383\n"
     ]
    }
   ],
   "source": [
    "print(len(privacy_domains))\n",
    "\n",
    "# give number of privacy related links for each domain\n",
    "japan_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            japan_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(japan_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tumi.co.jp/privacyPolicy.html\n",
      "https://mocom.tv/wo/privacy.phtml\n",
      "https://locondo.jp/shop/contents/privacy\n",
      "https://anzen.ne.jp/html/info.html#about_privacy\n",
      "https://gumpla.jp/privacy\n",
      "https://support.ac-pocketcamp.com/en-AU/privacy_policy\n",
      "https://ths-fooduniform.jp/html/privacy.html\n",
      "https://tanosu.com/privacy/\n",
      "https://buffaloes.co.jp/company/privacy.html\n",
      "https://cdn.kenko-mileage.jp/policy/privacy_policy.html\n",
      "https://sen-n.com/privacy-policy/\n",
      "https://crescendoalle.com/pages/privacy\n",
      "https://atomtech.co.jp/policies/privacy-policy\n",
      "https://wadatsumi.co/policies/privacy-policy\n",
      "https://danmachi-danchro.com/privacy-policy/\n",
      "https://tryt-worker.jp/company/privacypolicy/\n",
      "https://privacy.tver.jp/tver-id-external-data-integration/\n",
      "https://talk.jp/privacy\n",
      "https://ball-goods.com/?mode=privacy\n",
      "https://izumo-pbx.jp/contact.html#privacy\n",
      "https://sug-web.jp/privacy-policy/\n",
      "https://denkohome.com/privacy/\n",
      "https://freeblog-video.com/privacy-policy/\n",
      "https://irotsuku.com/info/privacy\n",
      "https://www.convention.co.jp/privacy/\n",
      "https://emmafrancis.jp/contents/terms#privacy_content\n",
      "https://www.evastore.jp/shop/pages/privacy.aspx#anchor-cookie_block\n",
      "https://www.bornfreegroup.jp/privacypolicy\n",
      "https://car-repo.jp/privacy-policy\n",
      "https://howsie-shop.jp/pages/privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from japan_privacy_links\n",
    "random_japan_privacy_links = random.sample(japan_privacy_links, 30)\n",
    "for link in random_japan_privacy_links:\n",
    "    print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0           https://www.tumi.co.jp/privacyPolicy.html   \n",
      "1                   https://mocom.tv/wo/privacy.phtml   \n",
      "2            https://locondo.jp/shop/contents/privacy   \n",
      "3    https://anzen.ne.jp/html/info.html#about_privacy   \n",
      "4                           https://gumpla.jp/privacy   \n",
      "5   https://support.ac-pocketcamp.com/en-AU/privac...   \n",
      "6        https://ths-fooduniform.jp/html/privacy.html   \n",
      "7                         https://tanosu.com/privacy/   \n",
      "8        https://buffaloes.co.jp/company/privacy.html   \n",
      "9   https://cdn.kenko-mileage.jp/policy/privacy_po...   \n",
      "10                  https://sen-n.com/privacy-policy/   \n",
      "11            https://crescendoalle.com/pages/privacy   \n",
      "12     https://atomtech.co.jp/policies/privacy-policy   \n",
      "13       https://wadatsumi.co/policies/privacy-policy   \n",
      "14       https://danmachi-danchro.com/privacy-policy/   \n",
      "15      https://tryt-worker.jp/company/privacypolicy/   \n",
      "16  https://privacy.tver.jp/tver-id-external-data-...   \n",
      "17                            https://talk.jp/privacy   \n",
      "18               https://ball-goods.com/?mode=privacy   \n",
      "19          https://izumo-pbx.jp/contact.html#privacy   \n",
      "20                 https://sug-web.jp/privacy-policy/   \n",
      "21                     https://denkohome.com/privacy/   \n",
      "22         https://freeblog-video.com/privacy-policy/   \n",
      "23                  https://irotsuku.com/info/privacy   \n",
      "24              https://www.convention.co.jp/privacy/   \n",
      "25  https://emmafrancis.jp/contents/terms#privacy_...   \n",
      "26  https://www.evastore.jp/shop/pages/privacy.asp...   \n",
      "27         https://www.bornfreegroup.jp/privacypolicy   \n",
      "28                 https://car-repo.jp/privacy-policy   \n",
      "29               https://howsie-shop.jp/pages/privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   3aa81fddc11f27554b348c4bacbdd6892201fd52f8316c...  \n",
      "1   f93003101a1dc35a8f84f77d5de4e50349401e6d7c4a68...  \n",
      "2   0604ad08c6272bc865c58070051960ddfd5c300f184d37...  \n",
      "3   731e8b4b89538c01b1b76420dff82480750bca42dd4d0f...  \n",
      "4   5fe338baacbd7f12d15613ae812baf4dd208b4d7d366a1...  \n",
      "5   4cfbdc8279ca6745bd1f58ee304f2664d80e8c439f2ffc...  \n",
      "6   0463f7194b0d3656e2354bc5b665044706b80a1cf22807...  \n",
      "7   2903c5b5b7fdc1debad2519b112f6336a7fbb35b4dc2bf...  \n",
      "8   b8693e71c58177ea524837d0503a4cfe207b5c1255ee0b...  \n",
      "9   024a526957f6b4637b126fdf22972e336da0f19c26ae27...  \n",
      "10  ee602a8262c3146e5b85c8a2ccec7334771e3d4b642e43...  \n",
      "11  f9e29796d4e6e2805030d3152ebb73b1297b26cc566892...  \n",
      "12  f3d0f3111dea5cc64f168f9927c3484b1232ef225dec01...  \n",
      "13  114031d73fe94b624387a9a61be1963621eba2cee79d04...  \n",
      "14  97eac313480d61761278cd181640503989412bbdf93637...  \n",
      "15  d5287ba8731acaf5aa06af1d0efd71f154713c7db69b6f...  \n",
      "16  60290a97a2bc3830100a40686c30336eaec60205c08944...  \n",
      "17  17a2b0fa9e931351b5581078c36f0aed9cf152c18bee47...  \n",
      "18  be860561cbe09928fd8369165d5220cd49506cf5acc2fd...  \n",
      "19  d0a945673bbae0fded01e61ad5d67fff842eca1011ff07...  \n",
      "20  d6bfe39b0eb99223ef4d81f7055e581ec683cae3a2dd4b...  \n",
      "21  0edaf50f6ee2ab3d38009a753d3105fd3a1f677d433efa...  \n",
      "22  ab47d235fbfc20f22d0ccdd16d0be74d7a7254a39f8d33...  \n",
      "23  c628917e715e52f6e9006e016a9aa09175b5b0e7aedc4c...  \n",
      "24  babc3060c024455ee2ade4741f5d8e49be716a1f783bd4...  \n",
      "25  e2e3885ef0abb740caf804b640a1bcd73bba652f5aaaa9...  \n",
      "26  a85a4da4288ffc49316c36e8cfdc7d0e1eb92dab2f1e6c...  \n",
      "27  2a82c56ac400c385da4d277ae8c0d4c9c120b596b10921...  \n",
      "28  0646142af80e14eabc4a880466468e493ed1e8485f24b9...  \n",
      "29  a92c2f2f86f61a7897c95c6634357081e55e91ce38dd6f...  \n"
     ]
    }
   ],
   "source": [
    "# random_japan_privacy_links\n",
    "# create a dataframe with random_japan_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_japan_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df.to_csv(\"policies/sample_policies_japanese/sample_policies_japanese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Error fetching the website: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n",
      "Text content extracted from the website:\n",
      "\n",
      "Detected language: ja\n"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_japan_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df[df['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ja\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_japanese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"Korea\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n",
      "1409\n",
      "Number of domains with no privacy links: 1292\n",
      "Number of domains with privacy links: 1437\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "korean_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            korean_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(korean_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://newsway.co.kr/company/privacypolicy\n",
      "https://daumee.co.kr/member/privacy.html\n",
      "https://tophub.today/allactivity?privacy_source=activity_log_top_menu\n",
      "https://duole.com/privacy/v2/gouji?template_id=1#nav6\n",
      "https://xyzcdn.net/privacy#content\n",
      "https://bluepops.co.kr/member/privacy.html\n",
      "https://fifaro.com/text/privacy\n",
      "https://myprotein.tw/customer-services/privacy-and-security.list\n",
      "https://nsfwkr.net/bbs/register.php#privacy\n",
      "https://devicemart.co.kr/service/privacy\n",
      "https://gck99.com.tw/privacy.php\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://hk-bestcasino.com/privacy-policy/\n",
      "https://sanmin.com.tw/static/termspprivacy\n",
      "https://lolchess.gg/about/privacy\n",
      "http://www.creme21.co.kr/member/privacy.html\n",
      "https://ajou.ac.kr/kr/ajou/privacy.do\n",
      "https://balletnmodel.com/?mode=privacy\n",
      "https://roo.cash/privacy\n",
      "https://daouoffice.com/privacy_v09_1.jsp\n",
      "https://m.cnhnb.com/html/zhuanti/privacy/?id=1\n",
      "https://privacy.kakao.com/main?lang=ko\n",
      "https://htisec.com/zh-hk/data-privacy-policy\n",
      "https://cagongtv.com/content/privacy\n",
      "https://godamanga.com/privacy\n",
      "https://nikke-kr.com/privacypolicy#_Pursuant_to_our\n",
      "https://store.steamchina.com/privacy_agreement?snr=1_60_4__global-responsive-menu\n",
      "https://nikke-kr.com/privacypolicy#_Changes\n",
      "https://www.pcone.com.tw/service/privacyPolicy\n",
      "https://qquing.net/bbs/content.php?co_id=privacy\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from korean_privacy_links\n",
    "random_korean_privacy_links = random.sample(korean_privacy_links, 30)\n",
    "for link in random_korean_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Sample Policy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0         https://newsway.co.kr/company/privacypolicy   \n",
      "1            https://daumee.co.kr/member/privacy.html   \n",
      "2   https://tophub.today/allactivity?privacy_sourc...   \n",
      "3   https://duole.com/privacy/v2/gouji?template_id...   \n",
      "4                  https://xyzcdn.net/privacy#content   \n",
      "5          https://bluepops.co.kr/member/privacy.html   \n",
      "6                     https://fifaro.com/text/privacy   \n",
      "7   https://myprotein.tw/customer-services/privacy...   \n",
      "8         https://nsfwkr.net/bbs/register.php#privacy   \n",
      "9            https://devicemart.co.kr/service/privacy   \n",
      "10                   https://gck99.com.tw/privacy.php   \n",
      "11           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "12          https://hk-bestcasino.com/privacy-policy/   \n",
      "13         https://sanmin.com.tw/static/termspprivacy   \n",
      "14                  https://lolchess.gg/about/privacy   \n",
      "15       http://www.creme21.co.kr/member/privacy.html   \n",
      "16              https://ajou.ac.kr/kr/ajou/privacy.do   \n",
      "17             https://balletnmodel.com/?mode=privacy   \n",
      "18                           https://roo.cash/privacy   \n",
      "19           https://daouoffice.com/privacy_v09_1.jsp   \n",
      "20     https://m.cnhnb.com/html/zhuanti/privacy/?id=1   \n",
      "21             https://privacy.kakao.com/main?lang=ko   \n",
      "22       https://htisec.com/zh-hk/data-privacy-policy   \n",
      "23               https://cagongtv.com/content/privacy   \n",
      "24                      https://godamanga.com/privacy   \n",
      "25  https://nikke-kr.com/privacypolicy#_Pursuant_t...   \n",
      "26  https://store.steamchina.com/privacy_agreement...   \n",
      "27        https://nikke-kr.com/privacypolicy#_Changes   \n",
      "28     https://www.pcone.com.tw/service/privacyPolicy   \n",
      "29   https://qquing.net/bbs/content.php?co_id=privacy   \n",
      "\n",
      "                                                 hash  \n",
      "0   76f8d00688ce5ee4129d2cfe7810c53c1b81d4c5d82809...  \n",
      "1   4441eea2dddb934443cac7616ded62ace14dbbe17db651...  \n",
      "2   c9effc5fde671cafb31a85969f944b22ad7b6775b254cc...  \n",
      "3   4d2472373011c6f66d1a53123afd473dc329bfb630b9c4...  \n",
      "4   4f64b7cbec077f0b235583598a40a9b63d470cfa3f1b9b...  \n",
      "5   0caf8650720351b11304dcee1993f55cbb741047ed40d8...  \n",
      "6   9b5b2c7347c49766fa80a9a1188f05ce3d4a8ffcf61a04...  \n",
      "7   5144d03775d8eb736b523d33a4f8ce47d6a9d5303c6942...  \n",
      "8   753f9e12c226ddeb58b75906a956d02f36f2efac0daa4f...  \n",
      "9   5f6d20a69b3005d6025dc37ebf4c4ead341e002de46958...  \n",
      "10  1f30698e7b51975a02398b3b2348c99ccb113797a52a59...  \n",
      "11  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "12  4923a3a0fa9fa1140657bd529ddb6521bd2c4e3086e7a4...  \n",
      "13  046e039e7063fbdbe44fd3ebb9fef9ee05b4a956a3d1bd...  \n",
      "14  9b59d857a77fb34026b08fb304430966c38bd53202b6aa...  \n",
      "15  690873a0195dd4c6636f6bcf98d18d19d419e603268a3d...  \n",
      "16  4670708b7d0f2e741a6397f518921b630a6b91a67a96d9...  \n",
      "17  364a237f7cba05b069bee8c6a1054e5d8bf78bdbdb189e...  \n",
      "18  e768bf3eda7dffeaa89387396650e9206183a83a7a2cb6...  \n",
      "19  727c420b9185f45531d425645b4bb26f2ecc842b508a36...  \n",
      "20  376f97bbd9cceed3b9f88be3aea686ec20826759df45ac...  \n",
      "21  dc17e872fe7bfe10149e37d265ab2bc485f9f6798dce79...  \n",
      "22  8734940804041e2df20ad8d0434db13c926ac56a3361e0...  \n",
      "23  881f0b3bd55fac152cd92092707570af34c86246ffe1f5...  \n",
      "24  76c470a3b4e996819762864767991b497a417e45be54e5...  \n",
      "25  83ea3f88c2ce742bc760a6d82e9777ccb8b6ea6e3f5ecd...  \n",
      "26  226fd14916a7377abc449b4c504336ab5b2cb9986e99e1...  \n",
      "27  721f807ed7e33469141926186f4bed9ca1813845a8ff18...  \n",
      "28  332d68596b80569706353ba450360c159bb25012c6420f...  \n",
      "29  8c04b043562fe037dde46590353e1ebaa7b6a97757d0a8...  \n"
     ]
    }
   ],
   "source": [
    "# random_korean_privacy_links\n",
    "# create a dataframe with random_korean_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_korean_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_korean_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_korean_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_korean_privacy_links.to_csv(\"policies/sample_policies_korean/sample_policies_korean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_korean_privacy_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Main Function\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom_korean_privacy_links\u001b[49m:\n\u001b[0;32m     29\u001b[0m     text_content \u001b[38;5;241m=\u001b[39m fetch_and_convert_website(link)\n\u001b[0;32m     30\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m df_random_korean_privacy_links[df_random_korean_privacy_links[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m link][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_korean_privacy_links' is not defined"
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_korean_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_korean_privacy_links[df_random_korean_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        if language == \"ko\":\n",
    "            # print(f\"\\nDetected language: {language}\")\n",
    "\n",
    "            # write to txt file with utf-8 encoding\n",
    "            with open(f\"policies/sample_policies_korean/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### China"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Websites Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank            domain\n",
      "0            1        google.com\n",
      "1            2      facebook.com\n",
      "2            3     microsoft.com\n",
      "3            4    googleapis.com\n",
      "4            5         apple.com\n",
      "...        ...               ...\n",
      "578155  578156  lurkerlounge.com\n",
      "578156  578157          528y.com\n",
      "578157  578158     garybartz.com\n",
      "578158  578159          fsagq.cn\n",
      "578159  578160    spottyd10.buzz\n",
      "\n",
      "[578160 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# read \"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\" into dataframe. the csv has no headers\n",
    "tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\", header=None, index_col=False)\n",
    "# tranco_df = pd.read_csv(\"../WebSiteRankings/tranco_J9LYY_8June2024_1year.csv\")\n",
    "\n",
    "# add headers to the dataframe\n",
    "tranco_df.columns = [\"rank\", \"domain\"]\n",
    "\n",
    "print(tranco_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_domains(country_code):\n",
    "    db = TinyDB('websites_by_language.json')\n",
    "\n",
    "\n",
    "    websites_table = db.table('websites')\n",
    "    Website = Query()\n",
    "\n",
    "    korean_websites = websites_table.search(Website.language == country_code)\n",
    "\n",
    "    all_korean_websites = [website['url'] for website in korean_websites]\n",
    "    print(f\"Total {country_code} websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "    # # websites_to_process = korean_websites[:100]\n",
    "    # # websites_to_process = [website['url'] for website in websites_to_process]\n",
    "    # # print(websites_to_process)\n",
    "\n",
    "    # policy_links_table = db.table('policy_links')\n",
    "    # existing_policy_links = policy_links_table.all()\n",
    "    # # only korean domains which has policy_link['country'] == \"Korea\"\n",
    "\n",
    "    # if country_code == \"ko\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "    # elif country_code == \"zh-cn\" or country_code == \"zh\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "    # elif country_code == \"ja\":\n",
    "    #     existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "\n",
    "    # print(f\"Number of {country_code} domains in policy links table: {len(set(existing_policy_links))}\")\n",
    "\n",
    "    # remaining_websites = []\n",
    "\n",
    "    # for each_website in all_korean_websites:\n",
    "    #     if each_website not in existing_policy_links:\n",
    "    #         remaining_websites.append(each_website)\n",
    "    # print(f\"Total remaining website {len(remaining_websites)}\")\n",
    "\n",
    "    return all_korean_websites\n",
    "\n",
    "    # return remaining_websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total zh-cn websites: 16652\n",
      "Total zh-tw websites: 15\n",
      "16667\n"
     ]
    }
   ],
   "source": [
    "chinese_websites_to_process = get_all_domains(\"zh-cn\")\n",
    "mandarin_websites_to_process = get_all_domains(\"zh-tw\")\n",
    "chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese_websites_to_process = get_remaining_domains(\"zh-cn\")\n",
    "# mandarin_websites_to_process = get_remaining_domains(\"zh\")\n",
    "# chinese_mandarin_domains_to_process = chinese_websites_to_process + mandarin_websites_to_process\n",
    "# print(len(chinese_mandarin_domains_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# top3K_chinese_mandarin_domains store to file top3K_chinese_mandarin_domains.txt\n",
    "with open(\"top3K_chinese_mandarin_domains.txt\", \"w\") as f:\n",
    "    for domain in top3K_chinese_mandarin_domains:\n",
    "        f.write(domain + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rank           domain\n",
      "64          65           qq.com\n",
      "316        317     bilibili.com\n",
      "1396      1397           360.cn\n",
      "1697      1698         amap.com\n",
      "1924      1925       alipay.com\n",
      "...        ...              ...\n",
      "326603  326604      dlwafuu.com\n",
      "326610  326611     zhengxsy.com\n",
      "326612  326613  huiqianbian.com\n",
      "326621  326622   sumflurish.com\n",
      "326622  326623    whdaiqian.com\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# find top 3000 chinese_mandarin_domains_to_process from tranco_df table.\n",
    "\n",
    "top3K_chinese_mandarin_domains = tranco_df[tranco_df['domain'].isin(chinese_mandarin_domains_to_process)]\n",
    "# print(top3K_chinese_mandarin_domains)\n",
    "# sort by thank rank column\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains.sort_values(by='rank')\n",
    "# only keep top 3000 domains\n",
    "top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains[:3000]\n",
    "print(top3K_chinese_mandarin_domains)\n",
    "\n",
    "# only keep domains in a list\n",
    "# top3K_chinese_mandarin_domains = top3K_chinese_mandarin_domains['domain'].tolist()\n",
    "# print(len(top3K_chinese_mandarin_domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053\n"
     ]
    }
   ],
   "source": [
    "#  read policy links from websites_by_language.json file in the policy_links table\n",
    "\n",
    "db = TinyDB('websites_by_language.json')\n",
    "policy_links_table = db.table('policy_links')\n",
    "\n",
    "# read all policy links\n",
    "# policy_links = policy_links_table.all()\n",
    "\n",
    "# filter privacy policy links based on Korea country\n",
    "policy_links = policy_links_table.search(Query().country == \"China\")\n",
    "# policy_links = policy_links_table.search(Query().country == \"Japan\")\n",
    "\n",
    "\n",
    "print(len(policy_links))\n",
    "privacy_domains = {}\n",
    "all_privacy_links = []\n",
    "for domain in policy_links:\n",
    "    # print(domain['domain'], len(domain['all_links']) )\n",
    "    # check if there is any privacy policy link in the all_links\n",
    "    privacy_links = []\n",
    "    for link in domain['all_links']:\n",
    "        if \"privacy\" in link or \"policy\" in link or \"cookies\" in link or \"gdpr\" in link or \"security\" in link or \"legal\" in link or \"agreement\" in link:\n",
    "            privacy_links.append(link)\n",
    "            all_privacy_links.append(link)\n",
    "            \n",
    "            # print(link)\n",
    "            # break\n",
    "    privacy_domains[domain['domain']] = privacy_links\n",
    "    # print(len(privacy_links))\n",
    "\n",
    "all_privacy_links = list(set(all_privacy_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053\n",
      "508\n",
      "Number of domains with no privacy links: 537\n",
      "Number of domains with privacy links: 516\n"
     ]
    }
   ],
   "source": [
    "# privacy domains\n",
    "print(len(privacy_domains))\n",
    "print(len(all_privacy_links))\n",
    "\n",
    "# check privacy_domains[domain['domain']] to see how many are empty and how many are non-empty\n",
    "empty_domains = []\n",
    "non_empty_domains = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    if len(links) == 0:\n",
    "        empty_domains.append(domain)\n",
    "    else:\n",
    "        non_empty_domains.append(domain)\n",
    "\n",
    "\n",
    "print(f\"Number of domains with no privacy links: {len(empty_domains)}\")\n",
    "print(f\"Number of domains with privacy links: {len(non_empty_domains)}\")\n",
    "# for domain, links in privacy_domains.items():\n",
    "#     print(domain, len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "# Test with just \"privacy\" keyword in the links\n",
    "# give number of privacy related links for each domain\n",
    "chinese_privacy_links = []\n",
    "for domain, links in privacy_domains.items():\n",
    "    # print(domain, len(links), links)\n",
    "    for link in links:\n",
    "        if \"privacy\" in link:\n",
    "            chinese_privacy_links.append(link)\n",
    "            # print(link)\n",
    "print(len(set(chinese_privacy_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ooopic.com/intro/about/privacyPolicy/\n",
      "https://ijinshan.com/function/privacy/\n",
      "https://gotokeep.com/privacy\n",
      "https://www.babytree.com/app/privacy.html\n",
      "https://www.sumy.org.cn/privacy.html\n",
      "https://pop800.com/privacy_policy.html\n",
      "https://www.smm.cn/privacy\n",
      "https://e.vhall.com/v3/privacyPolicy\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#special\n",
      "https://dmttang.com/label/privacy.html\n",
      "https://www.horoscopetruth.com/privacy\n",
      "https://lianxinapp.com/privacy.html\n",
      "http://www.camera360.com/privacy_zh.html\n",
      "https://freereceivesms.com/privacy/\n",
      "http://www.sumy.org.cn/privacy.html\n",
      "https://www.shanbay.com/help/about/privacy/\n",
      "https://pp.cn/privacy\n",
      "https://ppio.work/privacy-policy-20240524-v1.html\n",
      "https://www.dianxiaomi.com/privacy.htm\n",
      "https://learnku.com/docs/guide/privacy/4994\n",
      "https://zhibo8.com/web/privacyPolicyPc.html\n",
      "https://pop800.com/privacy_policy.html\n",
      "https://pp.cn/privacy\n",
      "https://qweather.com/terms/privacy\n",
      "https://hsbc.com.cn/help/mandatory-info/privacy-and-security/#store\n",
      "https://passport.migu.cn/portal/privacy/protocol?sourceid=208003\n",
      "https://www.bytem.com/privacy-policy/\n",
      "https://mubu.com/mubu-simple-privacy-policy/\n",
      "https://agreementservice.svs.nike.com.cn/rest/agreement?agreementType=privacyPolicy&uxId=com.nike.unite&country=CN&language=zh&requestType=redirect\n",
      "https://akspeedy.com/html/privacy-agreement.html\n"
     ]
    }
   ],
   "source": [
    "# randomly select 30 links from \n",
    "random_chinese_privacy_links = random.sample(chinese_privacy_links, 30)\n",
    "for link in random_chinese_privacy_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Chinese Sample Privacy Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url  \\\n",
      "0   https://www.ooopic.com/intro/about/privacyPolicy/   \n",
      "1              https://ijinshan.com/function/privacy/   \n",
      "2                        https://gotokeep.com/privacy   \n",
      "3           https://www.babytree.com/app/privacy.html   \n",
      "4                https://www.sumy.org.cn/privacy.html   \n",
      "5              https://pop800.com/privacy_policy.html   \n",
      "6                          https://www.smm.cn/privacy   \n",
      "7                https://e.vhall.com/v3/privacyPolicy   \n",
      "8   https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "9              https://dmttang.com/label/privacy.html   \n",
      "10             https://www.horoscopetruth.com/privacy   \n",
      "11                https://lianxinapp.com/privacy.html   \n",
      "12           http://www.camera360.com/privacy_zh.html   \n",
      "13                https://freereceivesms.com/privacy/   \n",
      "14                http://www.sumy.org.cn/privacy.html   \n",
      "15        https://www.shanbay.com/help/about/privacy/   \n",
      "16                              https://pp.cn/privacy   \n",
      "17  https://ppio.work/privacy-policy-20240524-v1.html   \n",
      "18             https://www.dianxiaomi.com/privacy.htm   \n",
      "19        https://learnku.com/docs/guide/privacy/4994   \n",
      "20        https://zhibo8.com/web/privacyPolicyPc.html   \n",
      "21             https://pop800.com/privacy_policy.html   \n",
      "22                              https://pp.cn/privacy   \n",
      "23                 https://qweather.com/terms/privacy   \n",
      "24  https://hsbc.com.cn/help/mandatory-info/privac...   \n",
      "25  https://passport.migu.cn/portal/privacy/protoc...   \n",
      "26              https://www.bytem.com/privacy-policy/   \n",
      "27       https://mubu.com/mubu-simple-privacy-policy/   \n",
      "28  https://agreementservice.svs.nike.com.cn/rest/...   \n",
      "29   https://akspeedy.com/html/privacy-agreement.html   \n",
      "\n",
      "                                                 hash  \n",
      "0   8d962324b48bcb726942bf7680cdd5040194308634d07a...  \n",
      "1   d4a1e978f5dffa9582f54472aa18bdd66d1877e7b7e274...  \n",
      "2   a8f8b53118ef694ea6875d6ee92e2777f6c10d33a9ff1d...  \n",
      "3   0255d8eab3c9cca5fd508b0e53619dd182324876327466...  \n",
      "4   b2545fb9903fc3e5eb1017c0dcd4a45a7f6e4489304f4d...  \n",
      "5   8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "6   2183f58b06af982b21730f60cdc8c663635ee1c8b4fe7b...  \n",
      "7   e494cd8c6a67e37867349ac09a1f9213fbe7df0b61d8cd...  \n",
      "8   9b4f04e5f22470e874b919e7a32fcc099d2effac6568e5...  \n",
      "9   f24495fb39865d429751b81a8c1b988bacbc5a96c11c19...  \n",
      "10  d269dc0aee86f1b3ec76c500799f621873caa7cbc4f138...  \n",
      "11  b5688f5362d3adc3c8f280b0e3a9f35b7ece9f3675717e...  \n",
      "12  5157c09ae62f74e520014e2ed663eb11d09d09e61e2ccb...  \n",
      "13  4d8146865fe3ad7eccb95832e3c9d41523bc632e57ea75...  \n",
      "14  110088c2194dfac2d6f06036caf9c406767390765e0643...  \n",
      "15  5b6bdf689f9d2032e5f568bf9fcf39ec00b3b5cc43ba19...  \n",
      "16  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "17  19a1e09a202516bf1bd71cc7b66f5fca56bac5a5fd39d6...  \n",
      "18  24b23e040b3fa35aae0499bd009090ea829c3775280326...  \n",
      "19  bd3d38f95ee2147c84a0884f90148bfeac6174362f6681...  \n",
      "20  887c0920886d2263a96ab93a7eec9f944f6251511bd30c...  \n",
      "21  8767e700abbb36e90cfb34b1465eaaac5a101aaafc0a3f...  \n",
      "22  16aa14edf5cfbc5de3213b8457550eb7a4db4ac0aa0959...  \n",
      "23  f9c7bf29552b15866235771d00876a0238df455031d876...  \n",
      "24  4d06405ba73ec8f20c172acb745d01bc2e448050340ac0...  \n",
      "25  e211de0e09e38a56f73a46dac3cfdda6e6beb89537410b...  \n",
      "26  acad12dcc280f6c44c42c109fb0bec4d9c82b4db3d8051...  \n",
      "27  d704126b9f686c3e070adba47cb94485c0f89eb723022d...  \n",
      "28  4c46cf5e8f005eea1c129536c1bf1ab26984b6ff27cde2...  \n",
      "29  299aa5a241ba54d526fcfa6ee7fa755315f331e02f6942...  \n"
     ]
    }
   ],
   "source": [
    "# random_chinese_privacy_links\n",
    "\n",
    "# create a dataframe with random_chinese_privacy_links and each url's hash value using sha256\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for link in random_chinese_privacy_links:\n",
    "    hash_object = hashlib.sha256(link.encode())\n",
    "    hex_dig = hash_object.hexdigest()\n",
    "    data.append([link, hex_dig])\n",
    "\n",
    "df_random_chinese_privacy_links = pd.DataFrame(data, columns=[\"url\", \"hash\"])\n",
    "print(df_random_chinese_privacy_links)\n",
    "\n",
    "# save df to policies/sample_policies_japanese\n",
    "df_random_chinese_privacy_links.to_csv(\"policies/sample_policies_chinese/sample_policies_chinese.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching the website: 403 Client Error: Forbidden for url: https://www.ooopic.com/intro/about/privacyPolicy/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Main Function\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m random_chinese_privacy_links:\n\u001b[1;32m---> 29\u001b[0m     text_content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_and_convert_website\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m df_random_chinese_privacy_links[df_random_chinese_privacy_links[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m link][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhash\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# print(hash_value)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mfetch_and_convert_website\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_and_convert_website\u001b[39m(url):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# Fetch the website content\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Check if the request was successful\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Parse the website content\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1092\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[0;32m   1095\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1096\u001b[0m         (\n\u001b[0;32m   1097\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconn\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1102\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1103\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:642\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_time_off:\n\u001b[0;32m    634\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    635\u001b[0m         (\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem time is way off (before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRECENT_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). This will probably \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    639\u001b[0m         SystemTimeWarning,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[1;32m--> 642\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39mis_verified\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\connection.py:783\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[1;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[0;32m    781\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[1;32m--> 783\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:469\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\urllib3\\util\\ssl_.py:513\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    510\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:517\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    512\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   1102\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1104\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1382\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[0;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fetch_and_convert_website(url):\n",
    "    try:\n",
    "        # Fetch the website content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the website content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get text content and remove extra whitespace\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "\n",
    "for link in random_chinese_privacy_links:\n",
    "    text_content = fetch_and_convert_website(link)\n",
    "    hash_value = df_random_chinese_privacy_links[df_random_chinese_privacy_links['url'] == link]['hash'].values[0]\n",
    "    # print(hash_value)\n",
    "    if text_content:\n",
    "        # Print the text content\n",
    "        # print(\"Text content extracted from the website:\")\n",
    "        # print(text_content)\n",
    "                \n",
    "        # Detect and print the language of the text content\n",
    "        language = detect_language(text_content)\n",
    "        with open(f\"policies/sample_policies_chinese/{hash_value}.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(text_content)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Progress of Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Korean websites: 2730\n",
      "Number of Korean domains in policy links table: 2729, percentage: 0.9996336996336996\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "korean_websites = websites_table.search(Website.language == \"ko\")\n",
    "\n",
    "all_korean_websites = [website['url'] for website in korean_websites]\n",
    "print(f\"Total Korean websites: {len(set(all_korean_websites))}\")\n",
    "\n",
    "# websites_to_process = korean_websites[:100]\n",
    "# websites_to_process = [website['url'] for website in websites_to_process]\n",
    "# print(websites_to_process)\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "# only korean domains which has policy_link['country'] == \"Korea\"\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Korea\"]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "# existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links]\n",
    "\n",
    "print(f\"Number of Korean domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_korean_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_korean_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Japanese websites: 3380\n",
      "Number of Japan domains in policy links table: 3352, percentage: 0.991715976331361\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "all_japanese_websites = [website['url'] for website in japanese_website]\n",
    "print(f\"Total Japanese websites: {len(set(all_japanese_websites))}\")\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"Japan\"]\n",
    "\n",
    "print(f\"Number of Japan domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/len(set(all_japanese_websites))}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_japanese_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = TinyDB('websites_by_language.json')\n",
    "websites_table = db.table('websites')\n",
    "Website = Query()\n",
    "\n",
    "# japanese_website = websites_table.search(Website.language == \"ja\")\n",
    "# # websites = websites_table.search(Website.language == \"ja\")\n",
    "\n",
    "print(f\"Total Chinese websites: 3000\")\n",
    "\n",
    "policy_links_table = db.table('policy_links')\n",
    "existing_policy_links = policy_links_table.all()\n",
    "existing_policy_links = [policy_link['domain'] for policy_link in existing_policy_links if policy_link['country'] == \"China\"]\n",
    "\n",
    "print(f\"Number of China domains in policy links table: {len(set(existing_policy_links))}, percentage: {len(set(existing_policy_links))/3000}\")\n",
    "\n",
    "\n",
    "remaining_websites = []\n",
    "\n",
    "for each_website in all_japanese_websites:\n",
    "    if each_website not in existing_policy_links:\n",
    "        remaining_websites.append(each_website)\n",
    "print(len(remaining_websites))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
